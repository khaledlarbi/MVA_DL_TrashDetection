{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/khaledlarbi/MVA_DL_TrashDetection/blob/main/anchor_boxes.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6XSixqNNb_7Z",
        "toc": true
      },
      "source": [
        "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
        "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Preparation-of-the-proposed-region-to-train-the-Fast-RCNN\" data-toc-modified-id=\"Preparation-of-the-proposed-region-to-train-the-Fast-RCNN-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Preparation of the proposed region to train the Fast RCNN</a></span></li><li><span><a href=\"#Region-proposal-network\" data-toc-modified-id=\"Region-proposal-network-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Region proposal network</a></span></li><li><span><a href=\"#Training-RPN-network\" data-toc-modified-id=\"Training-RPN-network-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Training RPN network</a></span></li><li><span><a href=\"#Coco-dataset\" data-toc-modified-id=\"Coco-dataset-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Coco dataset</a></span></li></ul></div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-gZieto6b_7d"
      },
      "source": [
        "This notebook aims to provide functions that produce anchor boxes as decribed in the paper."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0YTdXFc3b_7e"
      },
      "source": [
        "A box will be describe either as a numpy array $[y^-, x^-, y^+, x^+]$  or as a numpy array $[c_y, c_x, h,w]$\n",
        "\n",
        "TODO : CHECK +1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "outputs_hidden": true
        },
        "id": "A8hRjHO551eS",
        "outputId": "304eb51a-d031-49f9-d969-7cad5183a024"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (1.10.1)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.6/dist-packages (0.11.2)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.6/dist-packages (0.10.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.6/dist-packages (from torch) (3.7.4.3)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from torch) (0.8)\n",
            "Requirement already satisfied: pillow!=8.3.0,>=5.3.0 in /usr/local/lib/python3.6/dist-packages (from torchvision) (8.0.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torchvision) (1.19.4)\n",
            "\u001b[33mWARNING: You are using pip version 20.2.4; however, version 21.3.1 is available.\n",
            "You should consider upgrading via the '/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "!pip3 install torch torchvision torchaudio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tya5x2dU51eU",
        "outputId": "efef5f0a-cfcc-4583-ff80-5521706932a3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pycocotools in /usr/local/lib/python3.6/dist-packages (2.0.3)\n",
            "Requirement already satisfied: cython>=0.27.3 in /usr/local/lib/python3.6/dist-packages (from pycocotools) (0.29.25)\n",
            "Requirement already satisfied: matplotlib>=2.1.0 in /usr/local/lib/python3.6/dist-packages (from pycocotools) (3.3.3)\n",
            "Requirement already satisfied: setuptools>=18.0 in /usr/local/lib/python3.6/dist-packages (from pycocotools) (51.0.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.1.0->pycocotools) (2.4.7)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.1.0->pycocotools) (0.10.0)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.1.0->pycocotools) (8.0.1)\n",
            "Requirement already satisfied: numpy>=1.15 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.1.0->pycocotools) (1.19.4)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.1.0->pycocotools) (1.3.1)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.1.0->pycocotools) (2.8.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from cycler>=0.10->matplotlib>=2.1.0->pycocotools) (1.15.0)\n",
            "\u001b[33mWARNING: You are using pip version 20.2.4; however, version 21.3.1 is available.\n",
            "You should consider upgrading via the '/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "!pip3 install pycocotools"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZXgjTh3oAbWt"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "from torchvision import transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as patches #In order to draw the box ! (je sais pas pourquoi j'Ã©cris en anglais)\n",
        "from torchvision import models\n",
        "import torch.utils.data as data\n",
        "from PIL import Image\n",
        "import os\n",
        "import os.path\n",
        "import torchvision\n",
        "\n",
        "use_cuda = torch.cuda.is_available()\n",
        "device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n",
        "torch.backends.cudnn.benchmark = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uOI94_f_51eV"
      },
      "outputs": [],
      "source": [
        "import copy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gEMPqMqMrBsn"
      },
      "source": [
        "# Anchors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-12-08T23:12:00.294254Z",
          "start_time": "2021-12-08T23:12:00.281183Z"
        },
        "id": "RtROAiS1b_7f"
      },
      "outputs": [],
      "source": [
        "def vertice_to_yxhw(anchor):\n",
        "    res = (np.mean((anchor[0],anchor[2])),np.mean((anchor[1],anchor[3])), anchor[2] - anchor[0] + 1, anchor[3] - anchor[1]+1)\n",
        "    return np.array(res)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QujcnpGi51eW"
      },
      "outputs": [],
      "source": [
        "def vertice_to_yxhw_array(anchor):\n",
        "    res = (np.mean((anchor[:,0],anchor[:,2]), axis = 0),np.mean((anchor[:,1],anchor[:,3]), axis = 0), anchor[:,2] - anchor[:,0] + 1, anchor[:,3] - anchor[:,1]+1)\n",
        "    return np.transpose(np.array(res))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-12-08T23:12:00.310283Z",
          "start_time": "2021-12-08T23:12:00.296255Z"
        },
        "id": "nzn9r1Dzb_7g"
      },
      "outputs": [],
      "source": [
        "def yxhw_to_vertice(anchor):\n",
        "    res = (anchor[0] - anchor[2]/2, anchor[1] - anchor[3]/2, anchor[0] + anchor[2]/2, anchor[1] + anchor[3]/2)\n",
        "    return np.array(res)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4DmF9weOH8Uj"
      },
      "outputs": [],
      "source": [
        "def xywh_to_vertice(anchor):\n",
        "  anchor_perm = (anchor[1] + 0.5*anchor[3] ,anchor[0] + 0.5*anchor[2],anchor[3],anchor[2])\n",
        "  return yxhw_to_vertice(anchor_perm)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-12-08T23:12:00.326279Z",
          "start_time": "2021-12-08T23:12:00.312275Z"
        },
        "id": "4NrJLfwrb_7g"
      },
      "outputs": [],
      "source": [
        "def anchor_box(center, ratio, scale, shape_initial, shape_featured):\n",
        "    sub_width = shape_initial[0]/shape_featured[0]\n",
        "    sub_height = shape_initial[1]/shape_featured[1]\n",
        "    anchor_width = sub_width*scale*np.sqrt(ratio)\n",
        "    anchor_height = sub_height*scale/np.sqrt(ratio)\n",
        "    \n",
        "    ym = center[1] - anchor_height/2\n",
        "    yp = center[1] + anchor_height/2\n",
        "    xm = center[0] - anchor_width/2\n",
        "    xp = center[0] + anchor_width/2\n",
        "    \n",
        "    anchor = (ym,xm,yp,xp)\n",
        "    return(anchor)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-12-08T23:12:00.341279Z",
          "start_time": "2021-12-08T23:12:00.328260Z"
        },
        "id": "MEYHa3sPb_7h"
      },
      "outputs": [],
      "source": [
        "def list_centers(shape_initial, shape_featured):\n",
        "    ratio_h = shape_initial[1]/shape_featured[1]\n",
        "    ratio_w = shape_initial[0]/shape_featured[0]\n",
        "    #intiail center is the center at the left top corner\n",
        "    all_centers = [np.array((ratio_w/2, ratio_h/2),dtype=float) + np.array((ratio_w*i, ratio_h*j),dtype=float) for i in range(int(shape_featured[0])) for j in range(int(shape_featured[1]))]\n",
        "    return(all_centers)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-12-08T23:12:00.356279Z",
          "start_time": "2021-12-08T23:12:00.343272Z"
        },
        "id": "2klfx8Fjb_7h"
      },
      "outputs": [],
      "source": [
        "def anchor_boxes(list_ratios, list_scales, shape_initial, shape_featured):\n",
        "    list_center = list_centers(shape_initial, shape_featured)\n",
        "    all_anchors = [anchor_box(center, ratio, scale,shape_initial,shape_featured) for center in list_center for ratio in list_ratios\n",
        "                   for scale in list_scales]\n",
        "    return(all_anchors)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-12-08T23:12:00.372270Z",
          "start_time": "2021-12-08T23:12:00.358270Z"
        },
        "id": "Y1jD1fLqb_7i"
      },
      "outputs": [],
      "source": [
        "def check_anchor_inside(anchor_box, shape_initial):\n",
        "    ym = anchor_box[0]\n",
        "    yp = anchor_box[2]\n",
        "    xm = anchor_box[1]\n",
        "    xp = anchor_box[3]\n",
        "    is_inside = (min(xm,xp)>0) & (max(xm,xp)<shape_initial[0]) & (max(yp,ym) < shape_initial[1]) & (min(ym,yp) > 0) \n",
        "    return(is_inside)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vq8CaL-MS5Jq"
      },
      "outputs": [],
      "source": [
        "# def iou(box1,box2):\n",
        "#     xm = max(box1[1], box2[1])\n",
        "#     xp = min(box1[3], box2[3])\n",
        "#     ym = max(box1[0], box2[0])\n",
        "#     yp = min(box1[2], box2[2])\n",
        "    \n",
        "#     intersection = 0\n",
        "    \n",
        "#     if((xm < xp) &(ym < yp)):\n",
        "#         intersection = (xp - xm)*(yp-ym)\n",
        "    \n",
        "#     union = (box1[3]-box1[1])*(box1[2] - box1[0]) + (box2[3]-box2[1])*(box2[2] - box2[0]) - intersection\n",
        "#     return(intersection/union)\n",
        "\n",
        "# def iou_anchors_vs_gtbox(list_anchors, list_gt_box):\n",
        "#     res = np.transpose([[iou(anchor, gt_box) for anchor in list_anchors] for gt_box in list_gt_box])\n",
        "#     return(np.array(res))\n",
        "\n",
        "def iou_anchors_vs_gtbox(list_anchors, list_gt_box):\n",
        "  t_anchors = torch.tensor(np.array(list_anchors))\n",
        "  t_list_gt_box = torch.tensor(np.array(list_gt_box))\n",
        "  iou = torchvision.ops.box_iou(t_anchors, t_list_gt_box)\n",
        "  return iou.numpy()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iTlLHh_Vb_7j"
      },
      "source": [
        "**TODO** : changer la forme de cette fonction en utilisant que des *arrays*."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-12-08T23:12:00.419270Z",
          "start_time": "2021-12-08T23:12:00.406270Z"
        },
        "id": "B8S6FE2db_7j"
      },
      "outputs": [],
      "source": [
        "#Return an array with :\n",
        "#for all ground truth box, the anchors which maximize the IOU with it\n",
        "#for all anchor, the max of the IOU\n",
        "\n",
        "#the first column of the array is the index and the last the IOU \n",
        "def best_anchors_from_iou(dt_anchors_vs_gtbox):\n",
        "    #index highest by gtbox (cond a)\n",
        "    dt_anchors_vs_gtbox.argmax(axis = 0)\n",
        "    ind_argmax = np.where(dt_anchors_vs_gtbox == dt_anchors_vs_gtbox.max(axis = 0))[0]\n",
        "    cond_a = dt_anchors_vs_gtbox[ind_argmax,:].max(axis = 1)\n",
        "    \n",
        "    #highest by anchors box (cond b)\n",
        "    index = dt_anchors_vs_gtbox.argmax(axis = 1)\n",
        "    iou_max = dt_anchors_vs_gtbox.max(axis = 1)\n",
        "    cond_b = dt_anchors_vs_gtbox[np.arange(dt_anchors_vs_gtbox.shape[0]),index]\n",
        "    \n",
        "    index_res = np.concatenate((ind_argmax,np.arange(dt_anchors_vs_gtbox.shape[0])))\n",
        "    res = np.concatenate((cond_a, cond_b), axis=0)\n",
        "    res = np.column_stack((index_res,res))\n",
        "    return(res)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-12-08T23:12:00.434260Z",
          "start_time": "2021-12-08T23:12:00.421270Z"
        },
        "id": "9VqTF3Itb_7j"
      },
      "outputs": [],
      "source": [
        "#label_from_iou returns a np.array containing for each anchor its label. (+1 if foreground, 0 if background and -1 if not used\n",
        "#during the learning phase)\n",
        "#The default thresholds are defined according the original paper about Fatest RCNN.\n",
        "\n",
        "def label_from_iou(dt_anchors_vs_gtbox,pos_threshold = 0.7, neg_threshold = 0.3):\n",
        "    label = np.full(dt_anchors_vs_gtbox.shape[0],-1)\n",
        "    iou_max = dt_anchors_vs_gtbox.max(axis = 1)\n",
        "    #positive labels : 1\n",
        "    label[iou_max > pos_threshold] = 1\n",
        "    #negative labels : 0\n",
        "    label[iou_max < neg_threshold] = 0\n",
        "    sum((iou_max < neg_threshold))\n",
        "\n",
        "    #for anchors whose maximize IOU for a given object : +1\n",
        "    dt_anchors_vs_gtbox.argmax(axis = 0)\n",
        "    ind_argmax = np.where(dt_anchors_vs_gtbox == dt_anchors_vs_gtbox.max(axis = 0))[0]\n",
        "    label[ind_argmax] = 1\n",
        "    return(label)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-12-08T23:12:00.449260Z",
          "start_time": "2021-12-08T23:12:00.436260Z"
        },
        "id": "aV_HYx5Ab_7k"
      },
      "outputs": [],
      "source": [
        "def loc(anchor_box, gt_box):\n",
        "    anchor_box = vertice_to_yxhw(anchor_box)\n",
        "    gt_box = vertice_to_yxhw(gt_box)\n",
        "    \n",
        "    y = (gt_box[0] - anchor_box[0])/anchor_box[2]\n",
        "    x = (gt_box[1] - anchor_box[1])/anchor_box[3]\n",
        "    w = np.log(gt_box[3]/anchor_box[3])\n",
        "    h = np.log(gt_box[2]/anchor_box[2])\n",
        "    \n",
        "    return np.array((y,x,h,w))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E_6_fU8Tb-yM"
      },
      "outputs": [],
      "source": [
        "def loc_list(anchors_list, gt_list):\n",
        "    anchor_box = vertice_to_yxhw_array(np.array(anchors_list))\n",
        "    gt_box = vertice_to_yxhw_array(np.array(gt_list))\n",
        "    \n",
        "    y = (gt_box[:,0] - anchor_box[:,0])/anchor_box[:,2]\n",
        "    x = (gt_box[:,1] - anchor_box[:,1])/anchor_box[:,3]\n",
        "    w = np.log(gt_box[:,3]/anchor_box[:,3])\n",
        "    h = np.log(gt_box[:,2]/anchor_box[:,2])\n",
        "    return np.transpose(np.array((y,x,h,w)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-12-08T23:12:00.465251Z",
          "start_time": "2021-12-08T23:12:00.451274Z"
        },
        "id": "CI9iedq9b_7k"
      },
      "outputs": [],
      "source": [
        "def deloc(anchor_box, reparam_box):\n",
        "    anchor_box = vertice_to_yxhw(anchor_box)\n",
        "    y = anchor_box[0] + (reparam_box[0] * anchor_box[2])\n",
        "    x = anchor_box[1] + (reparam_box[1] * anchor_box[3])\n",
        "    h = np.exp(reparam_box[2])*anchor_box[2]\n",
        "    w = np.exp(reparam_box[3])*anchor_box[3]\n",
        "    return np.array((y,x,h,w))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HdCVJYAb51eb"
      },
      "outputs": [],
      "source": [
        "def deloc_list(anchors_list, gt_list):\n",
        "    anchor_box = vertice_to_yxhw_array(np.array(anchors_list))\n",
        "    reparam_box = np.array(gt_list)\n",
        "    \n",
        "    y = anchor_box[:,0] + (reparam_box[:,0] * anchor_box[:,2])\n",
        "    x = anchor_box[:,1] + (reparam_box[:,1] * anchor_box[:,3])\n",
        "    h = np.exp(reparam_box[:,2])*anchor_box[:,2]\n",
        "    w = np.exp(reparam_box[:,3])*anchor_box[:,3]\n",
        "    \n",
        "    #    res = (anchor[0] - anchor[2]/2, anchor[1] - anchor[3]/2, anchor[0] + anchor[2]/2, anchor[1] + anchor[3]/2)\n",
        "    #   res = (np.mean((anchor[0],anchor[2])),np.mean((anchor[1],anchor[3])), anchor[2] - anchor[0] + 1, anchor[3] - anchor[1]+1)\n",
        "\n",
        "    return np.transpose(np.array((y - h/2,x-w/2,y + h/2,x + w/2)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-12-08T23:12:00.481256Z",
          "start_time": "2021-12-08T23:12:00.467253Z"
        },
        "id": "jeMfYSLTb_7k"
      },
      "outputs": [],
      "source": [
        "def reparam_all_anchors(list_anchors, list_gt_box,iou,pos_threshold = 0.7, neg_threshold = 0.3):\n",
        "    index_max_gtbox = iou.argmax(axis = 1)\n",
        "    gt_box_by_anchors = [list_gt_box[i] for i in index_max_gtbox]\n",
        "    #TODO : change suboptimal ZIP\n",
        "    res = loc_list(list_anchors, gt_box_by_anchors)\n",
        "    #compute labels\n",
        "    labels = label_from_iou(iou, pos_threshold, neg_threshold)\n",
        "    return list(res),labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-12-08T23:12:00.497279Z",
          "start_time": "2021-12-08T23:12:00.483259Z"
        },
        "id": "FSjeRrgIb_7k"
      },
      "outputs": [],
      "source": [
        "def deparam_all_anchors(list_anchors, list_box_param):\n",
        "    res = [(deloc(anchor, box_param)) for anchor,box_param in zip(list_anchors, list_box_param)]\n",
        "    return res"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-12-08T23:12:00.513279Z",
          "start_time": "2021-12-08T23:12:00.500252Z"
        },
        "id": "sORoyapOb_7l"
      },
      "outputs": [],
      "source": [
        "#TODO : heck how to fill when\n",
        "\n",
        "def index_training_proposal(dt_anchors_vs_gtbox, nsize = 256, pos_ratio = 0.5,pos_threshold = 0.7, neg_threshold = 0.3):\n",
        "    #number of positive units we need to reach in the training sample (we want a balanced sample)\n",
        "    nb_pos_to_drawn = round(nsize*pos_ratio)\n",
        "    lab = label_from_iou(dt_anchors_vs_gtbox, pos_threshold, neg_threshold)\n",
        "    pos_lab = np.where(lab == 1)[0]\n",
        "    neg_lab = np.where(lab == 0)[0]\n",
        "    pos = len(pos_lab)\n",
        "    neg = len(neg_lab)\n",
        "    \n",
        "    if (pos > nb_pos_to_drawn):\n",
        "        disabled_index_pos = np.random.choice(pos_lab, size=(pos - nb_pos_to_drawn), replace = False)\n",
        "        lab[disabled_index_pos] = -1\n",
        "    \n",
        "    if (neg > nsize - nb_pos_to_drawn):\n",
        "        if(pos < nb_pos_to_drawn):\n",
        "            disabled_index_neg = np.random.choice(neg_lab, size=(neg - nsize + pos), replace = False)\n",
        "        else:\n",
        "            disabled_index_neg = np.random.choice(neg_lab, size=(neg + nb_pos_to_drawn - nsize), replace = False)\n",
        "        \n",
        "        lab[disabled_index_neg] = -1    \n",
        "    \n",
        "    res = np.where((lab == 0) | (lab == 1))[0]\n",
        "    return res"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WYbv_w7hKfct"
      },
      "outputs": [],
      "source": [
        "# def batch_training_proposal_RPN(image, feature_shape, ratios, scales,gt_box,nsize = 256, pos_ratio = 0.5, pos_threshold = 0.7, neg_threshold = 0.3):\n",
        "#     #define all anchors using the feature map and the initial picture shapes.\n",
        "#     anchors_boxes = anchor_boxes(ratios, scales, tuple(image.shape[2:]), tuple(feature_shape[2:]))\n",
        "#     #check if each box is inside the initial image\n",
        "#     index_boxes_inside = [j for j in range(len(anchors_boxes)) if check_anchor_inside(anchors_boxes[j], image_torch.shape[2:])]\n",
        "#     anchors_boxes = [anchors_boxes[j] for j in index_boxes_inside]\n",
        "#     #IOU anchors vs gt box\n",
        "#     iou_anc_gt_box = iou_anchors_vs_gtbox(anchors_boxes, gt_box)\n",
        "#     #Index of the units we keep\n",
        "#     ind_for_sample = index_training_proposal(iou_anc_gt_box,nsize, pos_ratio)\n",
        "#     anchors_boxes_reparam,lab_anchors = reparam_all_anchors(anchors_boxes,gt_box,iou_anc_gt_box,pos_threshold, neg_threshold)\n",
        "#     return ({\"image\" : image, \"boxes\" : torch.from_numpy(np.array(anchors_boxes_reparam)[ind_for_sample,:]),\n",
        "#              \"labels\" : torch.from_numpy(lab_anchors[ind_for_sample]), \"indices\" : np.array(index_boxes_inside)[ind_for_sample]})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-12-08T23:12:00.529260Z",
          "start_time": "2021-12-08T23:12:00.515270Z"
        },
        "id": "3VXA4Re5b_7l"
      },
      "outputs": [],
      "source": [
        "#TODO : SUBTOPTIMAL\n",
        "\n",
        "def batch_training_proposal_RPN(image, feature_shape, ratios, scales,gt_box,device, nsize = 256, pos_ratio = 0.5, pos_threshold = 0.7, neg_threshold = 0.3):\n",
        "    #define all anchors using the feature map and the initial picture shapes.\n",
        "    anchors_boxes = anchor_boxes(ratios, scales, tuple(image.shape[2:]), tuple(feature_shape[2:]))\n",
        "    #check if each box is inside the initial image\n",
        "    index_boxes_inside = [j for j in range(len(anchors_boxes)) if check_anchor_inside(anchors_boxes[j], image_torch.shape[2:])]\n",
        "    anchors_boxes_checked = [anchors_boxes[j] for j in index_boxes_inside]\n",
        "    #IOU anchors vs gt box\n",
        "    iou_anc_gt_box = iou_anchors_vs_gtbox(anchors_boxes, gt_box)\n",
        "    iou_anc_gt_box_check = iou_anc_gt_box[index_boxes_inside,:]\n",
        "    #Index of the units we keep\n",
        "    ind_for_sample = index_training_proposal(iou_anc_gt_box_check,nsize, pos_ratio)\n",
        "    indice_to_kept = np.array(index_boxes_inside)[ind_for_sample]\n",
        "\n",
        "    anchors_boxes_reparam,lab_anchors = reparam_all_anchors(anchors_boxes,gt_box,iou_anc_gt_box,pos_threshold, neg_threshold)\n",
        "\n",
        "    final_label = np.full(len(anchors_boxes), -1)\n",
        "    final_label[indice_to_kept] = lab_anchors[indice_to_kept]\n",
        "    \n",
        "    #we need initial anchors_boxes for training FastRNN part\n",
        "    anchors_boxes = torch.from_numpy(np.array(anchors_boxes))\n",
        "    return ({\"image\" : image.to(device), \"boxes\" : torch.from_numpy(np.array(anchors_boxes_reparam)).to(device),\n",
        "             \"labels\" : torch.from_numpy(final_label).to(device), \"anchors_initial\" : anchors_boxes})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-12-08T23:12:00.559249Z",
          "start_time": "2021-12-08T23:12:00.531283Z"
        },
        "id": "rg80p3vKb_7l"
      },
      "outputs": [],
      "source": [
        "image_torch = 800*torch.rand((1,3,800,800))\n",
        "feature_torch = torch.rand([1,512,50,50])\n",
        "\n",
        "ratio = [0.5, 1, 2]\n",
        "anchor_scales = [8, 16, 32]\n",
        "\n",
        "gt_box = [np.array([20, 30, 400, 500]), np.array([300, 400, 500, 600])]\n",
        "labels_gt_box = np.array((\"chien\",\"chat\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0MkkoKxfDdWx"
      },
      "outputs": [],
      "source": [
        "def batch_training_proposal_multi_RPN(batch,feature_shape, ratio, scales,device,nsize = 256, pos_ratio = 0.5, pos_threshold = 0.7, neg_threshold = 0.3):\n",
        "  image_torch, dict_label = batch\n",
        "  boxes = [list(np.array(dict_label[j][\"boxes\"])) for j in range(len(dict_label))]  #remove empty one\n",
        "  boxes = [[xywh_to_vertice(box) for box in box_list]for box_list in boxes]\n",
        "\n",
        "  gt_labels = [list(np.array(dict_label[j][\"labels\"])) for j in range(len(dict_label))]  #remove empty one\n",
        "  \n",
        "  res = [batch_training_proposal_RPN(image_torch[j].unsqueeze(0), feature_shape, ratio, anchor_scales, boxes[j],device,nsize, pos_ratio, pos_threshold, neg_threshold) for j in range(len(boxes)) if len(boxes[j]) != 0]\n",
        "  \n",
        "  ind_with_box = torch.tensor([i for i in range(len(boxes)) if len(boxes[i]) != 0])\n",
        "  image_torch = torch.index_select(image_torch,0, ind_with_box)\n",
        "  boxes_fin = torch.stack([elem[\"boxes\"] for elem in res])\n",
        "  labels = torch.stack([elem[\"labels\"] for elem in res])\n",
        "  anchors = torch.stack([elem[\"anchors_initial\"] for elem in res])\n",
        "\n",
        "  return {\"images\" : image_torch.to(device), \"boxes\": boxes_fin.to(device), \"labels\": labels.to(device),\"gt_box\": boxes, \"gt_labels\": gt_labels, \"anchors\": anchors}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QkGNzP5pb_7m"
      },
      "source": [
        "# Preparation of the proposed region to train the Fast RCNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-12-08T23:12:01.750681Z",
          "start_time": "2021-12-08T23:12:01.737311Z"
        },
        "id": "EJeoQrA2b_7m"
      },
      "outputs": [],
      "source": [
        "def clip_predicted_boxes(list_box, th_min, th_max):\n",
        "    list_box = np.array(list_box)\n",
        "    return list(np.clip(list_box,th_min,th_max))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-12-08T23:12:01.766679Z",
          "start_time": "2021-12-08T23:12:01.752679Z"
        },
        "id": "sX5nhvlVb_7m"
      },
      "outputs": [],
      "source": [
        "#remove all boxes with at least the width or the height less that 16\n",
        "def boxes_hw_min(list_box, list_score, min_size = 16):\n",
        "    boxes = np.array(list_box)\n",
        "    height = boxes[:, 2] - boxes[:, 0]\n",
        "    width = boxes[:, 3] - boxes[:, 1]\n",
        "    box_kept = np.where((height > min_size) & (width > min_size))[0]\n",
        "    list_box_kept = list(boxes[box_kept])\n",
        "    list_score = [list_score[j] for j in box_kept]\n",
        "    return list_box_kept, list_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-12-08T23:12:01.782680Z",
          "start_time": "2021-12-08T23:12:01.768698Z"
        },
        "id": "l4n3c2yab_7m"
      },
      "outputs": [],
      "source": [
        "def nms(list_box, list_score, top_pre, top_post, thresold):\n",
        "    list_score = np.array(list_score)\n",
        "    order = list_score.argsort()[::-1]\n",
        "    order = order[:top_pre]\n",
        "    keep = []\n",
        "    list_box = np.array(list_box)\n",
        "    \n",
        "    ym = list_box[:,0]\n",
        "    xm = list_box[:,1]\n",
        "    yp = list_box[:,2]\n",
        "    xp = list_box[:,3]\n",
        "    areas = (xp - xm + 1) * (yp - ym + 1)\n",
        "\n",
        "    while len(order)>0:\n",
        "        i = order[0]\n",
        "        yym = np.maximum(ym[i], ym[order[1:]])\n",
        "        xxm = np.maximum(xm[i], xm[order[1:]])\n",
        "        yyp = np.minimum(yp[i], yp[order[1:]])\n",
        "        xxp = np.minimum(xp[i], xp[order[1:]])\n",
        "        \n",
        "        width = np.maximum(0.0, xxp - xxm + 1)\n",
        "        height = np.maximum(0.0, yyp - yym + 1)\n",
        "        intersection = width*height\n",
        "        ovr = intersection/(areas[i] + areas[order[1:]] - intersection)\n",
        "        \n",
        "        ind_to_keep = np.where(ovr <= thresold)[0]\n",
        "        order = order[ind_to_keep + 1]\n",
        "        keep.append(i)\n",
        "    \n",
        "    keep = keep[:top_post]\n",
        "    return(list_box[keep,:])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cp5YhD2Qb_7m"
      },
      "source": [
        "# Region proposal network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-12-08T23:12:44.975470Z",
          "start_time": "2021-12-08T23:12:44.966089Z"
        },
        "id": "9zJP45j1b_7m"
      },
      "outputs": [],
      "source": [
        "#boxes as tensor [N, 5]\n",
        "def roi_pooling(boxes, feature_map,scale,adaptative_max_pool):\n",
        "    boxes_coord = boxes[:,1:].mul(scale).long() #scale + round\n",
        "    res = [feature_map.narrow(0, boxes[i,0].int(),1)[..., boxes_coord[i,1]:(boxes_coord[i,3]+1), boxes_coord[i,0]:(boxes_coord[i,2]+1)] for i in range(boxes_coord.shape[0])]\n",
        "    res = [adaptative_max_pool(element) for element in res]\n",
        "    res = torch.cat(res, axis = 0)\n",
        "    return(res)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-12-08T23:12:53.742770Z",
          "start_time": "2021-12-08T23:12:53.724769Z"
        },
        "id": "kwKmwLDsb_7m"
      },
      "outputs": [],
      "source": [
        "#0 in labels_gt_box must be the background\n",
        "def batch_training_proposal_FastRCNN(feature_map,list_box,list_gt_box,labels_gt_box, nsize = 128, pos_ratio = 0.25, pos_iou_threshold = 0.5,\n",
        "                                    neg_iou_threshold_p = 0.5, neg_iou_threshold_n = 0.0, adaptative_max_pool = torch.nn.AdaptiveMaxPool2d((7,7),return_indices=False),scale = 1):\n",
        "\n",
        "    #number of positive units we need to reach in the training sample (we want a balanced sample)\n",
        "    nb_pos_to_drawn = round(nsize*pos_ratio)\n",
        "    iou = iou_anchors_vs_gtbox(list_box, list_gt_box)\n",
        "    #compute the maximum for each anchor\n",
        "    gt_roi_label = np.argmax(iou, axis = 1)\n",
        "    gt_roi_max = np.max(iou, axis = 1)\n",
        "    labels = labels_gt_box[gt_roi_label]\n",
        "    #assign the label if greater that pos_iou_threshold\n",
        "    #assign background if between the two negative thresholds\n",
        "    gt_pos = np.where((gt_roi_max > pos_iou_threshold))[0]\n",
        "    gt_neg = np.where((gt_roi_max < neg_iou_threshold_p) & (gt_roi_max > neg_iou_threshold_n))[0] #background -- 0\n",
        "\n",
        "    #Nb of positives and negatives boxes get using the thresholds\n",
        "    pos = len(gt_pos)\n",
        "    neg = len(gt_neg)\n",
        "    \n",
        "    enable_index_pos = np.array([],dtype=int)\n",
        "\n",
        "    #Subsampling from it\n",
        "    if (pos > nb_pos_to_drawn):\n",
        "        disabled_index_pos = np.random.choice(range(len(gt_pos)), size=(pos - nb_pos_to_drawn), replace = False)\n",
        "        gt_pos = np.delete(gt_pos, disabled_index_pos)\n",
        "\n",
        "    if (neg > nsize - nb_pos_to_drawn):\n",
        "#         if(pos < nb_pos_to_drawn):\n",
        "#             print(neg)\n",
        "#             print(pos)\n",
        "#             print(nsize)\n",
        "#             disabled_index_neg = np.random.choice(range(len(gt_neg)), size=(neg - nsize + pos), replace = False)\n",
        "#             gt_neg = np.delete(gt_neg, disabled_index_neg)\n",
        "#         else:\n",
        "        disabled_index_neg = np.random.choice(range(len(gt_neg)), size=(neg + nb_pos_to_drawn - nsize), replace = False)\n",
        "        gt_neg = np.delete(gt_neg, disabled_index_neg)\n",
        "\n",
        "    if(neg + pos < nsize):\n",
        "        enable_index_pos = np.random.choice(np.append(gt_pos,gt_neg),size = nsize - neg - pos, replace = True)\n",
        "        \n",
        "    \n",
        "        \n",
        "    \n",
        "    #if negative : assign background labels with it's 0\n",
        "    labels[gt_neg] = 0\n",
        "    final_index = np.append(np.append(gt_pos,gt_neg),enable_index_pos)\n",
        "    #Non reparams\n",
        "    non_reparam = np.array(list_box)[final_index,:]\n",
        "    #Need to transform from yxhw to xywh\n",
        "    non_reparam = non_reparam[:,(1,0,3,2)]\n",
        "    non_reparam = np.hstack((np.zeros((non_reparam.shape[0],1)), non_reparam))\n",
        "    non_reparam = torch.from_numpy(non_reparam)\n",
        "\n",
        "    data_for_training = roi_pooling(non_reparam, feature_map,scale, adaptative_max_pool)\n",
        "    #Reparams\n",
        "    reparam = [loc(box,gt_box) for box,gt_box in zip(non_reparam, list(np.array(list_gt_box)[gt_roi_label,:]))]\n",
        "    \n",
        "    return  data_for_training, reparam,labels[final_index]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oEAZuBOiX_CF"
      },
      "source": [
        "# Load train dataset \n",
        "\n",
        "J'ai rÃ©ussi Ã  utiliser l'API Coco via `torchvision.datasets.CocoDetection` (https://pytorch.org/vision/stable/datasets.html)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kc-SDR8e-P0z",
        "outputId": "6cc2db42-b751-42d5-ccbb-f5becc4557e2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2021-12-17 01:23:35--  https://conservancy.umn.edu/bitstream/handle/11299/214865/dataset.zip?sequence=12&isAllowed=y\n",
            "Resolving conservancy.umn.edu (conservancy.umn.edu)... 128.101.122.105\n",
            "Connecting to conservancy.umn.edu (conservancy.umn.edu)|128.101.122.105|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 200\n",
            "Length: 553029970 (527M) [application/zip]\n",
            "Saving to: âdataset.zip?sequence=12&isAllowed=y.3â\n",
            "\n",
            "dataset.zip?sequenc 100%[===================>] 527.41M  14.5MB/s    in 37s     \n",
            "\n",
            "2021-12-17 01:24:13 (14.4 MB/s) - âdataset.zip?sequence=12&isAllowed=y.3â saved [553029970/553029970]\n",
            "\n",
            "Archive:  dataset.zip?sequence=12&isAllowed=y\n",
            "replace dataset/README.txt? [y]es, [n]o, [A]ll, [N]one, [r]ename: ^C\n"
          ]
        }
      ],
      "source": [
        "#Permet d'utiliser la co des serveurs Google (rip la mienne) et assure une meilleure reproductibilitÃ©\n",
        "!wget \"https://conservancy.umn.edu/bitstream/handle/11299/214865/dataset.zip?sequence=12&isAllowed=y\"\n",
        "!unzip \"dataset.zip?sequence=12&isAllowed=y\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XuMoQmEAVwfs"
      },
      "outputs": [],
      "source": [
        "from torchvision import datasets\n",
        "from torch.utils.data import DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L6-SecLQN6hP"
      },
      "outputs": [],
      "source": [
        "# The directory containing the source images\n",
        "data_path = \"dataset/instance_version/train\"\n",
        "\n",
        "# The path to the COCO labels JSON file\n",
        "labels_path = \"dataset/instance_version/instances_train_trashcan.json\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "23FFr0UOSPpG"
      },
      "source": [
        "#### Version 4 - resize des images, en ne gardant que les bbox et category_id des targets normalisÃ©es, dans un array de dictionnaires (targets de taille variables)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oRookaleStv6"
      },
      "outputs": [],
      "source": [
        "#Attention \"bbox\": [x,y,width,height]\n",
        "class CocoDetection_diy_bis(data.Dataset) :\n",
        "    \"\"\"`MS Coco Detection <http://mscoco.org/dataset/#detections-challenge2016>`_ Dataset.\n",
        "\n",
        "    Args:\n",
        "        root (string): Root directory where images are downloaded to.\n",
        "        annFile (string): Path to json annotation file.\n",
        "        resize : (int,int) size of the images wanted \n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, root, annFile, size, elements_index):\n",
        "        from pycocotools.coco import COCO\n",
        "        self.root = root\n",
        "        self.coco = COCO(annFile)\n",
        "        self.total_ids = list(self.coco.imgs.keys())\n",
        "        self.ids = [self.total_ids[j] for j in elements_index]\n",
        "        self.size = size\n",
        "        self.transform = transforms.Compose([transforms.Resize(size), transforms.ToTensor()])\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            index (int): Index\n",
        "\n",
        "        Returns:\n",
        "            tuple: Tuple (image, target). target is the object returned by ``coco.loadAnns``.\n",
        "        \"\"\"\n",
        "        coco = self.coco\n",
        "        img_id = self.ids[index]\n",
        "        ann_ids = coco.getAnnIds(imgIds=img_id)\n",
        "        target = coco.loadAnns(ann_ids)\n",
        "\n",
        "        path = coco.loadImgs(img_id)[0]['file_name']\n",
        "\n",
        "        # Resize des images :\n",
        "        img = Image.open(os.path.join(self.root, path)).convert('RGB')\n",
        "        original_size = img.size\n",
        "        img = self.transform(img)\n",
        "\n",
        "        # Targets dict :\n",
        "        targets = {'labels':[],'boxes':[]}\n",
        "\n",
        "        for elem in target :  \n",
        "          box = elem['bbox']\n",
        "          box[0] *= self.size[0] / original_size[0]\n",
        "          box[1] *= self.size[1] / original_size[1]\n",
        "          box[2] *= self.size[0] /original_size[0]\n",
        "          box[3] *= self.size[1] /  original_size[1]\n",
        "          targets['boxes'].append(box)\n",
        "          targets['labels'].append(elem['category_id'])\n",
        "\n",
        "        return img, targets\n",
        "        \n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.ids)\n",
        "\n",
        "    def __repr__(self):\n",
        "        fmt_str = 'Dataset ' + self.__class__.__name__ + '\\n'\n",
        "        fmt_str += '    Number of datapoints: {}\\n'.format(self.__len__())\n",
        "        fmt_str += '    Root Location: {}\\n'.format(self.root)\n",
        "        tmp = '    Transforms (if any): '\n",
        "        fmt_str += '{0}{1}\\n'.format(tmp, self.transform.__repr__().replace('\\n', '\\n' + ' ' * len(tmp)))\n",
        "        tmp = '    Target Transforms (if any): '\n",
        "        fmt_str += '{0}{1}'.format(tmp, self.target_transform.__repr__().replace('\\n', '\\n' + ' ' * len(tmp)))\n",
        "        return fmt_str"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y2652baDVnV8"
      },
      "outputs": [],
      "source": [
        "def collate_fn_diy (batch) : \n",
        "    \"\"\"\n",
        "    Parameters : \n",
        "    -----------\n",
        "    batch : list of tuples (img,targets)\n",
        "\n",
        "    Return : \n",
        "    -------\n",
        "    images : tensor of dim batch_size x 3 x 224 x 224\n",
        "    targets : list of dict containing : \n",
        "        - \"labels\": Tensor of dim [num_target_boxes] (where num_target_boxes is the number of ground-truth objects in the target) containing the class labels\n",
        "        - \"boxes\": Tensor of dim [num_target_boxes, 4] containing the target box coordinates\n",
        "    \"\"\"\n",
        "    imgs, trgts = list(zip(*batch)) # imgs et trgts sont dÃ©sormais des batch_size-tuples \n",
        "\n",
        "    imgs = [img.unsqueeze(0) for img in list(imgs)] #ajout d'une dimension supplÃ©mentaire Ã  tous les tenseurs\n",
        "    images = torch.cat(imgs) # concatÃ©nation en un seul tenseur\n",
        "\n",
        "    targets = []\n",
        "    for t in list(trgts) : \n",
        "      targets.append({'labels' : torch.from_numpy(np.array(t[\"labels\"])), \n",
        "                      'boxes' : torch.from_numpy(np.array(t[\"boxes\"]))})\n",
        "    \n",
        "    return images, targets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X7HBNfbHGA9z"
      },
      "source": [
        "\n",
        "TODO : que faire lorsqu'on a des images sans box et sans label donc ?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ah1baurMb_7n"
      },
      "source": [
        "# Training RPN network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-12-08T23:13:37.687322Z",
          "start_time": "2021-12-08T23:13:37.348201Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 83,
          "referenced_widgets": [
            "dd231e88790d42bea6867aa556651c8e",
            "7ff93690eff6496ca0eaff04af2d197e",
            "5d702123da9e48129ff7557a02f72b99",
            "5dfa0e0da40b4ddda4e7dd26a4243622",
            "f77ce4b2ae214c80984ba0b4fa2d2c14",
            "6eefd4e548024038b8e50141bdeef898",
            "079761f87ba8483e9ccf4b97da879ef1",
            "f0bcc1811e5b4fb49be83fd1bf52c557",
            "04e8eac584c947f8a4dd2e7ae9721dd7",
            "e9e980dc151b4fc3a242255c7f92dd34",
            "9ac6009437a84a5d8868dbfa7a209924"
          ]
        },
        "id": "6hNzYE92b_7o",
        "outputId": "86fd9ee3-f3ee-4130-fa6b-c55855a86bc3"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([1, 256, 200, 200])"
            ]
          },
          "execution_count": 37,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from torchvision import models\n",
        "\n",
        "resnet50 = models.resnet50(pretrained=True)\n",
        "image_torch = 800*torch.rand((1,3,800,800))\n",
        "#We choose the place where we extracted the feature map in order to get H_feature * W_feature around 2400 (papers)\n",
        "resnet50_features = nn.Sequential(*(list(resnet50.children())[:-5]))\n",
        "resnet50_features(image_torch).shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pm6g59e7b_7p"
      },
      "source": [
        "https://stackoverflow.com/questions/69480764/what-is-the-difference-between-resnet50-vgg16-etc-and-rcnn-faster-rcnn\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-12-08T23:13:41.640996Z",
          "start_time": "2021-12-08T23:13:41.621664Z"
        },
        "code_folding": [],
        "id": "EYl81Phib_7p"
      },
      "outputs": [],
      "source": [
        "class RPN(nn.Module):\n",
        "    #TODO : remove embedding_dim using wv.shape[1]\n",
        "    #define all the layers used in model\n",
        "    def __init__(self,mid_channels, in_channels,nb_anchors,pre_trained_model):\n",
        "        \n",
        "        #Constructor\n",
        "        super().__init__()        \n",
        "        \n",
        "        #embedding layer\n",
        "        self.pre_trained_model = pre_trained_model\n",
        "        self.conv1 = nn.Conv2d(in_channels, mid_channels, 3, 1, 1)\n",
        "        self.conv1.weight.data.normal_(0, 0.01)\n",
        "        self.conv1.bias.data.zero_()\n",
        "        self.sm = nn.Softmax(dim = 2)\n",
        "        self.reg_layer = nn.Conv2d(mid_channels, nb_anchors *4, 1, 1, 0)\n",
        "        self.reg_layer.weight.data.normal_(0, 0.01)\n",
        "        self.reg_layer.bias.data.zero_()\n",
        "\n",
        "        self.cls_layer = nn.Conv2d(mid_channels, nb_anchors *2, 1, 1, 0)\n",
        "        # classification layer\n",
        "        self.cls_layer.weight.data.normal_(0, 0.01)\n",
        "        self.cls_layer.bias.data.zero_()\n",
        "       \n",
        "\n",
        "    def forward(self,img):\n",
        "        nb_images = img.shape[0] #sale ?\n",
        "        x = self.pre_trained_model(img)\n",
        "        x = self.conv1(x)\n",
        "        pred_anchor = self.reg_layer(x)\n",
        "        pred_anchor = pred_anchor.permute(0, 2, 3, 1).contiguous().view(nb_images, -1, 4)\n",
        "        \n",
        "        pred_cls = self.cls_layer(x)\n",
        "        pred_cls = pred_cls.permute(0, 2, 3, 1).contiguous()\n",
        "        pred_cls = pred_cls.view(nb_images, -1, 2)\n",
        "        #pred_cls = self.sm(pred_cls)\n",
        "        return pred_anchor, pred_cls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "koHem-H00uDv"
      },
      "outputs": [],
      "source": [
        "def l1smooth(pred,target,nb_anchors = 9):\n",
        "  #zeros for those which are negative\n",
        "  boxes = ((target[\"labels\"] == 1).unsqueeze(-1).repeat(1,1,4).float() * target['boxes'])\n",
        "  pred = ((target[\"labels\"] == 1).unsqueeze(-1).repeat(1,1,4).float() * pred[0])\n",
        "  x = torch.abs(boxes.flatten() - pred.flatten())\n",
        "  return sum(0.5*(x**2)*(x < 1) + (x - 0.5)*(x >= 1)) / nb_anchors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aj4IJlRQ6xGf"
      },
      "outputs": [],
      "source": [
        "def RPN_loss(pred,target,feature_shape, loss_ce = nn.CrossEntropyLoss(ignore_index=-1),loss_sml1 = nn.SmoothL1Loss(reduction = \"sum\"),  lamb = 10):\n",
        "  #Remove all -1 for the computation of the CE loss\n",
        "  #Classification part\n",
        "  loss_cls = loss_ce(pred[1].view(-1,2),target['labels'].view(-1,1).squeeze().long())\n",
        "  #Regression part\n",
        "  boxes = ((target[\"labels\"] == 1).unsqueeze(-1).repeat(1,1,4).float() * target['boxes'])\n",
        "  pred = ((target[\"labels\"] == 1).unsqueeze(-1).repeat(1,1,4).float() * pred[0])\n",
        "  loss_reg = loss_sml1(boxes, pred)/torch.prod(feature_shape[2:])\n",
        "  #Final loss\n",
        "  loss = loss_cls + lamb*loss_reg\n",
        "  return loss\n",
        "\n",
        "def RPN_loss2(pred,target, nb_anchors = 56*56, lamb = 10):\n",
        "  #Remove all -1 for the computation of the CE loss\n",
        "  #Classification part\n",
        "  loss_ce = nn.CrossEntropyLoss(ignore_index=-1)\n",
        "  loss_cls = loss_ce(pred[1].view(-1,2),target['labels'].view(-1,1).squeeze().long())\n",
        "  #Regression part\n",
        "  loss_reg = l1smooth(pred, target, nb_anchors)\n",
        "  #Final loss\n",
        "  loss = loss_cls + lamb*loss_reg\n",
        "  return loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q3po-CtKr4v7"
      },
      "source": [
        "The new version of RPN is at least 8 time faster."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Qnn6nE28UI3"
      },
      "source": [
        "As indicated in the paper, we are using SGD optimiser with 0.001 as learning rate and 0.9 for momemtum"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SananpqIs0XB"
      },
      "outputs": [],
      "source": [
        "def fn_train_step_RPN(model, loss_fn, optimizer,feature_shape = torch.tensor((1,1,56,56))):\n",
        "  def train_step(data_loader):\n",
        "    model.train()\n",
        "    losses = 0\n",
        "    for input in data_loader:\n",
        "      input = batch_training_proposal_multi_RPN(input, feature_shape, ratio, anchor_scales, device)\n",
        "      output = model_RPN(input[\"images\"])\n",
        "\n",
        "      loss = RPN_loss(output, input,feature_shape)\n",
        "      loss.backward()\n",
        "\n",
        "      optimizer.step()\n",
        "      optimizer.zero_grad()\n",
        "\n",
        "      losses =+ loss.item()\n",
        "\n",
        "    return losses/len(data_loader)\n",
        "\n",
        "  return train_step"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sHG3LKTZvitA"
      },
      "source": [
        "## Split the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "87JcTZ2Duw10"
      },
      "outputs": [],
      "source": [
        "prop_train = 0.5\n",
        "prop_valid = 0.3\n",
        "prop_test = 0.2\n",
        "\n",
        "n = 6000\n",
        "n_train = int(n*prop_train)\n",
        "n_test = int(n*prop_test)\n",
        "n_valid = n - n_train- n_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "95X2zfsgveaS"
      },
      "outputs": [],
      "source": [
        "ind_n_train = np.random.choice(np.arange(n), n_train)\n",
        "ind_n_test = np.random.choice(np.setdiff1d(np.arange(n), ind_n_train), n_test)\n",
        "ind_n_valid = np.setdiff1d(np.setdiff1d(np.arange(n), ind_n_train),ind_n_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wNd1lcYOudBZ",
        "outputId": "ecc46dea-efd5-4828-8575-1facc549678c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "loading annotations into memory...\n",
            "Done (t=0.35s)\n",
            "creating index...\n",
            "index created!\n",
            "loading annotations into memory...\n",
            "Done (t=0.31s)\n",
            "creating index...\n",
            "index created!\n",
            "loading annotations into memory...\n",
            "Done (t=0.40s)\n",
            "creating index...\n",
            "index created!\n"
          ]
        }
      ],
      "source": [
        "#train Dataloader\n",
        "instances_train = CocoDetection_diy_bis(root = data_path, annFile = labels_path, size=(224,224), elements_index = ind_n_train)\n",
        "instances_train_dataloader = DataLoader(instances_train, batch_size=5, shuffle=True, collate_fn = collate_fn_diy)\n",
        "\n",
        "#valid dataloader\n",
        "instances_valid = CocoDetection_diy_bis(root = data_path, annFile = labels_path, size=(224,224), elements_index = ind_n_valid)\n",
        "instances_valid_dataloader = DataLoader(instances_valid, batch_size=5, shuffle=True, collate_fn = collate_fn_diy)\n",
        "\n",
        "\n",
        "#test dataloader\n",
        "instances_test = CocoDetection_diy_bis(root = data_path, annFile = labels_path, size=(224,224), elements_index = ind_n_test)\n",
        "instances_test_dataloader = DataLoader(instances_test, batch_size=5, shuffle=True, collate_fn = collate_fn_diy)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TSnXqSR-8TU6"
      },
      "outputs": [],
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "feature_shape = (1,1,56,56)\n",
        "model_RPN = RPN(256,256,9,resnet50_features).to(device)\n",
        "optimizer = optim.SGD(model_RPN.parameters(), lr=0.001, momentum=0.9)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o4_bCnb8CFZl",
        "outputId": "0b2efcdf-d827-4c44-a93d-3a4feb4d2ce9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\tEpoch 0 : Train Loss: 0.001 | \tValid Loss: 0.001\n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "#TODO : save better model ! on valid ?\n",
        "t0 = time.time()\n",
        "loss_list_train = [float('inf')]\n",
        "loss_list_valid = [float('inf')]\n",
        "nb_epoch = 10\n",
        "i = 0\n",
        "\n",
        "best_model = RPN(256,256,9,resnet50_features).to(device)\n",
        "\n",
        "for epoch in range(nb_epoch):\n",
        "  #training\n",
        "  model_RPN.train()\n",
        "  losses = 0\n",
        "  for element in instances_train_dataloader:\n",
        "    input = batch_training_proposal_multi_RPN(element, feature_shape, ratio, anchor_scales, device)\n",
        "    output = model_RPN(input[\"images\"])\n",
        "    loss = RPN_loss(output, input,torch.tensor(feature_shape))\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    optimizer.zero_grad()\n",
        "    losses =+ loss.item()\n",
        "  loss_epch = losses/len(instances_train_dataloader)\n",
        "  loss_list_train.append(loss_epch)\n",
        "  #valid\n",
        "  with torch.no_grad():\n",
        "    losses = 0\n",
        "    for element in instances_valid_dataloader:\n",
        "      input = batch_training_proposal_multi_RPN(element, feature_shape, ratio, anchor_scales, device)\n",
        "      output = model_RPN(input[\"images\"])\n",
        "      model_RPN.eval()\n",
        "      loss = RPN_loss(output, input, torch.tensor(feature_shape))\n",
        "      losses =+ loss.item()\n",
        "    loss_epch_valid = losses/len(instances_valid_dataloader)\n",
        "    if loss_epch_valid < min(loss_list_valid):\n",
        "        best_model.state_dict(copy.deepcopy(model_RPN.state_dict()))\n",
        "        #Ajoutez des mÃ©triques\n",
        "    loss_list_valid.append(loss_epch_valid)\n",
        "    print(f'\\tEpoch {i} : Train Loss: {loss_epch:.3f} | \\tValid Loss: {loss_epch_valid:.3f}')\n",
        "  i += 1\n",
        " \n",
        "\n",
        "t1 = time.time()\n",
        "\n",
        "total = t1-t0\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wFuXwXYi51eh"
      },
      "outputs": [],
      "source": [
        "torch.save(model_RPN.state_dict(), \"model_RPN.pt\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R2hA9uAT51eh"
      },
      "outputs": [],
      "source": [
        "exemple = next(iter(instances_train_dataloader))\n",
        "exemple = batch_training_proposal_multi_RPN(exemple, feature_shape, ratio, anchor_scales, device)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ovv1JwXg51eh"
      },
      "outputs": [],
      "source": [
        "exemple[\"gt_box\"][1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w6kJvdpm51eh"
      },
      "outputs": [],
      "source": [
        "model_RPN(exemple[\"images\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3kVHAhCJ51ei"
      },
      "source": [
        "## Transition between RPN and FastRCNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ISPGUqka51ei"
      },
      "outputs": [],
      "source": [
        "instances_train = CocoDetection_diy_bis(root = data_path, annFile = labels_path, size=(224,224), elements_index = ind_n_train)\n",
        "instances_train_dataloader = DataLoader(instances_train, batch_size=1, shuffle=True, collate_fn = collate_fn_diy)\n",
        "exemple =  batch_training_proposal_multi_RPN(next(iter(instances_train_dataloader)), feature_shape, ratio, anchor_scales, device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0IihDfwk51ei"
      },
      "outputs": [],
      "source": [
        "pre_trained_model_after_RPN = model_RPN._modules[\"pre_trained_model\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A_c_Tqns51ei"
      },
      "outputs": [],
      "source": [
        "#exemple batch containing a single exemple\n",
        "sm = nn.Softmax(dim = 2)\n",
        "def RPN_to_FRCNN(exemple, feature_size, min_size_anchors, before_top_nms,after_top_nms,pos_t, model, feature_extractor):\n",
        "    res = model(exemple[\"images\"])\n",
        "    prob = sm(res[0])[0,:,1].detach().cpu().numpy()\n",
        "    anchors = list(np.array(exemple[\"anchors\"][0].cpu()))\n",
        "    list_box = list((res[0][0].detach().cpu()).numpy())\n",
        "    boxes_hw = boxes_hw_min(clip_predicted_boxes(deloc_list(anchors,list_box),0,feature_size),prob,min_size_anchors)\n",
        "    list_box = list(nms(boxes_hw[0], boxes_hw[1], before_top_nms,after_top_nms,pos_t))\n",
        "    return {\"feature_map\" : feature_extractor(exemple[\"images\"]), \"list_roi\" : list_box, \"list_gt_labels\" : np.array(exemple[\"gt_labels\"][0]), \"list_gt_box\" : list(exemple[\"gt_box\"][0])}\n",
        "\n",
        "after_rpn = RPN_to_FRCNN(exemple, 224,16,12000,1000,0.7,model_RPN, pre_trained_model_after_RPN)\n",
        "\n",
        "res = batch_training_proposal_FastRCNN(after_rpn[\"feature_map\"],after_rpn[\"list_roi\"],after_rpn[\"list_gt_box\"],after_rpn[\"list_gt_labels\"], nsize = 128, pos_ratio = 0.25, pos_iou_threshold = 0.5,\n",
        "                                adaptative_max_pool = torch.nn.AdaptiveMaxPool2d((7,7),return_indices=False), scale =1/4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z95lxjRx51ei"
      },
      "outputs": [],
      "source": [
        "res[0].view(256,-1).shape"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "anchor_boxes.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": true,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": false
    },
    "toc-autonumbering": true,
    "toc-showmarkdowntxt": false,
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": false
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "04e8eac584c947f8a4dd2e7ae9721dd7": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "079761f87ba8483e9ccf4b97da879ef1": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5d702123da9e48129ff7557a02f72b99": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_079761f87ba8483e9ccf4b97da879ef1",
            "placeholder": "â",
            "style": "IPY_MODEL_6eefd4e548024038b8e50141bdeef898",
            "value": "100%"
          }
        },
        "5dfa0e0da40b4ddda4e7dd26a4243622": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_04e8eac584c947f8a4dd2e7ae9721dd7",
            "max": 102530333,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f0bcc1811e5b4fb49be83fd1bf52c557",
            "value": 102530333
          }
        },
        "6eefd4e548024038b8e50141bdeef898": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7ff93690eff6496ca0eaff04af2d197e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9ac6009437a84a5d8868dbfa7a209924": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dd231e88790d42bea6867aa556651c8e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_5d702123da9e48129ff7557a02f72b99",
              "IPY_MODEL_5dfa0e0da40b4ddda4e7dd26a4243622",
              "IPY_MODEL_f77ce4b2ae214c80984ba0b4fa2d2c14"
            ],
            "layout": "IPY_MODEL_7ff93690eff6496ca0eaff04af2d197e"
          }
        },
        "e9e980dc151b4fc3a242255c7f92dd34": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f0bcc1811e5b4fb49be83fd1bf52c557": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f77ce4b2ae214c80984ba0b4fa2d2c14": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9ac6009437a84a5d8868dbfa7a209924",
            "placeholder": "â",
            "style": "IPY_MODEL_e9e980dc151b4fc3a242255c7f92dd34",
            "value": " 97.8M/97.8M [00:01&lt;00:00, 97.8MB/s]"
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}