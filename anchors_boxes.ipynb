{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6XSixqNNb_7Z",
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Preparation-of-the-proposed-region-to-train-the-Fast-RCNN\" data-toc-modified-id=\"Preparation-of-the-proposed-region-to-train-the-Fast-RCNN-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Preparation of the proposed region to train the Fast RCNN</a></span></li><li><span><a href=\"#Region-proposal-network\" data-toc-modified-id=\"Region-proposal-network-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Region proposal network</a></span></li><li><span><a href=\"#Training-RPN-network\" data-toc-modified-id=\"Training-RPN-network-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Training RPN network</a></span></li><li><span><a href=\"#Coco-dataset\" data-toc-modified-id=\"Coco-dataset-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Coco dataset</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-gZieto6b_7d"
   },
   "source": [
    "This notebook aims to provide functions that produce anchor boxes as decribed in the paper."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0YTdXFc3b_7e"
   },
   "source": [
    "A box will be describe either as a numpy array $[y^-, x^-, y^+, x^+]$  or as a numpy array $[c_y, c_x, h,w]$\n",
    "\n",
    "TODO : CHECK +1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A8hRjHO551eS",
    "outputId": "304eb51a-d031-49f9-d969-7cad5183a024"
   },
   "outputs": [],
   "source": [
    "!pip3 install torch torchvision torchaudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tya5x2dU51eU",
    "outputId": "efef5f0a-cfcc-4583-ff80-5521706932a3"
   },
   "outputs": [],
   "source": [
    "!pip3 install pycocotools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "ZXgjTh3oAbWt"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches #In order to draw the box ! (je sais pas pourquoi j'Ã©cris en anglais)\n",
    "from torchvision import models\n",
    "import torch.utils.data as data\n",
    "from PIL import Image\n",
    "import os\n",
    "import os.path\n",
    "import torchvision\n",
    "import pycocotools\n",
    "import copy\n",
    "from PIL import Image, ImageFont, ImageDraw, ImageEnhance\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "CUDA_LAUNCH_BLOCKING=1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_EPOCH = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gEMPqMqMrBsn"
   },
   "source": [
    "# Anchors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-08T23:12:00.294254Z",
     "start_time": "2021-12-08T23:12:00.281183Z"
    },
    "id": "RtROAiS1b_7f"
   },
   "outputs": [],
   "source": [
    "def vertice_to_yxhw(anchor):\n",
    "    res = (np.mean((anchor[0],anchor[2])),np.mean((anchor[1],anchor[3])), anchor[2] - anchor[0] + 1, anchor[3] - anchor[1]+1)\n",
    "    return np.array(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "QujcnpGi51eW"
   },
   "outputs": [],
   "source": [
    "def vertice_to_yxhw_array(anchor):\n",
    "    res = (np.mean((anchor[:,0],anchor[:,2]), axis = 0),np.mean((anchor[:,1],anchor[:,3]), axis = 0), anchor[:,2] - anchor[:,0] + 1, anchor[:,3] - anchor[:,1]+1)\n",
    "    return np.transpose(np.array(res))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-08T23:12:00.310283Z",
     "start_time": "2021-12-08T23:12:00.296255Z"
    },
    "id": "nzn9r1Dzb_7g"
   },
   "outputs": [],
   "source": [
    "def yxhw_to_vertice(anchor):\n",
    "    res = (anchor[0] - anchor[2]/2, anchor[1] - anchor[3]/2, anchor[0] + anchor[2]/2, anchor[1] + anchor[3]/2)\n",
    "    return np.array(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {
    "id": "4DmF9weOH8Uj"
   },
   "outputs": [],
   "source": [
    "def xywh_to_vertice(anchor):\n",
    "  anchor_perm = (anchor[1] + 0.5*anchor[3] ,anchor[0] + 0.5*anchor[2],anchor[3],anchor[2])\n",
    "  return yxhw_to_vertice(anchor_perm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([100., 100., 140., 120.])"
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xywh_to_vertice((100,100,20,40))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-08T23:12:00.326279Z",
     "start_time": "2021-12-08T23:12:00.312275Z"
    },
    "id": "4NrJLfwrb_7g"
   },
   "outputs": [],
   "source": [
    "def anchor_box(center, ratio, scale, shape_initial, shape_featured):\n",
    "    sub_width = shape_initial[0]/shape_featured[0]\n",
    "    sub_height = shape_initial[1]/shape_featured[1]\n",
    "    anchor_width = sub_width*scale*np.sqrt(ratio)\n",
    "    anchor_height = sub_height*scale/np.sqrt(ratio)\n",
    "    \n",
    "    ym = center[1] - anchor_height/2\n",
    "    yp = center[1] + anchor_height/2\n",
    "    xm = center[0] - anchor_width/2\n",
    "    xp = center[0] + anchor_width/2\n",
    "    \n",
    "    anchor = (ym,xm,yp,xp)\n",
    "    return(anchor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-08T23:12:00.341279Z",
     "start_time": "2021-12-08T23:12:00.328260Z"
    },
    "id": "MEYHa3sPb_7h"
   },
   "outputs": [],
   "source": [
    "def list_centers(shape_initial, shape_featured):\n",
    "    ratio_h = shape_initial[1]/shape_featured[1]\n",
    "    ratio_w = shape_initial[0]/shape_featured[0]\n",
    "    #intiail center is the center at the left top corner\n",
    "    all_centers = [np.array((ratio_w/2, ratio_h/2),dtype=float) + np.array((ratio_w*i, ratio_h*j),dtype=float) for i in range(int(shape_featured[0])) for j in range(int(shape_featured[1]))]\n",
    "    return(all_centers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-08T23:12:00.356279Z",
     "start_time": "2021-12-08T23:12:00.343272Z"
    },
    "id": "2klfx8Fjb_7h"
   },
   "outputs": [],
   "source": [
    "def anchor_boxes(list_ratios, list_scales, shape_initial, shape_featured):\n",
    "    list_center = list_centers(shape_initial, shape_featured)\n",
    "    all_anchors = [anchor_box(center, ratio, scale,shape_initial,shape_featured) for center in list_center for ratio in list_ratios\n",
    "                   for scale in list_scales]\n",
    "    return(all_anchors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-08T23:12:00.372270Z",
     "start_time": "2021-12-08T23:12:00.358270Z"
    },
    "id": "Y1jD1fLqb_7i"
   },
   "outputs": [],
   "source": [
    "def check_anchor_inside(anchor_box, shape_initial):\n",
    "    ym = anchor_box[0]\n",
    "    yp = anchor_box[2]\n",
    "    xm = anchor_box[1]\n",
    "    xp = anchor_box[3]\n",
    "    is_inside = (min(xm,xp)>0) & (max(xm,xp)<shape_initial[0]) & (max(yp,ym) < shape_initial[1]) & (min(ym,yp) > 0) \n",
    "    return(is_inside)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iou_anchors_vs_gtbox(list_anchors, list_gt_box):\n",
    "  t_anchors = torch.tensor(np.array(list_anchors))\n",
    "  t_list_gt_box = torch.tensor(np.array(list_gt_box))\n",
    "  iou = torchvision.ops.box_iou(t_anchors, t_list_gt_box)\n",
    "  return iou.numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iTlLHh_Vb_7j"
   },
   "source": [
    "**TODO** : changer la forme de cette fonction en utilisant que des *arrays*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-08T23:12:00.419270Z",
     "start_time": "2021-12-08T23:12:00.406270Z"
    },
    "id": "B8S6FE2db_7j"
   },
   "outputs": [],
   "source": [
    "#Return an array with :\n",
    "#for all ground truth box, the anchors which maximize the IOU with it\n",
    "#for all anchor, the max of the IOU\n",
    "\n",
    "#the first column of the array is the index and the last the IOU \n",
    "def best_anchors_from_iou(dt_anchors_vs_gtbox):\n",
    "    #index highest by gtbox (cond a)\n",
    "    dt_anchors_vs_gtbox.argmax(axis = 0)\n",
    "    ind_argmax = np.where(dt_anchors_vs_gtbox == dt_anchors_vs_gtbox.max(axis = 0))[0]\n",
    "    cond_a = dt_anchors_vs_gtbox[ind_argmax,:].max(axis = 1)\n",
    "    \n",
    "    #highest by anchors box (cond b)\n",
    "    index = dt_anchors_vs_gtbox.argmax(axis = 1)\n",
    "    iou_max = dt_anchors_vs_gtbox.max(axis = 1)\n",
    "    cond_b = dt_anchors_vs_gtbox[np.arange(dt_anchors_vs_gtbox.shape[0]),index]\n",
    "    \n",
    "    index_res = np.concatenate((ind_argmax,np.arange(dt_anchors_vs_gtbox.shape[0])))\n",
    "    res = np.concatenate((cond_a, cond_b), axis=0)\n",
    "    res = np.column_stack((index_res,res))\n",
    "    return(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-08T23:12:00.434260Z",
     "start_time": "2021-12-08T23:12:00.421270Z"
    },
    "id": "9VqTF3Itb_7j"
   },
   "outputs": [],
   "source": [
    "#label_from_iou returns a np.array containing for each anchor its label. (+1 if foreground, 0 if background and -1 if not used\n",
    "#during the learning phase)\n",
    "#The default thresholds are defined according the original paper about Fatest RCNN.\n",
    "\n",
    "def label_from_iou(dt_anchors_vs_gtbox,pos_threshold = 0.7, neg_threshold = 0.3):\n",
    "    label = np.full(dt_anchors_vs_gtbox.shape[0],-1)\n",
    "    iou_max = dt_anchors_vs_gtbox.max(axis = 1)\n",
    "    #positive labels : 1\n",
    "    label[iou_max > pos_threshold] = 1\n",
    "    #negative labels : 0\n",
    "    label[iou_max < neg_threshold] = 0\n",
    "    sum((iou_max < neg_threshold))\n",
    "\n",
    "    #for anchors whose maximize IOU for a given object : +1\n",
    "    dt_anchors_vs_gtbox.argmax(axis = 0)\n",
    "    ind_argmax = np.where(dt_anchors_vs_gtbox == dt_anchors_vs_gtbox.max(axis = 0))[0]\n",
    "    label[ind_argmax] = 1\n",
    "    return(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-08T23:12:00.449260Z",
     "start_time": "2021-12-08T23:12:00.436260Z"
    },
    "id": "aV_HYx5Ab_7k"
   },
   "outputs": [],
   "source": [
    "def loc(anchor_box, gt_box):\n",
    "    anchor_box = vertice_to_yxhw(anchor_box)\n",
    "    gt_box = vertice_to_yxhw(gt_box)\n",
    "    \n",
    "    y = (gt_box[0] - anchor_box[0])/anchor_box[2]\n",
    "    x = (gt_box[1] - anchor_box[1])/anchor_box[3]\n",
    "    w = np.log(gt_box[3]/anchor_box[3])\n",
    "    h = np.log(gt_box[2]/anchor_box[2])\n",
    "    \n",
    "    return np.array((y,x,h,w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "E_6_fU8Tb-yM"
   },
   "outputs": [],
   "source": [
    "def loc_list(anchors_list, gt_list):\n",
    "    anchor_box = vertice_to_yxhw_array(np.array(anchors_list))\n",
    "    gt_box = vertice_to_yxhw_array(np.array(gt_list))\n",
    "    \n",
    "    y = (gt_box[:,0] - anchor_box[:,0])/anchor_box[:,2]\n",
    "    x = (gt_box[:,1] - anchor_box[:,1])/anchor_box[:,3]\n",
    "    w = np.log(gt_box[:,3]/anchor_box[:,3])\n",
    "    h = np.log(gt_box[:,2]/anchor_box[:,2])\n",
    "    return np.transpose(np.array((y,x,h,w)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-08T23:12:00.465251Z",
     "start_time": "2021-12-08T23:12:00.451274Z"
    },
    "id": "CI9iedq9b_7k"
   },
   "outputs": [],
   "source": [
    "def deloc(anchor_box, reparam_box):\n",
    "    anchor_box = vertice_to_yxhw(anchor_box)\n",
    "    y = anchor_box[0] + (reparam_box[0] * anchor_box[2])\n",
    "    x = anchor_box[1] + (reparam_box[1] * anchor_box[3])\n",
    "    h = np.exp(reparam_box[2])*anchor_box[2]\n",
    "    w = np.exp(reparam_box[3])*anchor_box[3]\n",
    "    return np.array((y,x,h,w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "HdCVJYAb51eb"
   },
   "outputs": [],
   "source": [
    "#anchors : vertice\n",
    "#gt_list : prediction\n",
    "def deloc_list(anchors_list, gt_list):\n",
    "    anchor_box = vertice_to_yxhw_array(np.array(anchors_list))\n",
    "    reparam_box = np.array(gt_list)\n",
    "    \n",
    "    y = anchor_box[:,0] + (reparam_box[:,0] * anchor_box[:,2])\n",
    "    x = anchor_box[:,1] + (reparam_box[:,1] * anchor_box[:,3])\n",
    "    h = np.exp(reparam_box[:,2])*anchor_box[:,2]\n",
    "    w = np.exp(reparam_box[:,3])*anchor_box[:,3]\n",
    "    \n",
    "    #    res = (anchor[0] - anchor[2]/2, anchor[1] - anchor[3]/2, anchor[0] + anchor[2]/2, anchor[1] + anchor[3]/2)\n",
    "    #   res = (np.mean((anchor[0],anchor[2])),np.mean((anchor[1],anchor[3])), anchor[2] - anchor[0] + 1, anchor[3] - anchor[1]+1)\n",
    "\n",
    "    return np.transpose(np.array((y - h/2,x-w/2,y + h/2,x + w/2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-08T23:12:00.481256Z",
     "start_time": "2021-12-08T23:12:00.467253Z"
    },
    "id": "jeMfYSLTb_7k"
   },
   "outputs": [],
   "source": [
    "def reparam_all_anchors(list_anchors, list_gt_box,iou,pos_threshold = 0.7, neg_threshold = 0.3):\n",
    "    index_max_gtbox = iou.argmax(axis = 1)\n",
    "    gt_box_by_anchors = [list_gt_box[i] for i in index_max_gtbox]\n",
    "    #TODO : change suboptimal ZIP\n",
    "    res = loc_list(list_anchors, gt_box_by_anchors)\n",
    "    #compute labels\n",
    "    labels = label_from_iou(iou, pos_threshold, neg_threshold)\n",
    "    return list(res),labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-08T23:12:00.497279Z",
     "start_time": "2021-12-08T23:12:00.483259Z"
    },
    "id": "FSjeRrgIb_7k"
   },
   "outputs": [],
   "source": [
    "def deparam_all_anchors(list_anchors, list_box_param):\n",
    "    res = [(deloc(anchor, box_param)) for anchor,box_param in zip(list_anchors, list_box_param)]\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-08T23:12:00.513279Z",
     "start_time": "2021-12-08T23:12:00.500252Z"
    },
    "id": "sORoyapOb_7l"
   },
   "outputs": [],
   "source": [
    "#TODO : heck how to fill when\n",
    "\n",
    "def index_training_proposal(dt_anchors_vs_gtbox, nsize = 256, pos_ratio = 0.5,pos_threshold = 0.7, neg_threshold = 0.3):\n",
    "    #number of positive units we need to reach in the training sample (we want a balanced sample)\n",
    "    nb_pos_to_drawn = round(nsize*pos_ratio)\n",
    "    lab = label_from_iou(dt_anchors_vs_gtbox, pos_threshold, neg_threshold)\n",
    "    pos_lab = np.where(lab == 1)[0]\n",
    "    neg_lab = np.where(lab == 0)[0]\n",
    "    pos = len(pos_lab)\n",
    "    neg = len(neg_lab)\n",
    "    \n",
    "    if (pos > nb_pos_to_drawn):\n",
    "        disabled_index_pos = np.random.choice(pos_lab, size=(pos - nb_pos_to_drawn), replace = False)\n",
    "        lab[disabled_index_pos] = -1\n",
    "    \n",
    "    if (neg > nsize - nb_pos_to_drawn):\n",
    "        if(pos < nb_pos_to_drawn):\n",
    "            disabled_index_neg = np.random.choice(neg_lab, size=(neg - nsize + pos), replace = False)\n",
    "        else:\n",
    "            disabled_index_neg = np.random.choice(neg_lab, size=(neg + nb_pos_to_drawn - nsize), replace = False)\n",
    "        \n",
    "        lab[disabled_index_neg] = -1    \n",
    "    \n",
    "    res = np.where((lab == 0) | (lab == 1))[0]\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "WYbv_w7hKfct"
   },
   "outputs": [],
   "source": [
    "# def batch_training_proposal_RPN(image, feature_shape, ratios, scales,gt_box,nsize = 256, pos_ratio = 0.5, pos_threshold = 0.7, neg_threshold = 0.3):\n",
    "#     #define all anchors using the feature map and the initial picture shapes.\n",
    "#     anchors_boxes = anchor_boxes(ratios, scales, tuple(image.shape[2:]), tuple(feature_shape[2:]))\n",
    "#     #check if each box is inside the initial image\n",
    "#     index_boxes_inside = [j for j in range(len(anchors_boxes)) if check_anchor_inside(anchors_boxes[j], image_torch.shape[2:])]\n",
    "#     anchors_boxes = [anchors_boxes[j] for j in index_boxes_inside]\n",
    "#     #IOU anchors vs gt box\n",
    "#     iou_anc_gt_box = iou_anchors_vs_gtbox(anchors_boxes, gt_box)\n",
    "#     #Index of the units we keep\n",
    "#     ind_for_sample = index_training_proposal(iou_anc_gt_box,nsize, pos_ratio)\n",
    "#     anchors_boxes_reparam,lab_anchors = reparam_all_anchors(anchors_boxes,gt_box,iou_anc_gt_box,pos_threshold, neg_threshold)\n",
    "#     return ({\"image\" : image, \"boxes\" : torch.from_numpy(np.array(anchors_boxes_reparam)[ind_for_sample,:]),\n",
    "#              \"labels\" : torch.from_numpy(lab_anchors[ind_for_sample]), \"indices\" : np.array(index_boxes_inside)[ind_for_sample]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-08T23:12:00.529260Z",
     "start_time": "2021-12-08T23:12:00.515270Z"
    },
    "id": "3VXA4Re5b_7l"
   },
   "outputs": [],
   "source": [
    "#TODO : SUBTOPTIMAL\n",
    "\n",
    "def batch_training_proposal_RPN(image, feature_shape, ratios, scales,gt_box,device, nsize = 256, pos_ratio = 0.5, pos_threshold = 0.7, neg_threshold = 0.3):\n",
    "    #define all anchors using the feature map and the initial picture shapes.\n",
    "    anchors_boxes = anchor_boxes(ratios, scales, tuple(image.shape[2:]), tuple(feature_shape[2:]))\n",
    "    #check if each box is inside the initial image\n",
    "    index_boxes_inside = [j for j in range(len(anchors_boxes)) if check_anchor_inside(anchors_boxes[j], image_torch.shape[2:])]\n",
    "    anchors_boxes_checked = [anchors_boxes[j] for j in index_boxes_inside]\n",
    "    #IOU anchors vs gt box\n",
    "    iou_anc_gt_box = iou_anchors_vs_gtbox(anchors_boxes, gt_box)\n",
    "    iou_anc_gt_box_check = iou_anc_gt_box[index_boxes_inside,:]\n",
    "    #Index of the units we keep\n",
    "    ind_for_sample = index_training_proposal(iou_anc_gt_box_check,nsize, pos_ratio)\n",
    "    indice_to_kept = np.array(index_boxes_inside)[ind_for_sample]\n",
    "\n",
    "    anchors_boxes_reparam,lab_anchors = reparam_all_anchors(anchors_boxes,gt_box,iou_anc_gt_box,pos_threshold, neg_threshold)\n",
    "\n",
    "    final_label = np.full(len(anchors_boxes), -1)\n",
    "    final_label[indice_to_kept] = lab_anchors[indice_to_kept]\n",
    "    \n",
    "    #we need initial anchors_boxes for training FastRNN part\n",
    "    anchors_boxes = torch.from_numpy(np.array(anchors_boxes))\n",
    "    return ({\"image\" : image.to(device), \"boxes\" : torch.from_numpy(np.array(anchors_boxes_reparam)).to(device),\n",
    "             \"labels\" : torch.from_numpy(final_label).to(device), \"anchors_initial\" : anchors_boxes})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-08T23:12:00.559249Z",
     "start_time": "2021-12-08T23:12:00.531283Z"
    },
    "id": "rg80p3vKb_7l"
   },
   "outputs": [],
   "source": [
    "image_torch = 800*torch.rand((1,3,800,800))\n",
    "feature_torch = torch.rand([1,512,50,50])\n",
    "\n",
    "ratio = [0.5, 1, 2]\n",
    "anchor_scales = [8, 16, 32]\n",
    "\n",
    "gt_box = [np.array([20, 30, 400, 500]), np.array([300, 400, 500, 600])]\n",
    "labels_gt_box = np.array((\"chien\",\"chat\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "0MkkoKxfDdWx"
   },
   "outputs": [],
   "source": [
    "def batch_training_proposal_multi_RPN(batch,feature_shape, ratio, scales,device,nsize = 256, pos_ratio = 0.5, pos_threshold = 0.7, neg_threshold = 0.3):\n",
    "  image_torch, dict_label = batch\n",
    "  boxes = [list(np.array(dict_label[j][\"boxes\"])) for j in range(len(dict_label))]  #remove empty one\n",
    "  boxes = [[xywh_to_vertice(box) for box in box_list]for box_list in boxes]\n",
    "\n",
    "  gt_labels = [list(np.array(dict_label[j][\"labels\"])) for j in range(len(dict_label))]  #remove empty one\n",
    "  \n",
    "  res = [batch_training_proposal_RPN(image_torch[j].unsqueeze(0), feature_shape, ratio, anchor_scales, boxes[j],device,nsize, pos_ratio, pos_threshold, neg_threshold) for j in range(len(boxes)) if len(boxes[j]) != 0]\n",
    "  \n",
    "  ind_with_box = torch.tensor([i for i in range(len(boxes)) if len(boxes[i]) != 0])\n",
    "  image_torch = torch.index_select(image_torch,0, ind_with_box)\n",
    "  boxes_fin = torch.stack([elem[\"boxes\"] for elem in res])\n",
    "  labels = torch.stack([elem[\"labels\"] for elem in res])\n",
    "  anchors = torch.stack([elem[\"anchors_initial\"] for elem in res])\n",
    "\n",
    "  return {\"images\" : image_torch.to(device), \"boxes\": boxes_fin.to(device), \"labels\": labels.to(device),\"gt_box\": boxes, \"gt_labels\": gt_labels, \"anchors\": anchors}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QkGNzP5pb_7m"
   },
   "source": [
    "# Preparation of the proposed region to train the Fast RCNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-08T23:12:01.750681Z",
     "start_time": "2021-12-08T23:12:01.737311Z"
    },
    "id": "EJeoQrA2b_7m"
   },
   "outputs": [],
   "source": [
    "def clip_predicted_boxes(list_box, th_min, th_max):\n",
    "    list_box = np.array(list_box)\n",
    "    return list(np.clip(list_box,th_min,th_max))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-08T23:12:01.766679Z",
     "start_time": "2021-12-08T23:12:01.752679Z"
    },
    "id": "sX5nhvlVb_7m"
   },
   "outputs": [],
   "source": [
    "#remove all boxes with at least the width or the height less that 16\n",
    "def boxes_hw_min(list_box, list_score, min_size = 16):\n",
    "    boxes = np.array(list_box)\n",
    "    height = boxes[:, 2] - boxes[:, 0]\n",
    "    width = boxes[:, 3] - boxes[:, 1]\n",
    "    box_kept = np.where((height > min_size) & (width > min_size))[0]\n",
    "    list_box_kept = list(boxes[box_kept])\n",
    "    list_score = [list_score[j] for j in box_kept]\n",
    "    return list_box_kept, list_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-08T23:12:01.782680Z",
     "start_time": "2021-12-08T23:12:01.768698Z"
    },
    "id": "l4n3c2yab_7m"
   },
   "outputs": [],
   "source": [
    "def nms(list_box, list_score, top_pre, top_post, thresold):\n",
    "    list_score = np.array(list_score)\n",
    "    order = list_score.argsort()[::-1]\n",
    "    order = order[:top_pre]\n",
    "    keep = []\n",
    "    list_box = np.array(list_box)\n",
    "    \n",
    "    ym = list_box[:,0]\n",
    "    xm = list_box[:,1]\n",
    "    yp = list_box[:,2]\n",
    "    xp = list_box[:,3]\n",
    "    areas = (xp - xm + 1) * (yp - ym + 1)\n",
    "\n",
    "    while len(order)>0:\n",
    "        i = order[0]\n",
    "        yym = np.maximum(ym[i], ym[order[1:]])\n",
    "        xxm = np.maximum(xm[i], xm[order[1:]])\n",
    "        yyp = np.minimum(yp[i], yp[order[1:]])\n",
    "        xxp = np.minimum(xp[i], xp[order[1:]])\n",
    "        \n",
    "        width = np.maximum(0.0, xxp - xxm + 1)\n",
    "        height = np.maximum(0.0, yyp - yym + 1)\n",
    "        intersection = width*height\n",
    "        ovr = intersection/(areas[i] + areas[order[1:]] - intersection)\n",
    "        \n",
    "        ind_to_keep = np.where(ovr <= thresold)[0]\n",
    "        order = order[ind_to_keep + 1]\n",
    "        keep.append(i)\n",
    "    \n",
    "    keep = keep[:top_post]\n",
    "    return(list_box[keep,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cp5YhD2Qb_7m"
   },
   "source": [
    "# Region proposal network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-08T23:12:44.975470Z",
     "start_time": "2021-12-08T23:12:44.966089Z"
    },
    "id": "9zJP45j1b_7m"
   },
   "outputs": [],
   "source": [
    "#boxes as tensor [N, 5]\n",
    "def roi_pooling(boxes, feature_map,scale,adaptative_max_pool):\n",
    "    boxes_coord = boxes[:,1:].mul(scale).long() #scale + round\n",
    "    res = [feature_map.narrow(0, boxes[i,0].int(),1)[..., boxes_coord[i,1]:(boxes_coord[i,3]+1), boxes_coord[i,0]:(boxes_coord[i,2]+1)] for i in range(boxes_coord.shape[0])]\n",
    "    res = [adaptative_max_pool(element) for element in res]\n",
    "    res = torch.cat(res, axis = 0)\n",
    "    return(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-08T23:12:53.742770Z",
     "start_time": "2021-12-08T23:12:53.724769Z"
    },
    "id": "kwKmwLDsb_7m"
   },
   "outputs": [],
   "source": [
    "#0 in labels_gt_box must be the background\n",
    "def batch_training_proposal_FastRCNN(feature_map,list_box,list_gt_box,labels_gt_box, nsize = 128, pos_ratio = 0.25, pos_iou_threshold = 0.5,\n",
    "                                    neg_iou_threshold_p = 0.5, neg_iou_threshold_n = 0.0, adaptative_max_pool = torch.nn.AdaptiveMaxPool2d((7,7),return_indices=False),scale = 1):\n",
    "\n",
    "    #number of positive units we need to reach in the training sample (we want a balanced sample)\n",
    "    nb_pos_to_drawn = round(nsize*pos_ratio)\n",
    "    iou = iou_anchors_vs_gtbox(list_box, list_gt_box)\n",
    "    #compute the maximum for each anchor\n",
    "    gt_roi_label = np.argmax(iou, axis = 1)\n",
    "    gt_roi_max = np.max(iou, axis = 1)\n",
    "    labels = labels_gt_box[gt_roi_label]\n",
    "    #assign the label if greater that pos_iou_threshold\n",
    "    #assign background if between the two negative thresholds\n",
    "    gt_pos = np.where((gt_roi_max > pos_iou_threshold))[0]\n",
    "    gt_neg = np.where((gt_roi_max < neg_iou_threshold_p) & (gt_roi_max > neg_iou_threshold_n))[0] #background -- 0\n",
    "\n",
    "    #Nb of positives and negatives boxes get using the thresholds\n",
    "    pos = len(gt_pos)\n",
    "    neg = len(gt_neg)\n",
    "    \n",
    "    enable_index_pos = np.array([],dtype=int)\n",
    "\n",
    "    #Subsampling from it\n",
    "    if (pos > nb_pos_to_drawn):\n",
    "        disabled_index_pos = np.random.choice(range(len(gt_pos)), size=(pos - nb_pos_to_drawn), replace = False)\n",
    "        gt_pos = np.delete(gt_pos, disabled_index_pos)\n",
    "\n",
    "    if (neg > nsize - nb_pos_to_drawn):\n",
    "#         if(pos < nb_pos_to_drawn):\n",
    "#             print(neg)\n",
    "#             print(pos)\n",
    "#             print(nsize)\n",
    "#             disabled_index_neg = np.random.choice(range(len(gt_neg)), size=(neg - nsize + pos), replace = False)\n",
    "#             gt_neg = np.delete(gt_neg, disabled_index_neg)\n",
    "#         else:\n",
    "        disabled_index_neg = np.random.choice(range(len(gt_neg)), size=(neg + nb_pos_to_drawn - nsize), replace = False)\n",
    "        gt_neg = np.delete(gt_neg, disabled_index_neg)\n",
    "\n",
    "    if(neg + pos < nsize):\n",
    "        enable_index_pos = np.random.choice(np.append(gt_pos,gt_neg),size = nsize - neg - pos, replace = True)\n",
    "        \n",
    "    \n",
    "        \n",
    "    \n",
    "    #if negative : assign background labels with it's 0\n",
    "    labels[gt_neg] = 0\n",
    "    final_index = np.append(np.append(gt_pos,gt_neg),enable_index_pos)\n",
    "    #Non reparams\n",
    "    non_reparam = np.array(list_box)[final_index,:]\n",
    "    #Need to transform from yxhw to xywh\n",
    "    non_reparam = non_reparam[:,(1,0,3,2)]\n",
    "    non_reparam = np.hstack((np.zeros((non_reparam.shape[0],1)), non_reparam))\n",
    "    non_reparam = torch.from_numpy(non_reparam)\n",
    "\n",
    "    data_for_training = roi_pooling(non_reparam, feature_map,scale, adaptative_max_pool)\n",
    "    #Reparams\n",
    "    reparam = [loc(box,gt_box) for box,gt_box in zip(non_reparam, list(np.array(list_gt_box)[gt_roi_label,:]))]\n",
    "    \n",
    "    return  data_for_training, reparam,labels[final_index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oEAZuBOiX_CF"
   },
   "source": [
    "# Load train dataset \n",
    "\n",
    "J'ai rÃ©ussi Ã  utiliser l'API Coco via `torchvision.datasets.CocoDetection` (https://pytorch.org/vision/stable/datasets.html)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "kc-SDR8e-P0z",
    "outputId": "6cc2db42-b751-42d5-ccbb-f5becc4557e2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2021-12-30 15:22:16--  https://conservancy.umn.edu/bitstream/handle/11299/214865/dataset.zip?sequence=12&isAllowed=y\n",
      "Resolving conservancy.umn.edu (conservancy.umn.edu)... 128.101.122.105\n",
      "Connecting to conservancy.umn.edu (conservancy.umn.edu)|128.101.122.105|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 200\n",
      "Length: 553029970 (527M) [application/zip]\n",
      "Saving to: âdataset.zip?sequence=12&isAllowed=y.2â\n",
      "\n",
      "?sequence=12&isAllo  10%[=>                  ]  58.00M  12.8MB/s    eta 43s    ^C\n",
      "Archive:  dataset.zip?sequence=12&isAllowed=y\n",
      "replace dataset/README.txt? [y]es, [n]o, [A]ll, [N]one, [r]ename: ^C\n"
     ]
    }
   ],
   "source": [
    "#Permet d'utiliser la co des serveurs Google (rip la mienne) et assure une meilleure reproductibilitÃ©\n",
    "!wget \"https://conservancy.umn.edu/bitstream/handle/11299/214865/dataset.zip?sequence=12&isAllowed=y\"\n",
    "!unzip \"dataset.zip?sequence=12&isAllowed=y\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "XuMoQmEAVwfs"
   },
   "outputs": [],
   "source": [
    "from torchvision import datasets\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "L6-SecLQN6hP"
   },
   "outputs": [],
   "source": [
    "# The directory containing the source images\n",
    "data_path = \"dataset/instance_version/train\"\n",
    "\n",
    "# The path to the COCO labels JSON file\n",
    "labels_path = \"dataset/instance_version/instances_train_trashcan.json\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "23FFr0UOSPpG"
   },
   "source": [
    "#### Version 4 - resize des images, en ne gardant que les bbox et category_id des targets normalisÃ©es, dans un array de dictionnaires (targets de taille variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "id": "oRookaleStv6"
   },
   "outputs": [],
   "source": [
    "#Attention \"bbox\": [x,y,width,height]\n",
    "class CocoDetection_diy_bis(data.Dataset) :\n",
    "    \"\"\"`MS Coco Detection <http://mscoco.org/dataset/#detections-challenge2016>`_ Dataset.\n",
    "\n",
    "    Args:\n",
    "        root (string): Root directory where images are downloaded to.\n",
    "        annFile (string): Path to json annotation file.\n",
    "        resize : (int,int) size of the images wanted \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, root, annFile, size, elements_index):\n",
    "        from pycocotools.coco import COCO\n",
    "        self.root = root\n",
    "        self.coco = COCO(annFile)\n",
    "        self.total_ids = list(self.coco.imgs.keys())\n",
    "        self.ids = [self.total_ids[j] for j in elements_index]\n",
    "        self.size = size\n",
    "        self.transform = transforms.Compose([transforms.Resize(size), transforms.ToTensor(),transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                 std=[0.229, 0.224, 0.225])])\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            index (int): Index\n",
    "\n",
    "        Returns:\n",
    "            tuple: Tuple (image, target). target is the object returned by ``coco.loadAnns``.\n",
    "        \"\"\"\n",
    "        coco = self.coco\n",
    "        img_id = self.ids[index]\n",
    "        ann_ids = coco.getAnnIds(imgIds=img_id)\n",
    "        target = coco.loadAnns(ann_ids)\n",
    "\n",
    "        path = coco.loadImgs(img_id)[0]['file_name']\n",
    "\n",
    "        # Resize des images :\n",
    "        img = Image.open(os.path.join(self.root, path)).convert('RGB')\n",
    "        original_size = img.size\n",
    "        img = self.transform(img)\n",
    "\n",
    "        # Targets dict :\n",
    "        targets = {'labels':[],'boxes':[]}\n",
    "\n",
    "        for elem in target :  \n",
    "          box = elem['bbox']\n",
    "          box[0] *= self.size[0] / original_size[0]\n",
    "          box[1] *= self.size[1] / original_size[1]\n",
    "          box[2] *= self.size[0] /original_size[0]\n",
    "          box[3] *= self.size[1] /  original_size[1]\n",
    "          targets['boxes'].append(box)\n",
    "          targets['labels'].append(elem['category_id'])\n",
    "        print(self.size[0]/original_size[0])\n",
    "\n",
    "        return img, targets\n",
    "        \n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ids)\n",
    "\n",
    "    def __repr__(self):\n",
    "        fmt_str = 'Dataset ' + self.__class__.__name__ + '\\n'\n",
    "        fmt_str += '    Number of datapoints: {}\\n'.format(self.__len__())\n",
    "        fmt_str += '    Root Location: {}\\n'.format(self.root)\n",
    "        tmp = '    Transforms (if any): '\n",
    "        fmt_str += '{0}{1}\\n'.format(tmp, self.transform.__repr__().replace('\\n', '\\n' + ' ' * len(tmp)))\n",
    "        tmp = '    Target Transforms (if any): '\n",
    "        fmt_str += '{0}{1}'.format(tmp, self.target_transform.__repr__().replace('\\n', '\\n' + ' ' * len(tmp)))\n",
    "        return fmt_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "y2652baDVnV8"
   },
   "outputs": [],
   "source": [
    "def collate_fn_diy (batch) : \n",
    "    \"\"\"\n",
    "    Parameters : \n",
    "    -----------\n",
    "    batch : list of tuples (img,targets)\n",
    "\n",
    "    Return : \n",
    "    -------\n",
    "    images : tensor of dim batch_size x 3 x 224 x 224\n",
    "    targets : list of dict containing : \n",
    "        - \"labels\": Tensor of dim [num_target_boxes] (where num_target_boxes is the number of ground-truth objects in the target) containing the class labels\n",
    "        - \"boxes\": Tensor of dim [num_target_boxes, 4] containing the target box coordinates\n",
    "    \"\"\"\n",
    "    imgs, trgts = list(zip(*batch)) # imgs et trgts sont dÃ©sormais des batch_size-tuples \n",
    "\n",
    "    imgs = [img.unsqueeze(0) for img in list(imgs)] #ajout d'une dimension supplÃ©mentaire Ã  tous les tenseurs\n",
    "    images = torch.cat(imgs) # concatÃ©nation en un seul tenseur\n",
    "\n",
    "    targets = []\n",
    "    for t in list(trgts) : \n",
    "      targets.append({'labels' : torch.from_numpy(np.array(t[\"labels\"])), \n",
    "                      'boxes' : torch.from_numpy(np.array(t[\"boxes\"]))})\n",
    "    \n",
    "    return images, targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X7HBNfbHGA9z"
   },
   "source": [
    "\n",
    "TODO : que faire lorsqu'on a des images sans box et sans label donc ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ah1baurMb_7n"
   },
   "source": [
    "# Training RPN network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-08T23:13:37.687322Z",
     "start_time": "2021-12-08T23:13:37.348201Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 83,
     "referenced_widgets": [
      "dd231e88790d42bea6867aa556651c8e",
      "7ff93690eff6496ca0eaff04af2d197e",
      "5d702123da9e48129ff7557a02f72b99",
      "5dfa0e0da40b4ddda4e7dd26a4243622",
      "f77ce4b2ae214c80984ba0b4fa2d2c14",
      "6eefd4e548024038b8e50141bdeef898",
      "079761f87ba8483e9ccf4b97da879ef1",
      "f0bcc1811e5b4fb49be83fd1bf52c557",
      "04e8eac584c947f8a4dd2e7ae9721dd7",
      "e9e980dc151b4fc3a242255c7f92dd34",
      "9ac6009437a84a5d8868dbfa7a209924"
     ]
    },
    "id": "6hNzYE92b_7o",
    "outputId": "86fd9ee3-f3ee-4130-fa6b-c55855a86bc3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 256, 200, 200])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchvision import models\n",
    "\n",
    "resnet50 = models.resnet50(pretrained=True)\n",
    "image_torch = 800*torch.rand((1,3,800,800))\n",
    "#We choose the place where we extracted the feature map in order to get H_feature * W_feature around 2400 (papers)\n",
    "resnet50_features = nn.Sequential(*(list(resnet50.children())[:-5]))\n",
    "resnet50_features(image_torch).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Pm6g59e7b_7p"
   },
   "source": [
    "https://stackoverflow.com/questions/69480764/what-is-the-difference-between-resnet50-vgg16-etc-and-rcnn-faster-rcnn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-08T23:13:41.640996Z",
     "start_time": "2021-12-08T23:13:41.621664Z"
    },
    "code_folding": [],
    "id": "EYl81Phib_7p"
   },
   "outputs": [],
   "source": [
    "class RPN(nn.Module):\n",
    "    #TODO : remove embedding_dim using wv.shape[1]\n",
    "    #define all the layers used in model\n",
    "    def __init__(self,mid_channels, in_channels,nb_anchors,pre_trained_model):\n",
    "        \n",
    "        #Constructor\n",
    "        super().__init__()        \n",
    "        \n",
    "        #embedding layer\n",
    "        self.pre_trained_model = pre_trained_model\n",
    "        self.conv1 = nn.Conv2d(in_channels, mid_channels, 3, 1, 1)\n",
    "        self.conv1.weight.data.normal_(0, 0.01)\n",
    "        self.conv1.bias.data.zero_()\n",
    "        self.sm = nn.Softmax(dim = 2)\n",
    "        self.reg_layer = nn.Conv2d(mid_channels, nb_anchors *4, 1, 1, 0)\n",
    "        self.reg_layer.weight.data.normal_(0, 0.01)\n",
    "        self.reg_layer.bias.data.zero_()\n",
    "\n",
    "        self.cls_layer = nn.Conv2d(mid_channels, nb_anchors *2, 1, 1, 0)\n",
    "        # classification layer\n",
    "        self.cls_layer.weight.data.normal_(0, 0.01)\n",
    "        self.cls_layer.bias.data.zero_()\n",
    "       \n",
    "\n",
    "    def forward(self,img):\n",
    "        nb_images = img.shape[0] #sale ?\n",
    "        x = self.pre_trained_model(img)\n",
    "        x = self.conv1(x)\n",
    "        pred_anchor = self.reg_layer(x)\n",
    "        pred_anchor = pred_anchor.permute(0, 2, 3, 1).contiguous().view(nb_images, -1, 4)\n",
    "        \n",
    "        pred_cls = self.cls_layer(x)\n",
    "        pred_cls = pred_cls.permute(0, 2, 3, 1).contiguous()\n",
    "        pred_cls = pred_cls.view(nb_images, -1, 2)\n",
    "        #pred_cls = self.sm(pred_cls)\n",
    "        return pred_anchor, pred_cls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "koHem-H00uDv"
   },
   "outputs": [],
   "source": [
    "def l1smooth(pred,target,nb_anchors = 9):\n",
    "  #zeros for those which are negative\n",
    "  boxes = ((target[\"labels\"] == 1).unsqueeze(-1).repeat(1,1,4).float() * target['boxes'])\n",
    "  pred = ((target[\"labels\"] == 1).unsqueeze(-1).repeat(1,1,4).float() * pred[0])\n",
    "  x = torch.abs(boxes.flatten() - pred.flatten())\n",
    "  return sum(0.5*(x**2)*(x < 1) + (x - 0.5)*(x >= 1)) / nb_anchors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "aj4IJlRQ6xGf"
   },
   "outputs": [],
   "source": [
    "def RPN_loss(pred,target,feature_shape, loss_ce = nn.CrossEntropyLoss(ignore_index=-1),loss_sml1 = nn.SmoothL1Loss(reduction = \"sum\"),  lamb = 10):\n",
    "  #Remove all -1 for the computation of the CE loss\n",
    "  #Classification part\n",
    "  loss_cls = loss_ce(pred[1].view(-1,2),target['labels'].view(-1,1).squeeze().long())\n",
    "  #Regression part\n",
    "  boxes = ((target[\"labels\"] == 1).unsqueeze(-1).repeat(1,1,4).float() * target['boxes'])\n",
    "  pred = ((target[\"labels\"] == 1).unsqueeze(-1).repeat(1,1,4).float() * pred[0])\n",
    "  loss_reg = loss_sml1(boxes, pred)/torch.prod(feature_shape[2:])\n",
    "  #Final loss\n",
    "  loss = loss_cls + lamb*loss_reg\n",
    "  return loss\n",
    "\n",
    "def RPN_loss2(pred,target, nb_anchors = 56*56, lamb = 10):\n",
    "  #Remove all -1 for the computation of the CE loss\n",
    "  #Classification part\n",
    "  loss_ce = nn.CrossEntropyLoss(ignore_index=-1)\n",
    "  loss_cls = loss_ce(pred[1].view(-1,2),target['labels'].view(-1,1).squeeze().long())\n",
    "  #Regression part\n",
    "  loss_reg = l1smooth(pred, target, nb_anchors)\n",
    "  #Final loss\n",
    "  loss = loss_cls + lamb*loss_reg\n",
    "  return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q3po-CtKr4v7"
   },
   "source": [
    "The new version of RPN is at least 8 time faster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6Qnn6nE28UI3"
   },
   "source": [
    "As indicated in the paper, we are using SGD optimiser with 0.001 as learning rate and 0.9 for momemtum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "SananpqIs0XB"
   },
   "outputs": [],
   "source": [
    "def fn_train_step_RPN(model, loss_fn, optimizer,feature_shape = torch.tensor((1,1,56,56))):\n",
    "  def train_step(data_loader):\n",
    "    model.train()\n",
    "    losses = 0\n",
    "    for input in data_loader:\n",
    "      input = batch_training_proposal_multi_RPN(input, feature_shape, ratio, anchor_scales, device)\n",
    "      output = model_RPN(input[\"images\"])\n",
    "\n",
    "      loss = RPN_loss(output, input,feature_shape)\n",
    "      loss.backward()\n",
    "\n",
    "      optimizer.step()\n",
    "      optimizer.zero_grad()\n",
    "\n",
    "      losses =+ loss.item()\n",
    "\n",
    "    return losses/len(data_loader)\n",
    "\n",
    "  return train_step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sHG3LKTZvitA"
   },
   "source": [
    "## Split the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wNd1lcYOudBZ",
    "outputId": "ecc46dea-efd5-4828-8575-1facc549678c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.32s)\n",
      "creating index...\n",
      "index created!\n",
      "loading annotations into memory...\n",
      "Done (t=0.25s)\n",
      "creating index...\n",
      "index created!\n",
      "loading annotations into memory...\n",
      "Done (t=0.39s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "prop_train = 0.5\n",
    "prop_valid = 0.3\n",
    "prop_test = 0.2\n",
    "\n",
    "n = 6000\n",
    "n_train = int(n*prop_train)\n",
    "n_test = int(n*prop_test)\n",
    "n_valid = n - n_train- n_test\n",
    "\n",
    "ind_n_train = np.random.choice(np.arange(n), n_train)\n",
    "ind_n_test = np.random.choice(np.setdiff1d(np.arange(n), ind_n_train), n_test)\n",
    "ind_n_valid = np.setdiff1d(np.setdiff1d(np.arange(n), ind_n_train),ind_n_test)\n",
    "\n",
    "#train Dataloader\n",
    "instances_train = CocoDetection_diy_bis(root = data_path, annFile = labels_path, size=(224,224), elements_index = ind_n_train)\n",
    "instances_train_dataloader = DataLoader(instances_train, batch_size=1, shuffle=True, collate_fn = collate_fn_diy)\n",
    "\n",
    "#valid dataloader\n",
    "instances_valid = CocoDetection_diy_bis(root = data_path, annFile = labels_path, size=(224,224), elements_index = ind_n_valid)\n",
    "instances_valid_dataloader = DataLoader(instances_valid, batch_size=1, shuffle=True, collate_fn = collate_fn_diy)\n",
    "\n",
    "\n",
    "#test dataloader\n",
    "instances_test = CocoDetection_diy_bis(root = data_path, annFile = labels_path, size=(224,224), elements_index = ind_n_test)\n",
    "instances_test_dataloader = DataLoader(instances_test, batch_size=1, shuffle=True, collate_fn = collate_fn_diy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "TSnXqSR-8TU6"
   },
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "feature_shape = (1,1,56,56)\n",
    "model_RPN = RPN(256,256,9,resnet50_features).to(device)\n",
    "optimizer = optim.SGD(model_RPN.parameters(), lr=0.001, momentum=0.9)\n",
    "best_model = RPN(256,256,9,resnet50_features).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "o4_bCnb8CFZl",
    "outputId": "0b2efcdf-d827-4c44-a93d-3a4feb4d2ce9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tEpoch 0 : Train Loss: 0.000 | \tValid Loss: 0.000\n",
      "\tEpoch 1 : Train Loss: 0.000 | \tValid Loss: 0.001\n",
      "\tEpoch 2 : Train Loss: 0.000 | \tValid Loss: 0.000\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "#TODO : save better model ! on valid ?\n",
    "t0 = time.time()\n",
    "loss_list_train = [float('inf')]\n",
    "loss_list_valid = [float('inf')]\n",
    "nb_epoch = N_EPOCH\n",
    "i = 0\n",
    "\n",
    "\n",
    "for epoch in range(nb_epoch):\n",
    "  #training\n",
    "  model_RPN.train()\n",
    "  losses = 0\n",
    "  for element in instances_train_dataloader:\n",
    "        if(len(element[1][0][\"labels\"]) != 0):\n",
    "            input = batch_training_proposal_multi_RPN(element, feature_shape, ratio, anchor_scales, device)\n",
    "            output = model_RPN(input[\"images\"])\n",
    "            loss = RPN_loss(output, input,torch.tensor(feature_shape))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            losses =+ loss.item()\n",
    "  loss_epch = losses/len(instances_train_dataloader)\n",
    "  loss_list_train.append(loss_epch)\n",
    "  #valid\n",
    "  with torch.no_grad():\n",
    "    losses = 0\n",
    "    for element in instances_valid_dataloader:\n",
    "        if(len(element[1][0][\"labels\"]) != 0):\n",
    "          input = batch_training_proposal_multi_RPN(element, feature_shape, ratio, anchor_scales, device)\n",
    "          output = model_RPN(input[\"images\"])\n",
    "          model_RPN.eval()\n",
    "          loss = RPN_loss(output, input, torch.tensor(feature_shape))\n",
    "          losses =+ loss.item()\n",
    "    loss_epch_valid = losses/len(instances_valid_dataloader)\n",
    "    if loss_epch_valid < min(loss_list_valid):\n",
    "        best_model = (copy.deepcopy(model_RPN.state_dict()))\n",
    "        #Ajoutez des mÃ©triques\n",
    "    loss_list_valid.append(loss_epch_valid)\n",
    "    print(f'\\tEpoch {i} : Train Loss: {loss_epch:.3f} | \\tValid Loss: {loss_epch_valid:.3f}')\n",
    "  i += 1\n",
    " \n",
    "\n",
    "t1 = time.time()\n",
    "\n",
    "total = t1-t0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wFuXwXYi51eh"
   },
   "outputs": [],
   "source": [
    "torch.save(best_model, \"model_RPN.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load previous trained Fast model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_RPN = RPN(256,256,9,resnet50_features).to(device)\n",
    "model_RPN.load_state_dict(torch.load(\"model_RPN.pt\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "R2hA9uAT51eh"
   },
   "outputs": [],
   "source": [
    "exemple = next(iter(instances_train_dataloader))\n",
    "exemple = batch_training_proposal_multi_RPN(exemple, feature_shape, ratio, anchor_scales, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ovv1JwXg51eh"
   },
   "outputs": [],
   "source": [
    "exemple[\"gt_box\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "w6kJvdpm51eh"
   },
   "outputs": [],
   "source": [
    "model_RPN(exemple[\"images\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il est possible d'utiliser directement le tensor image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3kVHAhCJ51ei"
   },
   "source": [
    "## Transition between RPN and FastRCNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.youtube.com/watch?v=cSO1nUj495Y&ab_channel=ArdianUmam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_exemple = next(iter(instances_train_dataloader))\n",
    "exemple =  batch_training_proposal_multi_RPN(batch_exemple, feature_shape, ratio, anchor_scales, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "id": "0IihDfwk51ei"
   },
   "outputs": [],
   "source": [
    "pre_trained_model_after_RPN = model_RPN._modules[\"pre_trained_model\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "#0 in labels_gt_box must be the background\n",
    "def batch_training_proposal_FastRCNN(feature_map,list_box,list_gt_box,labels_gt_box,device, nsize = 128, pos_ratio = 0.25, pos_iou_threshold = 0.5,\n",
    "                                    neg_iou_threshold_p = 0.5, neg_iou_threshold_n = 0.0, adaptative_max_pool = torch.nn.AdaptiveMaxPool2d((7,7),return_indices=False),\n",
    "                                     scale = 1):\n",
    "\n",
    "    #number of positive units we need to reach in the training sample (we want a balanced sample)\n",
    "    nb_pos_to_drawn = round(nsize*pos_ratio)\n",
    "    iou = iou_anchors_vs_gtbox(list_box, list_gt_box)\n",
    "    #compute the maximum for each anchor\n",
    "    gt_roi_label = np.argmax(iou, axis = 1)\n",
    "    gt_roi_max = np.max(iou, axis = 1)\n",
    "    labels = labels_gt_box[gt_roi_label]\n",
    "    #assign the label if greater that pos_iou_threshold\n",
    "    #assign background if between the two negative thresholds\n",
    "    gt_pos = np.where((gt_roi_max > pos_iou_threshold))[0]\n",
    "    gt_neg = np.where((gt_roi_max < neg_iou_threshold_p) & (gt_roi_max > neg_iou_threshold_n))[0] #background -- 0\n",
    "\n",
    "    #Nb of positives and negatives boxes get using the thresholds\n",
    "    pos = len(gt_pos)\n",
    "    neg = len(gt_neg)\n",
    "    \n",
    "    enable_index_pos = np.array([],dtype=int)\n",
    "    \n",
    "    #Subsampling from it\n",
    "    if (pos > nb_pos_to_drawn):\n",
    "        disabled_index_pos = np.random.choice(range(len(gt_pos)), size=(pos - nb_pos_to_drawn), replace = False)\n",
    "        gt_pos = np.delete(gt_pos, disabled_index_pos)\n",
    "\n",
    "    if (neg > nsize - nb_pos_to_drawn):\n",
    "        disabled_index_neg = np.random.choice(range(len(gt_neg)), size=(neg + nb_pos_to_drawn - nsize), replace = False)\n",
    "        gt_neg = np.delete(gt_neg, disabled_index_neg)\n",
    "\n",
    "    if(len(gt_neg) + len(gt_pos) < nsize):\n",
    "        enable_index_pos = np.random.choice(np.append(gt_pos,gt_neg),size = nsize - (len(gt_neg) + len(gt_pos)), replace = True)\n",
    "        \n",
    "    #if negative : assign background labels with it's 1000\n",
    "    labels[gt_neg] = 1000\n",
    "    labels[np.where(labels != 1000)] = 412 #imagenet for trashes\n",
    "    final_index = np.append(np.append(gt_pos,gt_neg),enable_index_pos)\n",
    "    #Non reparams\n",
    "    non_reparam = np.array(list_box)[final_index,:]\n",
    "    #Need to transform from yxhw to xywh\n",
    "    non_reparam_xywh = non_reparam[:,(1,0,3,2)]\n",
    "    non_reparam_xywh = np.hstack((np.zeros((non_reparam_xywh.shape[0],1)), non_reparam_xywh))\n",
    "    non_reparam_torch = torch.from_numpy(non_reparam_xywh)\n",
    "    \n",
    "\n",
    "    data_for_training = roi_pooling(non_reparam_torch, feature_map,scale, adaptative_max_pool)\n",
    "    array_gt_box_for_each = np.array(list_gt_box)[gt_roi_label,:]\n",
    "    reparam = loc_list(non_reparam, array_gt_box_for_each[final_index,:])\n",
    "    return  {'feature_map_extract' : data_for_training.to(device), 'boxes' : torch.tensor(np.array(reparam)).to(device), \n",
    "             'labels' : torch.tensor(np.array(labels[final_index])).to(device), 'non_reparam_box' : torch.from_numpy(non_reparam).to(device)}\n",
    "#data for training : set of extracted part of feature maps using roi pooling\n",
    "#boxes\n",
    "#labels for each boxes : ATTENTION PASSE EN IMAGENET !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "id": "A_c_Tqns51ei"
   },
   "outputs": [],
   "source": [
    "#exemple batch containing a single exemple\n",
    "sm = nn.Softmax(dim = 2)\n",
    "def RPN_to_FRCNN(exemple, feature_size, min_size_anchors, before_top_nms,after_top_nms,pos_t, model, feature_map):\n",
    "    #no learning in this part !\n",
    "    with torch.no_grad():\n",
    "        res = model(exemple[\"images\"])\n",
    "    \n",
    "    prob = sm(res[0])[0,:,1].detach().cpu().numpy()\n",
    "    anchors = list(np.array(exemple[\"anchors\"][0].cpu()))\n",
    "    list_box = list((res[0][0].detach().cpu()).numpy())\n",
    "    boxes_hw = boxes_hw_min(clip_predicted_boxes(deloc_list(anchors,list_box),0,feature_size),prob,min_size_anchors)\n",
    "    list_box = list(nms(boxes_hw[0], boxes_hw[1], before_top_nms,after_top_nms,pos_t))\n",
    "    return {\"feature_map\" : feature_map, \"list_roi\" : list_box, \"list_gt_labels\" : np.array(exemple[\"gt_labels\"][0]), \"list_gt_box\" : list(exemple[\"gt_box\"][0])}\n",
    "\n",
    "after_rpn = RPN_to_FRCNN(exemple, 224,16,12000,1000,0.7,model_RPN, pre_trained_model_after_RPN(exemple[\"images\"]))\n",
    "\n",
    "res = batch_training_proposal_FastRCNN(after_rpn[\"feature_map\"],after_rpn[\"list_roi\"],after_rpn[\"list_gt_box\"],after_rpn[\"list_gt_labels\"],device,\n",
    "                                       nsize = 128, pos_ratio = 0.25, pos_iou_threshold = 0.5,\n",
    "                                adaptative_max_pool = torch.nn.AdaptiveMaxPool2d((7,7),return_indices=False), scale = 1/4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-20.6274,  -9.3137,  24.6274,  13.3137],\n",
       "         [-43.2548, -20.6274,  47.2548,  24.6274],\n",
       "         [-88.5097, -43.2548,  92.5097,  47.2548],\n",
       "         ...,\n",
       "         [210.6863, 199.3726, 233.3137, 244.6274],\n",
       "         [199.3726, 176.7452, 244.6274, 267.2548],\n",
       "         [176.7452, 131.4903, 267.2548, 312.5097]]], dtype=torch.float64)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exemple[\"anchors\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'feature_map_extract': tensor([[[[4.2578e-01, 4.4314e-01, 4.7205e-01,  ..., 4.6826e-01,\n",
       "            4.5549e-01, 4.4630e-01],\n",
       "           [4.8899e-01, 4.8899e-01, 4.6668e-01,  ..., 4.1786e-01,\n",
       "            3.4810e-01, 3.4291e-01],\n",
       "           [4.8899e-01, 4.8899e-01, 4.6668e-01,  ..., 3.5445e-01,\n",
       "            3.5445e-01, 3.5205e-01],\n",
       "           ...,\n",
       "           [3.6038e-01, 3.6038e-01, 3.1071e-01,  ..., 2.6101e-01,\n",
       "            2.4965e-01, 2.2877e-01],\n",
       "           [3.0972e-01, 3.1071e-01, 3.1071e-01,  ..., 1.6760e-01,\n",
       "            1.6760e-01, 1.6316e-01],\n",
       "           [3.2131e-01, 3.1774e-01, 3.1375e-01,  ..., 1.7209e-01,\n",
       "            1.7209e-01, 1.6316e-01]],\n",
       " \n",
       "          [[6.8478e-01, 6.8478e-01, 6.8297e-01,  ..., 7.2280e-01,\n",
       "            7.1568e-01, 6.8715e-01],\n",
       "           [9.4576e-01, 9.4576e-01, 8.3126e-01,  ..., 6.8150e-01,\n",
       "            6.4678e-01, 6.2652e-01],\n",
       "           [9.4576e-01, 9.4576e-01, 8.3126e-01,  ..., 6.8150e-01,\n",
       "            6.4678e-01, 6.2652e-01],\n",
       "           ...,\n",
       "           [7.1953e-01, 7.1953e-01, 6.3304e-01,  ..., 5.5219e-01,\n",
       "            5.3777e-01, 4.9190e-01],\n",
       "           [6.1237e-01, 6.1237e-01, 6.0265e-01,  ..., 4.4140e-01,\n",
       "            3.9305e-01, 3.7552e-01],\n",
       "           [6.2203e-01, 6.1176e-01, 6.1176e-01,  ..., 4.4099e-01,\n",
       "            4.1505e-01, 3.9361e-01]],\n",
       " \n",
       "          [[4.1519e-02, 5.3813e-02, 1.2500e-01,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00],\n",
       "           [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00],\n",
       "           [0.0000e+00, 2.1701e-01, 3.2123e-01,  ..., 2.3326e-01,\n",
       "            2.5154e-01, 3.3386e-01],\n",
       "           ...,\n",
       "           [3.3011e-01, 3.3011e-01, 3.7929e-01,  ..., 3.1120e-01,\n",
       "            3.7128e-01, 3.7128e-01],\n",
       "           [2.7598e-01, 2.8721e-01, 2.9326e-01,  ..., 3.2140e-01,\n",
       "            3.1243e-01, 3.0013e-01],\n",
       "           [2.6929e-01, 3.1837e-01, 3.3184e-01,  ..., 3.1243e-01,\n",
       "            3.1243e-01, 3.0013e-01]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[2.2117e-01, 2.1980e-01, 2.2327e-01,  ..., 2.3754e-01,\n",
       "            2.2862e-01, 2.2265e-01],\n",
       "           [3.0091e-01, 3.0091e-01, 2.3030e-01,  ..., 1.9570e-01,\n",
       "            1.7365e-01, 1.5674e-01],\n",
       "           [2.9392e-01, 2.9392e-01, 2.3030e-01,  ..., 1.4181e-01,\n",
       "            1.4330e-01, 1.4653e-01],\n",
       "           ...,\n",
       "           [1.2192e-01, 1.2192e-01, 1.0730e-01,  ..., 1.0729e-01,\n",
       "            1.0408e-01, 1.0239e-01],\n",
       "           [9.5783e-02, 9.6415e-02, 9.6415e-02,  ..., 6.2054e-02,\n",
       "            5.7446e-02, 6.2186e-02],\n",
       "           [9.5783e-02, 9.5916e-02, 9.5916e-02,  ..., 5.8325e-02,\n",
       "            5.5874e-02, 6.1802e-02]],\n",
       " \n",
       "          [[1.1268e+00, 1.1268e+00, 1.1517e+00,  ..., 1.1034e+00,\n",
       "            1.1281e+00, 1.1550e+00],\n",
       "           [7.4157e-01, 8.9578e-01, 8.9578e-01,  ..., 6.9092e-01,\n",
       "            7.9028e-01, 7.9028e-01],\n",
       "           [7.1634e-01, 7.8549e-01, 7.8549e-01,  ..., 5.1943e-01,\n",
       "            5.1943e-01, 5.1900e-01],\n",
       "           ...,\n",
       "           [4.7785e-01, 4.9366e-01, 5.0609e-01,  ..., 5.0369e-01,\n",
       "            4.8458e-01, 4.8458e-01],\n",
       "           [4.8560e-01, 4.8560e-01, 4.6496e-01,  ..., 5.0050e-01,\n",
       "            4.7355e-01, 4.3440e-01],\n",
       "           [4.9971e-01, 4.9971e-01, 4.8748e-01,  ..., 4.8304e-01,\n",
       "            4.8304e-01, 4.7798e-01]],\n",
       " \n",
       "          [[5.5697e-01, 5.5697e-01, 3.7662e-01,  ..., 4.4225e-01,\n",
       "            3.7829e-01, 3.7829e-01],\n",
       "           [5.5697e-01, 5.6340e-01, 5.6340e-01,  ..., 4.4225e-01,\n",
       "            3.7829e-01, 3.7829e-01],\n",
       "           [2.8035e-01, 2.8035e-01, 1.3568e-01,  ..., 2.1564e-02,\n",
       "            2.8884e-02, 2.8884e-02],\n",
       "           ...,\n",
       "           [0.0000e+00, 6.3527e-04, 1.3090e-02,  ..., 4.1719e-02,\n",
       "            0.0000e+00, 3.5753e-02],\n",
       "           [0.0000e+00, 6.3527e-04, 1.3090e-02,  ..., 0.0000e+00,\n",
       "            3.2907e-02, 3.2907e-02],\n",
       "           [0.0000e+00, 0.0000e+00, 2.5953e-02,  ..., 0.0000e+00,\n",
       "            8.8211e-02, 8.8211e-02]]],\n",
       " \n",
       " \n",
       "         [[[3.9841e-01, 3.5694e-01, 4.2578e-01,  ..., 4.7205e-01,\n",
       "            4.7205e-01, 4.6826e-01],\n",
       "           [3.9892e-01, 4.1714e-01, 4.8899e-01,  ..., 4.6668e-01,\n",
       "            4.3800e-01, 4.1786e-01],\n",
       "           [3.9892e-01, 4.1999e-01, 4.8899e-01,  ..., 4.6668e-01,\n",
       "            3.9658e-01, 3.5445e-01],\n",
       "           ...,\n",
       "           [3.2443e-01, 2.8574e-01, 3.6038e-01,  ..., 3.1071e-01,\n",
       "            2.8492e-01, 2.6101e-01],\n",
       "           [3.2443e-01, 2.9211e-01, 3.0972e-01,  ..., 3.1071e-01,\n",
       "            2.4412e-01, 1.6760e-01],\n",
       "           [3.2578e-01, 3.2200e-01, 3.2131e-01,  ..., 3.1375e-01,\n",
       "            2.4530e-01, 1.7209e-01]],\n",
       " \n",
       "          [[6.8447e-01, 6.2402e-01, 6.8478e-01,  ..., 6.8297e-01,\n",
       "            7.2280e-01, 7.2280e-01],\n",
       "           [9.3925e-01, 8.2247e-01, 9.4576e-01,  ..., 8.3126e-01,\n",
       "            7.0956e-01, 6.8150e-01],\n",
       "           [9.3925e-01, 8.6816e-01, 9.4576e-01,  ..., 8.3126e-01,\n",
       "            7.0956e-01, 6.8150e-01],\n",
       "           ...,\n",
       "           [6.6372e-01, 6.7065e-01, 7.1953e-01,  ..., 6.3304e-01,\n",
       "            5.7992e-01, 5.5219e-01],\n",
       "           [6.7462e-01, 6.2130e-01, 6.1237e-01,  ..., 6.0265e-01,\n",
       "            5.4911e-01, 4.4140e-01],\n",
       "           [6.5763e-01, 6.3657e-01, 6.2203e-01,  ..., 6.1176e-01,\n",
       "            5.5634e-01, 4.4099e-01]],\n",
       " \n",
       "          [[1.3521e-01, 7.5056e-02, 4.1519e-02,  ..., 1.2500e-01,\n",
       "            1.2500e-01, 0.0000e+00],\n",
       "           [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00],\n",
       "           [9.2545e-02, 9.2545e-02, 0.0000e+00,  ..., 3.2123e-01,\n",
       "            3.2123e-01, 2.3326e-01],\n",
       "           ...,\n",
       "           [3.0649e-01, 2.7598e-01, 3.3011e-01,  ..., 3.7929e-01,\n",
       "            3.7929e-01, 3.1120e-01],\n",
       "           [3.3732e-01, 2.8594e-01, 2.7598e-01,  ..., 2.9326e-01,\n",
       "            3.2140e-01, 3.2140e-01],\n",
       "           [3.4498e-01, 3.4498e-01, 2.6929e-01,  ..., 3.3184e-01,\n",
       "            3.3184e-01, 3.1243e-01]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[2.0543e-01, 2.2117e-01, 2.2117e-01,  ..., 2.2327e-01,\n",
       "            2.3754e-01, 2.3754e-01],\n",
       "           [2.9746e-01, 3.0069e-01, 3.0091e-01,  ..., 2.3030e-01,\n",
       "            1.9901e-01, 1.9570e-01],\n",
       "           [2.4960e-01, 2.3392e-01, 2.9392e-01,  ..., 2.3030e-01,\n",
       "            1.5114e-01, 1.4181e-01],\n",
       "           ...,\n",
       "           [1.0397e-01, 1.0320e-01, 1.2192e-01,  ..., 1.0730e-01,\n",
       "            1.0729e-01, 1.0729e-01],\n",
       "           [1.0108e-01, 8.4855e-02, 9.5783e-02,  ..., 9.6415e-02,\n",
       "            8.1732e-02, 6.2054e-02],\n",
       "           [9.4020e-02, 9.4293e-02, 9.5783e-02,  ..., 9.5916e-02,\n",
       "            8.2555e-02, 5.8325e-02]],\n",
       " \n",
       "          [[1.0763e+00, 1.1163e+00, 1.1268e+00,  ..., 1.1517e+00,\n",
       "            1.1517e+00, 1.1034e+00],\n",
       "           [7.0668e-01, 7.4157e-01, 7.4157e-01,  ..., 8.9578e-01,\n",
       "            6.8206e-01, 6.9092e-01],\n",
       "           [7.5803e-01, 7.2461e-01, 7.1634e-01,  ..., 7.8549e-01,\n",
       "            5.1840e-01, 5.1943e-01],\n",
       "           ...,\n",
       "           [5.0081e-01, 5.0081e-01, 4.7785e-01,  ..., 5.0609e-01,\n",
       "            5.0609e-01, 5.0369e-01],\n",
       "           [4.6592e-01, 4.4651e-01, 4.8560e-01,  ..., 4.6496e-01,\n",
       "            5.0050e-01, 5.0050e-01],\n",
       "           [4.8851e-01, 4.5332e-01, 4.9971e-01,  ..., 4.8748e-01,\n",
       "            4.7558e-01, 4.8304e-01]],\n",
       " \n",
       "          [[1.8203e-01, 2.1543e-01, 5.5697e-01,  ..., 3.7662e-01,\n",
       "            4.4225e-01, 4.4225e-01],\n",
       "           [0.0000e+00, 0.0000e+00, 5.5697e-01,  ..., 5.6340e-01,\n",
       "            4.4225e-01, 4.4225e-01],\n",
       "           [0.0000e+00, 0.0000e+00, 2.8035e-01,  ..., 1.3568e-01,\n",
       "            2.1564e-02, 2.1564e-02],\n",
       "           ...,\n",
       "           [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 1.3090e-02,\n",
       "            4.1719e-02, 4.1719e-02],\n",
       "           [3.4082e-02, 3.4082e-02, 0.0000e+00,  ..., 1.3090e-02,\n",
       "            1.3090e-02, 0.0000e+00],\n",
       "           [3.4082e-02, 3.4082e-02, 0.0000e+00,  ..., 2.5953e-02,\n",
       "            2.5953e-02, 0.0000e+00]]],\n",
       " \n",
       " \n",
       "         [[[3.5694e-01, 4.2578e-01, 4.4314e-01,  ..., 4.6826e-01,\n",
       "            4.5549e-01, 4.4630e-01],\n",
       "           [4.1999e-01, 4.8899e-01, 4.6668e-01,  ..., 4.1786e-01,\n",
       "            3.5445e-01, 3.5205e-01],\n",
       "           [4.1999e-01, 4.2717e-01, 4.1882e-01,  ..., 3.5241e-01,\n",
       "            3.5445e-01, 3.5205e-01],\n",
       "           ...,\n",
       "           [2.8665e-01, 3.0972e-01, 3.1071e-01,  ..., 1.6490e-01,\n",
       "            1.6760e-01, 1.6316e-01],\n",
       "           [3.2131e-01, 3.1774e-01, 3.1375e-01,  ..., 1.6602e-01,\n",
       "            1.7209e-01, 1.6316e-01],\n",
       "           [2.0564e-01, 2.0184e-01, 2.3767e-01,  ..., 3.2035e-01,\n",
       "            2.0878e-01, 1.5395e-01]],\n",
       " \n",
       "          [[6.2402e-01, 6.8478e-01, 6.5653e-01,  ..., 7.2280e-01,\n",
       "            7.1568e-01, 6.8715e-01],\n",
       "           [8.6816e-01, 9.4576e-01, 8.3126e-01,  ..., 6.8150e-01,\n",
       "            6.4678e-01, 6.2652e-01],\n",
       "           [8.6816e-01, 8.6107e-01, 7.8130e-01,  ..., 6.4165e-01,\n",
       "            5.8965e-01, 5.7262e-01],\n",
       "           ...,\n",
       "           [6.3352e-01, 6.5227e-01, 6.2894e-01,  ..., 4.4140e-01,\n",
       "            4.0898e-01, 4.0144e-01],\n",
       "           [6.2203e-01, 6.0028e-01, 6.1176e-01,  ..., 4.6979e-01,\n",
       "            4.4172e-01, 3.9361e-01],\n",
       "           [5.4278e-01, 5.1656e-01, 5.6583e-01,  ..., 5.9364e-01,\n",
       "            4.9374e-01, 3.9496e-01]],\n",
       " \n",
       "          [[0.0000e+00, 4.1519e-02, 5.3813e-02,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00],\n",
       "           [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 1.5787e-01,\n",
       "            2.2483e-01, 1.4854e-01],\n",
       "           [1.4296e-01, 2.1731e-01, 2.4536e-01,  ..., 2.4650e-01,\n",
       "            3.1936e-01, 3.6270e-01],\n",
       "           ...,\n",
       "           [2.7598e-01, 2.5848e-01, 2.8721e-01,  ..., 3.2140e-01,\n",
       "            3.1243e-01, 3.0013e-01],\n",
       "           [2.3411e-01, 2.6929e-01, 3.1837e-01,  ..., 2.8311e-01,\n",
       "            3.1243e-01, 3.0182e-01],\n",
       "           [2.2192e-01, 3.1835e-01, 2.6259e-01,  ..., 3.6169e-01,\n",
       "            2.4780e-01, 3.4906e-01]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[2.2117e-01, 2.1980e-01, 2.0957e-01,  ..., 2.3754e-01,\n",
       "            2.2862e-01, 2.2265e-01],\n",
       "           [3.0069e-01, 3.0091e-01, 2.3030e-01,  ..., 1.9570e-01,\n",
       "            1.7365e-01, 1.5638e-01],\n",
       "           [1.6829e-01, 1.8652e-01, 1.7204e-01,  ..., 1.4181e-01,\n",
       "            1.3830e-01, 1.4029e-01],\n",
       "           ...,\n",
       "           [9.3557e-02, 1.0505e-01, 1.0319e-01,  ..., 6.6859e-02,\n",
       "            6.4024e-02, 6.4534e-02],\n",
       "           [9.4293e-02, 9.5783e-02, 9.5916e-02,  ..., 6.6536e-02,\n",
       "            6.4870e-02, 5.6019e-02],\n",
       "           [7.0571e-02, 6.7651e-02, 7.7471e-02,  ..., 1.0154e-01,\n",
       "            7.7218e-02, 5.6019e-02]],\n",
       " \n",
       "          [[1.1163e+00, 1.1268e+00, 1.1225e+00,  ..., 1.1034e+00,\n",
       "            1.0687e+00, 1.1281e+00],\n",
       "           [7.4157e-01, 7.0820e-01, 8.9578e-01,  ..., 6.8206e-01,\n",
       "            6.9092e-01, 7.9028e-01],\n",
       "           [7.1634e-01, 7.0820e-01, 6.4139e-01,  ..., 5.0066e-01,\n",
       "            4.9423e-01, 4.8069e-01],\n",
       "           ...,\n",
       "           [4.6311e-01, 4.8560e-01, 4.7743e-01,  ..., 5.0050e-01,\n",
       "            4.8162e-01, 4.3426e-01],\n",
       "           [4.5332e-01, 4.9971e-01, 4.8748e-01,  ..., 4.7558e-01,\n",
       "            4.8304e-01, 4.7798e-01],\n",
       "           [4.4532e-01, 4.7361e-01, 4.6846e-01,  ..., 4.6555e-01,\n",
       "            4.6672e-01, 4.7102e-01]],\n",
       " \n",
       "          [[2.1543e-01, 5.5697e-01, 3.7662e-01,  ..., 4.4225e-01,\n",
       "            1.8031e-01, 3.7829e-01],\n",
       "           [0.0000e+00, 5.5697e-01, 5.6340e-01,  ..., 4.4225e-01,\n",
       "            1.1586e-01, 3.7829e-01],\n",
       "           [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 2.1564e-02,\n",
       "            0.0000e+00, 2.8884e-02],\n",
       "           ...,\n",
       "           [0.0000e+00, 0.0000e+00, 6.3527e-04,  ..., 4.1719e-02,\n",
       "            0.0000e+00, 3.2907e-02],\n",
       "           [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 8.8211e-02],\n",
       "           [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00]]],\n",
       " \n",
       " \n",
       "         ...,\n",
       " \n",
       " \n",
       "         [[[4.2578e-01, 4.7205e-01, 4.6826e-01,  ..., 3.8395e-01,\n",
       "            2.2753e-01, 3.2167e-01],\n",
       "           [4.8899e-01, 4.8899e-01, 4.1786e-01,  ..., 3.1062e-01,\n",
       "            3.2544e-01, 2.2662e-01],\n",
       "           [4.2717e-01, 4.2717e-01, 3.5445e-01,  ..., 3.1062e-01,\n",
       "            2.3732e-01, 2.3790e-02],\n",
       "           ...,\n",
       "           [3.0478e-01, 3.1071e-01, 1.6490e-01,  ..., 1.3417e-01,\n",
       "            2.3449e-02, 2.3449e-02],\n",
       "           [3.2200e-01, 3.1774e-01, 1.7209e-01,  ..., 1.7527e-01,\n",
       "            1.1079e-01, 2.3449e-02],\n",
       "           [3.2200e-01, 3.1774e-01, 2.7569e-01,  ..., 1.7276e-01,\n",
       "            1.2766e-01, 9.5090e-02]],\n",
       " \n",
       "          [[6.8478e-01, 7.2280e-01, 7.2280e-01,  ..., 5.4081e-01,\n",
       "            3.7042e-01, 3.9666e-01],\n",
       "           [9.4576e-01, 9.4576e-01, 6.8150e-01,  ..., 5.1995e-01,\n",
       "            7.2803e-01, 3.1872e-01],\n",
       "           [8.6816e-01, 8.6107e-01, 6.4165e-01,  ..., 5.1995e-01,\n",
       "            5.5762e-01, 7.1785e-02],\n",
       "           ...,\n",
       "           [6.5227e-01, 6.5227e-01, 4.4140e-01,  ..., 3.4046e-01,\n",
       "            1.0241e-01, 9.9990e-02],\n",
       "           [6.3657e-01, 6.1176e-01, 4.4140e-01,  ..., 3.3110e-01,\n",
       "            2.7634e-01, 1.0267e-01],\n",
       "           [6.3657e-01, 6.1176e-01, 5.5759e-01,  ..., 3.2845e-01,\n",
       "            2.7996e-01, 2.3772e-01]],\n",
       " \n",
       "          [[7.5056e-02, 1.2500e-01, 0.0000e+00,  ..., 1.2529e-02,\n",
       "            2.3764e-01, 1.2676e-01],\n",
       "           [9.2545e-02, 3.2123e-01, 3.3386e-01,  ..., 3.2003e-01,\n",
       "            4.5118e-01, 3.1418e-02],\n",
       "           [9.2545e-02, 3.2123e-01, 3.3386e-01,  ..., 3.2003e-01,\n",
       "            2.3307e-01, 1.0617e-01],\n",
       "           ...,\n",
       "           [2.7598e-01, 3.2140e-01, 3.2140e-01,  ..., 3.7752e-01,\n",
       "            3.7752e-01, 0.0000e+00],\n",
       "           [3.4498e-01, 3.3184e-01, 3.2140e-01,  ..., 3.7124e-01,\n",
       "            2.9332e-01, 3.2942e-02],\n",
       "           [3.4498e-01, 3.2465e-01, 3.1449e-01,  ..., 4.1633e-01,\n",
       "            3.5765e-01, 3.0468e-01]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[2.2117e-01, 2.3754e-01, 2.3754e-01,  ..., 2.3377e-01,\n",
       "            2.3705e-01, 2.3848e-01],\n",
       "           [3.0091e-01, 3.0091e-01, 1.9570e-01,  ..., 1.6819e-01,\n",
       "            3.1797e-01, 1.9060e-01],\n",
       "           [1.8652e-01, 1.8652e-01, 1.4245e-01,  ..., 1.4946e-01,\n",
       "            1.7680e-01, 2.1346e-02],\n",
       "           ...,\n",
       "           [1.0505e-01, 1.0505e-01, 7.5905e-02,  ..., 1.0001e-01,\n",
       "            2.2159e-02, 2.1708e-02],\n",
       "           [9.5783e-02, 9.6415e-02, 6.2186e-02,  ..., 9.5152e-02,\n",
       "            9.5510e-02, 2.2387e-02],\n",
       "           [9.4301e-02, 9.5371e-02, 9.2862e-02,  ..., 9.2931e-02,\n",
       "            9.9396e-02, 9.3995e-02]],\n",
       " \n",
       "          [[1.1268e+00, 1.1517e+00, 1.1550e+00,  ..., 1.2825e+00,\n",
       "            1.2825e+00, 1.2143e+00],\n",
       "           [7.4157e-01, 8.9578e-01, 7.9028e-01,  ..., 8.2212e-01,\n",
       "            9.9399e-01, 9.9399e-01],\n",
       "           [7.2461e-01, 7.0820e-01, 5.0093e-01,  ..., 5.1343e-01,\n",
       "            7.9493e-01, 7.7254e-01],\n",
       "           ...,\n",
       "           [4.7789e-01, 5.0050e-01, 5.0050e-01,  ..., 7.5556e-01,\n",
       "            8.4623e-01, 8.4623e-01],\n",
       "           [4.9971e-01, 5.0050e-01, 5.0050e-01,  ..., 7.5556e-01,\n",
       "            9.9654e-01, 1.1348e+00],\n",
       "           [4.9971e-01, 4.9971e-01, 4.8304e-01,  ..., 4.5229e-01,\n",
       "            9.4860e-01, 1.1348e+00]],\n",
       " \n",
       "          [[5.5697e-01, 5.5697e-01, 4.4225e-01,  ..., 5.3656e-01,\n",
       "            7.8535e-01, 1.1285e+00],\n",
       "           [5.5697e-01, 5.6340e-01, 4.4225e-01,  ..., 5.3656e-01,\n",
       "            7.8535e-01, 1.1285e+00],\n",
       "           [0.0000e+00, 2.1564e-02, 2.8884e-02,  ..., 1.4906e-01,\n",
       "            2.2859e-01, 0.0000e+00],\n",
       "           ...,\n",
       "           [0.0000e+00, 4.1719e-02, 4.1719e-02,  ..., 0.0000e+00,\n",
       "            2.6870e-01, 2.8504e-01],\n",
       "           [3.4082e-02, 2.5953e-02, 8.8211e-02,  ..., 4.5142e-02,\n",
       "            1.7334e-01, 2.8504e-01],\n",
       "           [0.0000e+00, 2.5953e-02, 0.0000e+00,  ..., 7.2830e-02,\n",
       "            1.7334e-01, 2.1243e-01]]],\n",
       " \n",
       " \n",
       "         [[[3.9688e-01, 3.7240e-01, 3.6038e-01,  ..., 1.9904e-01,\n",
       "            2.3376e-02, 2.3411e-02],\n",
       "           [3.8990e-01, 3.9127e-01, 3.1071e-01,  ..., 1.3417e-01,\n",
       "            2.3449e-02, 2.3263e-02],\n",
       "           [3.5922e-01, 3.3210e-01, 3.2200e-01,  ..., 1.7527e-01,\n",
       "            9.6079e-02, 2.3190e-02],\n",
       "           ...,\n",
       "           [2.0678e-01, 2.0436e-01, 1.9247e-01,  ..., 1.0971e-01,\n",
       "            1.1373e-01, 1.4218e-01],\n",
       "           [1.6836e-01, 1.3857e-01, 1.1572e-01,  ..., 2.3220e-02,\n",
       "            9.9146e-02, 2.1071e-01],\n",
       "           [1.5261e-01, 1.3775e-01, 1.0353e-01,  ..., 3.5342e-02,\n",
       "            9.2686e-02, 9.6148e-02]],\n",
       " \n",
       "          [[8.5329e-01, 7.9419e-01, 7.1953e-01,  ..., 4.2555e-01,\n",
       "            2.9110e-01, 1.8130e-01],\n",
       "           [8.0068e-01, 7.9942e-01, 6.5227e-01,  ..., 3.4046e-01,\n",
       "            1.0241e-01, 1.0106e-01],\n",
       "           [7.8271e-01, 7.4599e-01, 6.3657e-01,  ..., 3.5700e-01,\n",
       "            2.7634e-01, 1.0106e-01],\n",
       "           ...,\n",
       "           [6.3662e-01, 5.8229e-01, 5.2027e-01,  ..., 3.2554e-01,\n",
       "            2.2585e-01, 2.3861e-01],\n",
       "           [6.0990e-01, 5.1851e-01, 4.4250e-01,  ..., 2.4203e-01,\n",
       "            2.1132e-01, 2.5873e-01],\n",
       "           [6.0001e-01, 5.5005e-01, 4.5686e-01,  ..., 2.5958e-01,\n",
       "            2.1132e-01, 1.7811e-01]],\n",
       " \n",
       "          [[2.4804e-01, 3.3963e-01, 3.7929e-01,  ..., 3.7752e-01,\n",
       "            1.4224e-01, 6.0704e-01],\n",
       "           [2.6212e-01, 3.3732e-01, 3.0830e-01,  ..., 3.7752e-01,\n",
       "            2.2745e-01, 4.4705e-01],\n",
       "           [3.4276e-01, 3.3732e-01, 3.4498e-01,  ..., 3.6242e-01,\n",
       "            2.9247e-01, 4.0609e-01],\n",
       "           ...,\n",
       "           [2.6922e-01, 3.3634e-01, 2.7866e-01,  ..., 3.2208e-01,\n",
       "            3.1239e-01, 3.5526e-01],\n",
       "           [2.6853e-01, 3.6780e-01, 3.4903e-01,  ..., 3.3827e-01,\n",
       "            3.0356e-01, 3.3297e-01],\n",
       "           [2.8342e-01, 3.3805e-01, 3.3246e-01,  ..., 2.7935e-01,\n",
       "            3.5563e-01, 3.3297e-01]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[1.4605e-01, 1.2244e-01, 1.2192e-01,  ..., 1.0429e-01,\n",
       "            8.4233e-02, 2.2615e-01],\n",
       "           [1.3692e-01, 1.2774e-01, 1.0505e-01,  ..., 1.0001e-01,\n",
       "            2.2387e-02, 5.6110e-02],\n",
       "           [8.9082e-02, 1.0482e-01, 9.5916e-02,  ..., 9.5152e-02,\n",
       "            9.5510e-02, 6.8052e-02],\n",
       "           ...,\n",
       "           [1.8350e-02, 1.8057e-02, 5.9500e-02,  ..., 5.8559e-02,\n",
       "            8.9431e-02, 1.4618e-01],\n",
       "           [1.8369e-02, 1.8732e-02, 1.8784e-02,  ..., 1.8791e-02,\n",
       "            9.3334e-02, 1.5623e-01],\n",
       "           [1.8218e-02, 1.8938e-02, 1.8784e-02,  ..., 1.8382e-02,\n",
       "            9.5478e-02, 1.1943e-01]],\n",
       " \n",
       "          [[5.1309e-01, 5.4153e-01, 5.7338e-01,  ..., 6.2597e-01,\n",
       "            8.4623e-01, 1.4220e+00],\n",
       "           [5.2564e-01, 5.0622e-01, 4.8560e-01,  ..., 7.5556e-01,\n",
       "            8.4623e-01, 7.9821e-01],\n",
       "           [5.2564e-01, 5.0641e-01, 4.9971e-01,  ..., 6.5389e-01,\n",
       "            1.1348e+00, 8.4907e-01],\n",
       "           ...,\n",
       "           [5.2019e-01, 4.7861e-01, 5.3100e-01,  ..., 4.7707e-01,\n",
       "            4.8377e-01, 4.6629e-01],\n",
       "           [5.4512e-01, 4.9181e-01, 5.3100e-01,  ..., 4.7707e-01,\n",
       "            4.6468e-01, 4.4674e-01],\n",
       "           [5.0600e-01, 4.7062e-01, 4.8632e-01,  ..., 4.5694e-01,\n",
       "            4.6225e-01, 4.4376e-01]],\n",
       " \n",
       "          [[0.0000e+00, 1.8111e-02, 8.6169e-03,  ..., 4.7850e-02,\n",
       "            2.2708e-01, 7.8149e-01],\n",
       "           [0.0000e+00, 8.9905e-04, 3.4082e-02,  ..., 8.8679e-03,\n",
       "            2.8504e-01, 3.1155e-01],\n",
       "           [0.0000e+00, 0.0000e+00, 3.4082e-02,  ..., 4.5142e-02,\n",
       "            2.2780e-01, 3.7168e-01],\n",
       "           ...,\n",
       "           [5.4369e-03, 2.2067e-02, 3.7042e-02,  ..., 4.5763e-02,\n",
       "            1.8552e-01, 2.7671e-01],\n",
       "           [2.8559e-02, 1.7543e-02, 4.3378e-02,  ..., 3.4927e-02,\n",
       "            7.9138e-02, 1.3013e-01],\n",
       "           [0.0000e+00, 3.0353e-02, 3.8632e-02,  ..., 1.2120e-01,\n",
       "            1.3264e-01, 1.0310e-01]]],\n",
       " \n",
       " \n",
       "         [[[3.9841e-01, 3.9841e-01, 3.5694e-01,  ..., 4.4314e-01,\n",
       "            4.7205e-01, 4.7205e-01],\n",
       "           [3.9892e-01, 3.9892e-01, 4.1999e-01,  ..., 4.8899e-01,\n",
       "            4.6668e-01, 3.9658e-01],\n",
       "           [3.2527e-01, 3.2527e-01, 2.8574e-01,  ..., 3.6038e-01,\n",
       "            3.1071e-01, 2.9240e-01],\n",
       "           ...,\n",
       "           [3.2745e-01, 3.2745e-01, 3.2200e-01,  ..., 3.1774e-01,\n",
       "            3.2800e-01, 3.2800e-01],\n",
       "           [3.2735e-01, 3.2735e-01, 2.9439e-01,  ..., 2.3276e-01,\n",
       "            3.2800e-01, 3.2800e-01],\n",
       "           [1.2156e-01, 1.2156e-01, 1.1572e-01,  ..., 1.7531e-01,\n",
       "            1.7531e-01, 1.7244e-01]],\n",
       " \n",
       "          [[8.9454e-01, 8.9454e-01, 7.8058e-01,  ..., 8.2693e-01,\n",
       "            7.3769e-01, 6.8297e-01],\n",
       "           [9.3925e-01, 9.3925e-01, 8.6816e-01,  ..., 9.4576e-01,\n",
       "            8.3126e-01, 7.0956e-01],\n",
       "           [7.2161e-01, 7.2161e-01, 6.7065e-01,  ..., 7.1953e-01,\n",
       "            6.3304e-01, 6.0821e-01],\n",
       "           ...,\n",
       "           [6.6010e-01, 6.6010e-01, 6.3657e-01,  ..., 6.1176e-01,\n",
       "            6.2284e-01, 6.2284e-01],\n",
       "           [6.6010e-01, 6.6010e-01, 6.2125e-01,  ..., 5.6583e-01,\n",
       "            6.2284e-01, 6.2284e-01],\n",
       "           [4.5928e-01, 4.5928e-01, 4.5396e-01,  ..., 4.7125e-01,\n",
       "            4.6236e-01, 4.5830e-01]],\n",
       " \n",
       "          [[1.3521e-01, 1.3521e-01, 7.5056e-02,  ..., 5.3813e-02,\n",
       "            1.2500e-01, 1.2500e-01],\n",
       "           [0.0000e+00, 9.2545e-02, 9.2545e-02,  ..., 2.1701e-01,\n",
       "            3.2123e-01, 3.2123e-01],\n",
       "           [3.0649e-01, 3.0649e-01, 2.7598e-01,  ..., 3.3011e-01,\n",
       "            3.7929e-01, 3.7929e-01],\n",
       "           ...,\n",
       "           [3.6332e-01, 3.6332e-01, 3.4498e-01,  ..., 3.1837e-01,\n",
       "            3.3184e-01, 3.3184e-01],\n",
       "           [3.6332e-01, 3.6332e-01, 3.1276e-01,  ..., 3.6667e-01,\n",
       "            3.0534e-01, 2.8792e-01],\n",
       "           [3.0521e-01, 3.3010e-01, 3.3010e-01,  ..., 3.2795e-01,\n",
       "            3.4903e-01, 3.4903e-01]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[2.9306e-01, 2.9746e-01, 3.0069e-01,  ..., 3.0091e-01,\n",
       "            2.2553e-01, 2.2327e-01],\n",
       "           [2.9306e-01, 2.9746e-01, 3.0069e-01,  ..., 3.0091e-01,\n",
       "            2.3030e-01, 1.6739e-01],\n",
       "           [1.1855e-01, 1.1855e-01, 1.0320e-01,  ..., 1.2192e-01,\n",
       "            1.0730e-01, 1.0673e-01],\n",
       "           ...,\n",
       "           [8.9057e-02, 9.2970e-02, 9.4293e-02,  ..., 9.5156e-02,\n",
       "            9.9975e-02, 9.9975e-02],\n",
       "           [8.7443e-02, 8.7443e-02, 8.3146e-02,  ..., 7.5796e-02,\n",
       "            9.9975e-02, 9.9975e-02],\n",
       "           [1.7606e-02, 1.7606e-02, 1.7843e-02,  ..., 4.0356e-02,\n",
       "            4.2022e-02, 4.2022e-02]],\n",
       " \n",
       "          [[1.0618e+00, 1.0763e+00, 1.1163e+00,  ..., 1.1268e+00,\n",
       "            1.1517e+00, 1.1517e+00],\n",
       "           [7.5803e-01, 7.5803e-01, 7.2461e-01,  ..., 8.9578e-01,\n",
       "            8.9578e-01, 5.2998e-01],\n",
       "           [5.1394e-01, 5.2293e-01, 5.2293e-01,  ..., 5.7338e-01,\n",
       "            5.7338e-01, 5.5402e-01],\n",
       "           ...,\n",
       "           [4.8851e-01, 4.8851e-01, 4.5332e-01,  ..., 4.9971e-01,\n",
       "            4.8748e-01, 4.6214e-01],\n",
       "           [4.5077e-01, 4.5077e-01, 4.5330e-01,  ..., 4.6846e-01,\n",
       "            4.6846e-01, 4.6104e-01],\n",
       "           [4.7070e-01, 4.8428e-01, 5.3100e-01,  ..., 4.6169e-01,\n",
       "            4.6169e-01, 4.6104e-01]],\n",
       " \n",
       "          [[8.6136e-02, 1.8203e-01, 2.1543e-01,  ..., 5.6340e-01,\n",
       "            5.6340e-01, 9.4054e-02],\n",
       "           [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 5.6340e-01,\n",
       "            5.6340e-01, 0.0000e+00],\n",
       "           [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 6.3527e-04,\n",
       "            1.3090e-02, 1.3090e-02],\n",
       "           ...,\n",
       "           [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
       "            2.5953e-02, 2.5953e-02],\n",
       "           [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 3.7042e-02,\n",
       "            3.7042e-02, 0.0000e+00],\n",
       "           [4.6387e-03, 4.6387e-03, 0.0000e+00,  ..., 1.4256e-02,\n",
       "            4.3378e-02, 4.3378e-02]]]], device='cuda:0', grad_fn=<CatBackward0>),\n",
       " 'boxes': tensor([[ 9.4731e-02, -9.4016e-02, -1.0667e-01, -3.0740e-01],\n",
       "         [ 8.5442e-02,  1.6696e-01, -1.2241e-01, -2.9799e-01],\n",
       "         [-1.1439e-02, -3.1734e-02, -3.0331e-01, -5.5459e-02],\n",
       "         [-4.9440e-02,  1.3268e-01, -3.8429e-01, -8.0810e-03],\n",
       "         [-1.4455e-01, -2.8862e-02, -6.2139e-01,  8.9190e-03],\n",
       "         [-5.1813e-01,  5.7685e-01, -9.6900e-01, -8.3049e-01],\n",
       "         [-3.1842e-01,  5.8730e-01, -9.7964e-01, -8.2083e-01],\n",
       "         [ 8.8563e-01,  3.8519e-01,  7.3913e-01, -1.0265e+00],\n",
       "         [ 1.3810e-01,  3.4714e-01,  3.4135e-01, -1.0704e+00],\n",
       "         [ 2.6862e-01,  4.4060e-01,  3.3288e-01, -9.6577e-01],\n",
       "         [-5.1479e-01,  4.9703e-01,  3.9185e-01, -9.0751e-01],\n",
       "         [ 7.4261e-03,  4.7938e-01,  3.7339e-01, -9.2537e-01],\n",
       "         [-9.1381e-01,  4.9822e-01,  3.8523e-01, -9.0631e-01],\n",
       "         [-3.3826e-01,  2.2987e-01, -1.0208e+00, -1.2194e+00],\n",
       "         [-3.8210e-01,  2.1271e-02, -1.0239e+00, -1.3688e+00],\n",
       "         [-8.0021e-02,  3.5977e-01, -4.5457e-01, -1.0556e+00],\n",
       "         [-6.5479e-01, -4.1033e-01, -9.8747e-01, -1.3775e+00],\n",
       "         [-2.2019e-01,  3.8829e-01, -8.6068e-01, -1.0230e+00],\n",
       "         [-5.9133e-01, -2.7958e-01, -1.0040e+00, -1.3752e+00],\n",
       "         [-4.7785e-01,  1.3701e-01, -1.0207e+00, -1.3555e+00],\n",
       "         [-4.8735e-01, -1.8880e-01, -1.0113e+00, -1.3799e+00],\n",
       "         [-4.3998e-01, -4.5444e-01, -1.0198e+00, -1.3801e+00],\n",
       "         [ 9.7193e-01, -1.0421e-01,  7.9955e-01, -1.5199e+00],\n",
       "         [-4.1863e-01,  2.7415e-01,  3.2640e-01, -1.1605e+00],\n",
       "         [-1.2622e-01,  1.7153e-01, -5.7110e-01, -1.3027e+00],\n",
       "         [-2.9206e-02,  2.3484e-01,  3.0056e-01, -1.2126e+00],\n",
       "         [ 9.5907e-01,  6.8780e-02,  7.9078e-01, -1.4688e+00],\n",
       "         [-2.2749e-01, -9.8617e-02, -3.4499e-01, -1.8174e+00],\n",
       "         [-3.8847e-01, -3.1585e-01, -1.0251e+00, -1.3812e+00],\n",
       "         [ 1.1639e+00,  5.7647e-01,  9.2217e-01, -8.3084e-01],\n",
       "         [ 6.3548e-01, -5.0715e-01,  5.4003e-01, -1.5492e+00],\n",
       "         [-2.1487e-01,  6.7791e-02, -8.4184e-01, -1.3709e+00],\n",
       "         [ 2.7134e-01, -2.4378e-01,  1.5335e-01, -2.1530e+00],\n",
       "         [-2.5910e-01, -1.4369e-01, -1.0104e+00, -1.3781e+00],\n",
       "         [ 5.9571e-01,  1.5381e-01,  5.0438e-01, -1.3295e+00],\n",
       "         [-2.5800e-01, -4.0417e-01, -1.0058e+00, -1.3855e+00],\n",
       "         [ 2.1096e-01, -5.0363e-01,  3.2197e-01, -1.5485e+00],\n",
       "         [-8.5781e-02,  1.3601e-02, -4.6838e-01, -1.3827e+00],\n",
       "         [-4.3134e-01, -8.8431e-02, -4.4477e-01, -1.7923e+00],\n",
       "         [-3.1525e-01, -5.3165e-01, -1.0397e+00, -1.3891e+00],\n",
       "         [ 5.0607e-01, -2.7847e-01,  4.1903e-01, -1.5480e+00],\n",
       "         [-7.9530e-01, -1.2709e-01,  3.3784e-01, -1.5634e+00],\n",
       "         [-2.8341e-01, -2.1688e-01, -4.6627e-01, -2.1664e+00],\n",
       "         [-1.5431e-01, -5.4194e-01, -6.4924e-01, -1.3857e+00],\n",
       "         [-3.9912e-01, -9.1804e-02,  3.3257e-01, -1.5556e+00],\n",
       "         [-8.4920e-02, -3.7427e-01, -4.6631e-01, -1.3812e+00],\n",
       "         [-3.9492e-01, -4.2052e-01,  3.3540e-01, -1.5542e+00],\n",
       "         [-1.0857e+00, -4.5509e-01,  3.3112e-01, -1.5560e+00],\n",
       "         [-8.8295e-01, -5.6103e-01,  3.3358e-01, -1.5651e+00],\n",
       "         [-4.9467e-01, -3.1007e-01,  3.3006e-01, -1.5499e+00],\n",
       "         [ 5.1215e-01, -8.7932e-02,  4.2505e-01, -1.5533e+00],\n",
       "         [ 1.3581e-01, -1.3472e-01, -3.9875e-02, -1.9116e+00],\n",
       "         [-1.9343e-01,  2.4477e-02,  3.3867e-01, -1.5499e+00],\n",
       "         [ 2.0719e-01,  2.4468e-02,  3.3093e-01, -1.5481e+00],\n",
       "         [-8.8744e-01,  2.2741e-02,  3.3651e-01, -1.5532e+00],\n",
       "         [ 4.1107e-01,  4.4512e-02,  3.4095e-01, -1.5124e+00],\n",
       "         [-1.0940e+00, -8.1628e-02,  3.4348e-01, -1.5661e+00],\n",
       "         [-2.8699e-01, -3.0543e-01,  3.2826e-01, -1.5518e+00],\n",
       "         [-7.0087e-02, -1.6519e-01, -4.3120e-01, -1.9987e+00],\n",
       "         [-2.9245e-01,  1.1545e-02, -4.4919e-01, -1.5748e+00],\n",
       "         [-4.1869e-01, -2.7548e-01, -4.9132e-01, -2.1861e+00],\n",
       "         [-9.8817e-01, -2.6380e-01,  3.3889e-01, -1.5712e+00],\n",
       "         [-6.9490e-01,  4.3030e-02,  3.3057e-01, -1.5151e+00],\n",
       "         [-7.9307e-01,  2.5440e-01,  3.2417e-01, -1.1864e+00],\n",
       "         [-2.7496e-01, -4.0582e-01, -5.1226e-01, -2.0891e+00],\n",
       "         [ 1.6254e-02, -2.4733e-01, -2.4818e-01, -2.1956e+00],\n",
       "         [-8.4454e-02, -3.8159e-01, -4.6518e-01, -2.1162e+00],\n",
       "         [ 5.0908e-03, -5.5494e-01, -2.7004e-01, -1.9367e+00],\n",
       "         [ 4.0320e-01, -2.3205e-01,  3.1117e-01, -1.1581e+00],\n",
       "         [ 4.2273e-01, -6.3247e-03,  3.3256e-01, -1.1559e+00],\n",
       "         [-1.0079e-02, -5.3319e-01, -3.0053e-01, -5.9715e-02],\n",
       "         [-1.6318e-02,  3.0223e-01, -3.1335e-01, -3.7879e-02],\n",
       "         [-4.1153e-02, -6.9389e-01, -3.6606e-01, -5.5839e-02],\n",
       "         [-4.6738e-02, -2.0961e-01, -3.7831e-01, -3.5966e-02],\n",
       "         [-5.1056e-02,  4.7464e-01, -3.8788e-01, -4.6473e-03],\n",
       "         [-5.0125e-02,  8.2895e-01, -3.8581e-01,  5.4286e-04],\n",
       "         [-1.5097e-01, -8.6774e-01, -6.3962e-01, -3.8313e-02],\n",
       "         [-4.8887e-01,  6.0368e-01, -9.8735e-01, -8.5638e-02],\n",
       "         [ 2.9921e-01, -6.2447e-01,  1.8884e-01, -1.2206e+00],\n",
       "         [-5.1709e-01,  7.7372e-01, -9.6787e-01, -8.0416e-02],\n",
       "         [-6.6612e-01, -6.1856e-01, -3.1676e-01, -1.2220e+00],\n",
       "         [-2.0933e-01, -7.1969e-01, -8.2258e-01, -5.2283e-03],\n",
       "         [-3.5050e-01, -6.1936e-01, -3.2809e-01, -1.2275e+00],\n",
       "         [-1.5994e-01,  6.9975e-01, -6.6566e-01,  4.2829e-02],\n",
       "         [-1.4784e-01,  3.3570e-01, -6.3070e-01,  3.6758e-02],\n",
       "         [-6.5734e-01, -4.1238e-01, -3.3416e-01, -1.2315e+00],\n",
       "         [-1.4674e-01, -5.4878e-01, -6.2757e-01,  3.1365e-03],\n",
       "         [-2.1336e-01,  5.1050e-01, -8.3655e-01,  2.2756e-02],\n",
       "         [-5.0284e-01, -4.1334e-02, -1.0295e+00, -1.2688e-02],\n",
       "         [ 2.2691e-01, -3.6011e-01,  9.4024e-02, -1.2268e+00],\n",
       "         [-5.5651e-01, -5.0842e-01, -3.3102e-01, -1.2358e+00],\n",
       "         [-1.8335e-01,  1.6767e-01, -7.3700e-01,  3.4477e-02],\n",
       "         [-8.2471e-01, -3.5366e-01, -3.2404e-01, -1.2373e+00],\n",
       "         [-4.8929e-01,  5.7840e-01, -3.4353e-01, -8.2905e-01],\n",
       "         [-4.5961e-01,  3.2149e-01, -1.0213e+00, -5.5859e-03],\n",
       "         [-5.5893e-01, -2.0324e-01, -1.0266e+00, -2.4031e-02],\n",
       "         [-2.4163e-01,  3.5059e-01, -9.4040e-01, -9.8582e-03],\n",
       "         [ 2.0467e-02, -3.0593e-01, -2.4005e-01, -1.2326e+00],\n",
       "         [-4.9558e-01,  9.9127e-02, -3.4041e-01, -1.2396e+00],\n",
       "         [-2.4318e-01, -6.5995e-04, -9.4639e-01, -9.5704e-04],\n",
       "         [-2.3714e-01, -4.5454e-01, -3.3990e-01, -1.2335e+00],\n",
       "         [ 5.9571e-01,  1.5381e-01,  5.0438e-01, -1.3295e+00],\n",
       "         [-6.6612e-01, -6.1856e-01, -3.1676e-01, -1.2220e+00],\n",
       "         [-5.5893e-01, -2.0324e-01, -1.0266e+00, -2.4031e-02],\n",
       "         [-2.7496e-01, -4.0582e-01, -5.1226e-01, -2.0891e+00],\n",
       "         [-5.0284e-01, -4.1334e-02, -1.0295e+00, -1.2688e-02],\n",
       "         [-1.6318e-02,  3.0223e-01, -3.1335e-01, -3.7879e-02],\n",
       "         [ 1.3581e-01, -1.3472e-01, -3.9875e-02, -1.9116e+00],\n",
       "         [-1.6318e-02,  3.0223e-01, -3.1335e-01, -3.7879e-02],\n",
       "         [-4.6738e-02, -2.0961e-01, -3.7831e-01, -3.5966e-02],\n",
       "         [-4.1869e-01, -2.7548e-01, -4.9132e-01, -2.1861e+00],\n",
       "         [ 1.6254e-02, -2.4733e-01, -2.4818e-01, -2.1956e+00],\n",
       "         [-8.0021e-02,  3.5977e-01, -4.5457e-01, -1.0556e+00],\n",
       "         [-6.6612e-01, -6.1856e-01, -3.1676e-01, -1.2220e+00],\n",
       "         [-1.4784e-01,  3.3570e-01, -6.3070e-01,  3.6758e-02],\n",
       "         [-8.5781e-02,  1.3601e-02, -4.6838e-01, -1.3827e+00],\n",
       "         [-9.8817e-01, -2.6380e-01,  3.3889e-01, -1.5712e+00],\n",
       "         [-3.1525e-01, -5.3165e-01, -1.0397e+00, -1.3891e+00],\n",
       "         [ 4.0320e-01, -2.3205e-01,  3.1117e-01, -1.1581e+00],\n",
       "         [-1.1439e-02, -3.1734e-02, -3.0331e-01, -5.5459e-02],\n",
       "         [-8.2471e-01, -3.5366e-01, -3.2404e-01, -1.2373e+00],\n",
       "         [ 1.6254e-02, -2.4733e-01, -2.4818e-01, -2.1956e+00],\n",
       "         [-8.8744e-01,  2.2741e-02,  3.3651e-01, -1.5532e+00],\n",
       "         [-2.8341e-01, -2.1688e-01, -4.6627e-01, -2.1664e+00],\n",
       "         [-5.1056e-02,  4.7464e-01, -3.8788e-01, -4.6473e-03],\n",
       "         [ 2.0467e-02, -3.0593e-01, -2.4005e-01, -1.2326e+00],\n",
       "         [-4.3134e-01, -8.8431e-02, -4.4477e-01, -1.7923e+00],\n",
       "         [-1.4784e-01,  3.3570e-01, -6.3070e-01,  3.6758e-02]], device='cuda:0',\n",
       "        dtype=torch.float64),\n",
       " 'labels': tensor([ 412,  412,  412,  412,  412, 1000, 1000, 1000, 1000, 1000, 1000, 1000,\n",
       "         1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000,\n",
       "         1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000,\n",
       "         1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000,\n",
       "         1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000,\n",
       "         1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000,\n",
       "         1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000,\n",
       "         1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000,\n",
       "         1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000,\n",
       "         1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000,  412,\n",
       "         1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000], device='cuda:0'),\n",
       " 'non_reparam_box': tensor([[0.0000e+00, 4.4107e+01, 6.0593e+01, 7.4369e+01],\n",
       "         [0.0000e+00, 3.6144e+01, 6.1570e+01, 6.6113e+01],\n",
       "         [0.0000e+00, 4.5420e+01, 7.3977e+01, 6.8719e+01],\n",
       "         [0.0000e+00, 4.2136e+01, 8.0301e+01, 6.4311e+01],\n",
       "         [0.0000e+00, 4.6064e+01, 1.0206e+02, 6.7848e+01],\n",
       "         [3.9276e+01, 0.0000e+00, 1.8417e+02, 5.1745e+01],\n",
       "         [9.8567e+00, 0.0000e+00, 1.5631e+02, 5.1238e+01],\n",
       "         [0.0000e+00, 0.0000e+00, 2.5436e+01, 6.3166e+01],\n",
       "         [1.1521e+01, 0.0000e+00, 4.9872e+01, 6.6047e+01],\n",
       "         [6.1278e+00, 0.0000e+00, 4.4813e+01, 5.9386e+01],\n",
       "         [3.7184e+01, 0.0000e+00, 7.3597e+01, 5.5968e+01],\n",
       "         [1.7293e+01, 0.0000e+00, 5.4403e+01, 5.6995e+01],\n",
       "         [5.2216e+01, 0.0000e+00, 8.8878e+01, 5.5900e+01],\n",
       "         [1.1782e+01, 0.0000e+00, 1.6442e+02, 7.6820e+01],\n",
       "         [1.8461e+01, 9.6969e+00, 1.7158e+02, 9.9056e+01],\n",
       "         [0.0000e+00, 0.0000e+00, 8.6221e+01, 6.5063e+01],\n",
       "         [5.9634e+01, 4.8626e+01, 2.0725e+02, 1.3877e+02],\n",
       "         [0.0000e+00, 0.0000e+00, 1.2992e+02, 6.2942e+01],\n",
       "         [5.0430e+01, 3.6753e+01, 2.0052e+02, 1.2670e+02],\n",
       "         [3.3229e+01, 0.0000e+00, 1.8587e+02, 8.8165e+01],\n",
       "         [3.4706e+01, 2.8366e+01, 1.8591e+02, 1.1873e+02],\n",
       "         [2.7418e+01, 5.2635e+01, 1.7991e+02, 1.4302e+02],\n",
       "         [0.0000e+00, 1.5204e+01, 2.3886e+01, 1.1930e+02],\n",
       "         [3.3381e+01, 0.0000e+00, 7.2325e+01, 7.2369e+01],\n",
       "         [0.0000e+00, 0.0000e+00, 9.7000e+01, 8.3581e+01],\n",
       "         [1.7334e+01, 0.0000e+00, 5.7323e+01, 7.6294e+01],\n",
       "         [0.0000e+00, 0.0000e+00, 2.4106e+01, 9.8861e+01],\n",
       "         [1.5330e+01, 0.0000e+00, 9.2498e+01, 1.4051e+02],\n",
       "         [1.9421e+01, 3.9952e+01, 1.7273e+02, 1.3044e+02],\n",
       "         [0.0000e+00, 0.0000e+00, 2.1015e+01, 5.1764e+01],\n",
       "         [0.0000e+00, 5.7573e+01, 3.1260e+01, 1.6480e+02],\n",
       "         [0.0000e+00, 5.3853e+00, 1.2747e+02, 9.4935e+01],\n",
       "         [0.0000e+00, 6.0815e+00, 4.6490e+01, 2.0302e+02],\n",
       "         [0.0000e+00, 2.4301e+01, 1.5106e+02, 1.1451e+02],\n",
       "         [0.0000e+00, 0.0000e+00, 3.2431e+01, 8.5873e+01],\n",
       "         [0.0000e+00, 4.7994e+01, 1.5037e+02, 1.3887e+02],\n",
       "         [8.1066e+00, 5.7192e+01, 4.7228e+01, 1.6433e+02],\n",
       "         [0.0000e+00, 9.7420e+00, 8.7434e+01, 1.0036e+02],\n",
       "         [3.0701e+01, 0.0000e+00, 1.1607e+02, 1.3701e+02],\n",
       "         [7.7028e+00, 5.9717e+01, 1.6329e+02, 1.5093e+02],\n",
       "         [0.0000e+00, 3.2853e+01, 3.5410e+01, 1.3995e+02],\n",
       "         [4.8292e+01, 1.5866e+01, 8.6782e+01, 1.2463e+02],\n",
       "         [1.7518e+01, 0.0000e+00, 1.0476e+02, 1.9962e+02],\n",
       "         [0.0000e+00, 6.0653e+01, 1.0497e+02, 1.5155e+02],\n",
       "         [3.2626e+01, 1.2338e+01, 7.1325e+01, 1.2026e+02],\n",
       "         [0.0000e+00, 4.5297e+01, 8.7251e+01, 1.3578e+02],\n",
       "         [3.2471e+01, 4.8153e+01, 7.1057e+01, 1.5592e+02],\n",
       "         [5.9918e+01, 5.1905e+01, 9.8674e+01, 1.5987e+02],\n",
       "         [5.1818e+01, 6.3509e+01, 9.0476e+01, 1.7247e+02],\n",
       "         [3.6419e+01, 3.6230e+01, 7.5217e+01, 1.4353e+02],\n",
       "         [0.0000e+00, 1.2021e+01, 3.5191e+01, 1.1969e+02],\n",
       "         [0.0000e+00, 0.0000e+00, 5.6613e+01, 1.5449e+02],\n",
       "         [2.4535e+01, 0.0000e+00, 6.2992e+01, 1.0730e+02],\n",
       "         [8.5111e+00, 1.0460e-01, 4.7274e+01, 1.0720e+02],\n",
       "         [5.1951e+01, 0.0000e+00, 9.0493e+01, 1.0766e+02],\n",
       "         [7.6514e-01, 0.0000e+00, 3.9132e+01, 1.0331e+02],\n",
       "         [5.9956e+01, 1.0749e+01, 9.8223e+01, 1.1982e+02],\n",
       "         [2.8138e+01, 3.5687e+01, 6.7008e+01, 1.4319e+02],\n",
       "         [0.0000e+00, 0.0000e+00, 8.4206e+01, 1.6864e+02],\n",
       "         [1.8625e+01, 0.0000e+00, 1.0438e+02, 1.1003e+02],\n",
       "         [2.9273e+01, 1.0861e+01, 1.1876e+02, 2.1447e+02],\n",
       "         [5.5888e+01, 3.0669e+01, 9.4337e+01, 1.4029e+02],\n",
       "         [4.4384e+01, 0.0000e+00, 8.3161e+01, 1.0360e+02],\n",
       "         [4.8364e+01, 0.0000e+00, 8.7397e+01, 7.4290e+01],\n",
       "         [1.5837e+01, 3.9310e+01, 1.0724e+02, 2.2400e+02],\n",
       "         [0.0000e+00, 4.6070e+00, 6.9955e+01, 2.1016e+02],\n",
       "         [0.0000e+00, 3.4208e+01, 8.7152e+01, 2.2400e+02],\n",
       "         [0.0000e+00, 6.5558e+01, 7.1524e+01, 2.2400e+02],\n",
       "         [0.0000e+00, 3.7186e+01, 3.9557e+01, 1.0938e+02],\n",
       "         [0.0000e+00, 2.0744e+01, 3.8699e+01, 9.2777e+01],\n",
       "         [0.0000e+00, 5.7609e+01, 7.3769e+01, 8.1011e+01],\n",
       "         [0.0000e+00, 3.7645e+01, 7.4734e+01, 6.0521e+01],\n",
       "         [0.0000e+00, 6.1512e+01, 7.8833e+01, 8.4820e+01],\n",
       "         [0.0000e+00, 4.9879e+01, 7.9817e+01, 7.2709e+01],\n",
       "         [0.0000e+00, 3.4289e+01, 8.0594e+01, 5.6384e+01],\n",
       "         [0.0000e+00, 2.6265e+01, 8.0425e+01, 4.8241e+01],\n",
       "         [0.0000e+00, 6.5583e+01, 1.0395e+02, 8.8469e+01],\n",
       "         [3.4977e+01, 2.9158e+01, 1.8257e+02, 5.3202e+01],\n",
       "         [0.0000e+00, 6.6497e+01, 4.4834e+01, 1.4341e+02],\n",
       "         [3.9122e+01, 2.5066e+01, 1.8385e+02, 4.8980e+01],\n",
       "         [4.9255e+01, 6.6049e+01, 1.2425e+02, 1.4307e+02],\n",
       "         [0.0000e+00, 6.1876e+01, 1.2502e+02, 8.3984e+01],\n",
       "         [2.5140e+01, 6.6163e+01, 1.0100e+02, 1.4361e+02],\n",
       "         [0.0000e+00, 3.0375e+01, 1.0672e+02, 5.1399e+01],\n",
       "         [0.0000e+00, 3.8281e+01, 1.0302e+02, 5.9439e+01],\n",
       "         [4.8797e+01, 4.9897e+01, 1.2512e+02, 1.2767e+02],\n",
       "         [0.0000e+00, 5.7917e+01, 1.0269e+02, 7.9833e+01],\n",
       "         [0.0000e+00, 3.4092e+01, 1.2679e+02, 5.5563e+01],\n",
       "         [3.7071e+01, 4.6120e+01, 1.9106e+02, 6.8402e+01],\n",
       "         [0.0000e+00, 4.5831e+01, 4.9393e+01, 1.2323e+02],\n",
       "         [4.0987e+01, 5.7465e+01, 1.1707e+02, 1.3557e+02],\n",
       "         [0.0000e+00, 4.1970e+01, 1.1468e+02, 6.3180e+01],\n",
       "         [6.1487e+01, 4.5205e+01, 1.3703e+02, 1.2343e+02],\n",
       "         [3.5795e+01, 0.0000e+00, 1.1285e+02, 5.1669e+01],\n",
       "         [3.0422e+01, 3.7808e+01, 1.8315e+02, 5.9925e+01],\n",
       "         [4.5739e+01, 4.9811e+01, 1.9929e+02, 7.2358e+01],\n",
       "         [0.0000e+00, 3.7051e+01, 1.4078e+02, 5.9267e+01],\n",
       "         [0.0000e+00, 4.1496e+01, 6.9381e+01, 1.1935e+02],\n",
       "         [3.6287e+01, 9.2227e+00, 1.1310e+02, 8.7632e+01],\n",
       "         [0.0000e+00, 4.5309e+01, 1.4163e+02, 6.7319e+01],\n",
       "         [1.6188e+01, 5.3211e+01, 9.2960e+01, 1.3114e+02],\n",
       "         [0.0000e+00, 0.0000e+00, 3.2431e+01, 8.5873e+01],\n",
       "         [4.9255e+01, 6.6049e+01, 1.2425e+02, 1.4307e+02],\n",
       "         [4.5739e+01, 4.9811e+01, 1.9929e+02, 7.2358e+01],\n",
       "         [1.5837e+01, 3.9310e+01, 1.0724e+02, 2.2400e+02],\n",
       "         [3.7071e+01, 4.6120e+01, 1.9106e+02, 6.8402e+01],\n",
       "         [0.0000e+00, 3.7645e+01, 7.4734e+01, 6.0521e+01],\n",
       "         [0.0000e+00, 0.0000e+00, 5.6613e+01, 1.5449e+02],\n",
       "         [0.0000e+00, 3.7645e+01, 7.4734e+01, 6.0521e+01],\n",
       "         [0.0000e+00, 4.9879e+01, 7.9817e+01, 7.2709e+01],\n",
       "         [2.9273e+01, 1.0861e+01, 1.1876e+02, 2.1447e+02],\n",
       "         [0.0000e+00, 4.6070e+00, 6.9955e+01, 2.1016e+02],\n",
       "         [0.0000e+00, 0.0000e+00, 8.6221e+01, 6.5063e+01],\n",
       "         [4.9255e+01, 6.6049e+01, 1.2425e+02, 1.4307e+02],\n",
       "         [0.0000e+00, 3.8281e+01, 1.0302e+02, 5.9439e+01],\n",
       "         [0.0000e+00, 9.7420e+00, 8.7434e+01, 1.0036e+02],\n",
       "         [5.5888e+01, 3.0669e+01, 9.4337e+01, 1.4029e+02],\n",
       "         [7.7028e+00, 5.9717e+01, 1.6329e+02, 1.5093e+02],\n",
       "         [0.0000e+00, 3.7186e+01, 3.9557e+01, 1.0938e+02],\n",
       "         [0.0000e+00, 4.5420e+01, 7.3977e+01, 6.8719e+01],\n",
       "         [6.1487e+01, 4.5205e+01, 1.3703e+02, 1.2343e+02],\n",
       "         [0.0000e+00, 4.6070e+00, 6.9955e+01, 2.1016e+02],\n",
       "         [5.1951e+01, 0.0000e+00, 9.0493e+01, 1.0766e+02],\n",
       "         [1.7518e+01, 0.0000e+00, 1.0476e+02, 1.9962e+02],\n",
       "         [0.0000e+00, 3.4289e+01, 8.0594e+01, 5.6384e+01],\n",
       "         [0.0000e+00, 4.1496e+01, 6.9381e+01, 1.1935e+02],\n",
       "         [3.0701e+01, 0.0000e+00, 1.1607e+02, 1.3701e+02],\n",
       "         [0.0000e+00, 3.8281e+01, 1.0302e+02, 5.9439e+01]], device='cuda:0',\n",
       "        dtype=torch.float64)}"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fast RCNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Fast_RCNN(nn.Module):\n",
    "    #TODO : remove embedding_dim using wv.shape[1]\n",
    "    #define all the layers used in model\n",
    "    def __init__(self,in_channels,mid_channels,nb_classes,pre_trained_model,min_size_anchors,\n",
    "                 before_top_nms,after_top_nms,pos_t, model_RPN, nsize, \n",
    "                 pos_ratio, pos_iou_threshold, number_roi_pooling, scale):\n",
    "        \n",
    "        #Constructor\n",
    "        super().__init__()        \n",
    "        \n",
    "        self.in_channels = in_channels\n",
    "        self.mid_channels = mid_channels\n",
    "        self.nb_classes = nb_classes\n",
    "        self.pre_trained_model = pre_trained_model\n",
    "        self.min_size_anchors = min_size_anchors\n",
    "        self.before_top_nms = before_top_nms\n",
    "        self.after_top_nms = after_top_nms\n",
    "        self.pos_t = pos_t\n",
    "        self.model_RPN = model_RPN\n",
    "        \n",
    "        #for batch FRCNN\n",
    "        self.nsize = nsize\n",
    "        self.pos_ratio = pos_ratio\n",
    "        self.pos_iou_threshold = pos_iou_threshold\n",
    "        self.number_roi_pooling = number_roi_pooling\n",
    "        self.scale = scale\n",
    "        \n",
    "        #embedding layer\n",
    "        self.pre_trained_model = pre_trained_model\n",
    "        self.lin1 = nn.Linear(in_channels, mid_channels)\n",
    "        self.lin2 = nn.Linear(mid_channels, mid_channels)\n",
    "        self.score = nn.Linear(mid_channels, (nb_classes + 1))\n",
    "        self.boxes = nn.Linear(mid_channels, 4*(nb_classes + 1))\n",
    "        \n",
    "        self.boxes.weight.data.normal_(0, 0.01)\n",
    "        self.boxes.bias.data.zero_()\n",
    "\n",
    "        self.score.weight.data.normal_(0, 0.01)\n",
    "        self.score.bias.data.zero_()\n",
    "       \n",
    "    \n",
    "        \n",
    "    def forward(self,batch):\n",
    "        nb_images = batch[\"images\"].shape[0] #sale ?\n",
    "        feature_maped = self.pre_trained_model(batch[\"images\"].to(device))\n",
    "        after_rpn = RPN_to_FRCNN(batch,batch[\"images\"].shape[-1],self.min_size_anchors,self.before_top_nms,self.after_top_nms,self.pos_t,self.model_RPN, feature_maped)\n",
    "        minibatch_FRCNN = batch_training_proposal_FastRCNN(after_rpn[\"feature_map\"],after_rpn[\"list_roi\"],after_rpn[\"list_gt_box\"],after_rpn[\"list_gt_labels\"], \n",
    "                                                           device,nsize = self.nsize, pos_ratio = self.pos_ratio, pos_iou_threshold = self.pos_iou_threshold,\n",
    "                                adaptative_max_pool = torch.nn.AdaptiveMaxPool2d((self.number_roi_pooling,self.number_roi_pooling),return_indices=False),\n",
    "                                                           scale = self.scale)\n",
    "        x = minibatch_FRCNN[\"feature_map_extract\"] #put the feature map extracted\n",
    "        x = x.view(128, -1) #128 as parameters\n",
    "        x = self.lin1(x)\n",
    "        x = self.lin2(x)\n",
    "        x_score = self.score(x)\n",
    "        x_boxes = self.boxes(x)\n",
    "        return x_boxes,x_score, minibatch_FRCNN\n",
    "\n",
    "#def RPN_to_FRCNN(exemple, feature_size, min_size_anchors, before_top_nms,after_top_nms,pos_t, model, feature_map):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import models\n",
    "\n",
    "resnet50 = models.resnet50(pretrained=True)\n",
    "image_torch = 800*torch.rand((1,3,800,800))\n",
    "#We choose the place where we extracted the feature map in order to get H_feature * W_feature around 2400 (papers)\n",
    "resnet50_features = nn.Sequential(*(list(resnet50.children())[:-5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "fast_rcnn = Fast_RCNN(12544, 4096, 1000, resnet50_features,16,12000,1000,0.7,model_RPN,128,0.25,0.5,7,1/4).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Losses for Fast RCNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "#target : results of batch_training_proposal_FastRCNN function (dict with featured_extracted, boxes, labels)\n",
    "#pred : resultats of forward function\n",
    "\n",
    "def FRCNN_loss(pred,target, loss_ce = nn.CrossEntropyLoss(ignore_index=-1),loss_sml1 = nn.SmoothL1Loss(reduction = \"sum\"),  lamb = 1):\n",
    "  #Remove all -1 for the computation of the CE loss\n",
    "  #Classification part\n",
    "  loss_cls = loss_ce(pred[1],target[\"labels\"])\n",
    "  #Regression part (\"1000\" is the background)\n",
    "  boxes = (res[\"labels\"] != 1000).unsqueeze(-1).repeat(1,1,4).float() * res[\"boxes\"]\n",
    "  nb_feature = pred[0].shape[0]\n",
    "  pred = pred[0].view(nb_feature,-1,4)[torch.arange(0,nb_feature).long(), res[\"labels\"]] * (res[\"labels\"] != 1000).unsqueeze(-1).repeat(1,1,4).float() \n",
    "  loss_reg = loss_sml1(boxes, pred)\n",
    "  #Final loss\n",
    "  loss = loss_cls + lamb*loss_reg\n",
    "  return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training for Fast RCNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet50 = models.resnet50(pretrained=True)\n",
    "resnet50_features = nn.Sequential(*(list(resnet50.children())[:-5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "fast_rcnn = Fast_RCNN(12544, 4096, 1000, copy.deepcopy(resnet50_features),16,12000,1000,0.7,model_RPN,128,0.25,0.5,7,1/4).to(device)\n",
    "optimizer = optim.SGD(fast_rcnn.parameters(), lr=0.001, momentum=0.9)\n",
    "best_model_frcnn = Fast_RCNN(12544, 4096, 1000, copy.deepcopy(resnet50_features),16,12000,1000,0.7,model_RPN,128,0.25,0.5,7,1/4).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1+2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "#TODO : save better model ! on valid ?\n",
    "t0 = time.time()\n",
    "loss_list_train = [float('inf')]\n",
    "loss_list_valid = [float('inf')]\n",
    "nb_epoch = N_EPOCH\n",
    "i = 0\n",
    "\n",
    "\n",
    "for epoch in range(nb_epoch):\n",
    "  #training\n",
    "  fast_rcnn.train()\n",
    "  losses = 0\n",
    "  for element in instances_train_dataloader:\n",
    "    if(len(element[1][0][\"labels\"]) != 0):\n",
    "        exemple =  batch_training_proposal_multi_RPN(batch_exemple, feature_shape, ratio, anchor_scales, device)    \n",
    "        output = fast_rcnn(exemple)\n",
    "        loss = FRCNN_loss(output, output[2])\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        losses =+ loss.item()\n",
    "  loss_epch = losses/len(instances_train_dataloader)\n",
    "  loss_list_train.append(loss_epch)\n",
    "  #valid\n",
    "  with torch.no_grad():\n",
    "    losses = 0\n",
    "    for element in instances_valid_dataloader:\n",
    "        if(len(element[1][0][\"labels\"]) != 0):\n",
    "          input = batch_training_proposal_multi_RPN(element, feature_shape, ratio, anchor_scales, device)\n",
    "          output = fast_rcnn(exemple)\n",
    "          fast_rcnn.eval()\n",
    "          loss = FRCNN_loss(output, output[2])\n",
    "          losses =+ loss.item()\n",
    "    loss_epch_valid = losses/len(instances_valid_dataloader)\n",
    "    if loss_epch_valid < min(loss_list_valid):\n",
    "        print(True)\n",
    "        best_model_frcnn = copy.deepcopy(fast_rcnn.state_dict())\n",
    "        #Ajoutez des mÃ©triques\n",
    "    loss_list_valid.append(loss_epch_valid)\n",
    "    print(f'\\tEpoch {i} : Train Loss: {loss_epch:.3f} | \\tValid Loss: {loss_epch_valid:.3f}')\n",
    "  i += 1\n",
    "\n",
    "t1 = time.time()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(best_model_frcnn,\"model_frcnn.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alternative learning startegy : we learned RPN network once again but using the initilisation given by Fast RCNN. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fast_rcnn.load_state_dict(torch.load(\"model_frcnn.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_extractor_after_FRCNN = copy.deepcopy(fast_rcnn.pre_trained_model)\n",
    "feature_shape = (1,1,56,56)\n",
    "\n",
    "model_RPN_after_FRCNN = RPN(256,256,9,feature_extractor_after_FRCNN).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Freeze pre-trained model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in model_RPN_after_FRCNN.pre_trained_model.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "optimizer = optim.SGD(model_RPN_after_FRCNN.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "#TODO : save better model ! on valid ?\n",
    "t0 = time.time()\n",
    "loss_list_train = [float('inf')]\n",
    "loss_list_valid = [float('inf')]\n",
    "nb_epoch = N_EPOCH\n",
    "i = 0\n",
    "\n",
    "best_model_afterFRCNN = RPN(256,256,9,feature_extractor_after_FRCNN).to(device)\n",
    "\n",
    "for epoch in range(nb_epoch):\n",
    "  #training\n",
    "  model_RPN_after_FRCNN.train()\n",
    "  losses = 0\n",
    "  for element in instances_train_dataloader:\n",
    "        if(len(element[1][0][\"labels\"]) != 0):\n",
    "            input = batch_training_proposal_multi_RPN(element, feature_shape, ratio, anchor_scales, device)\n",
    "            output = model_RPN_after_FRCNN(input[\"images\"])\n",
    "            loss = RPN_loss(output, input,torch.tensor(feature_shape))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            losses =+ loss.item()\n",
    "  loss_epch = losses/len(instances_train_dataloader)\n",
    "  loss_list_train.append(loss_epch)\n",
    "  #valid\n",
    "  with torch.no_grad():\n",
    "    losses = 0\n",
    "    for element in instances_valid_dataloader:\n",
    "        if(len(element[1][0][\"labels\"]) != 0):\n",
    "          input = batch_training_proposal_multi_RPN(element, feature_shape, ratio, anchor_scales, device)\n",
    "          output = model_RPN_after_FRCNN(input[\"images\"])\n",
    "          model_RPN_after_FRCNN.eval()\n",
    "          loss = RPN_loss(output, input, torch.tensor(feature_shape))\n",
    "          losses =+ loss.item()\n",
    "    loss_epch_valid = losses/len(instances_valid_dataloader)\n",
    "    if loss_epch_valid < min(loss_list_valid):\n",
    "        best_model_afterFRCNN = (copy.deepcopy(model_RPN.state_dict()))\n",
    "        #Ajoutez des mÃ©triques\n",
    "    loss_list_valid.append(loss_epch_valid)\n",
    "    print(f'\\tEpoch {i} : Train Loss: {loss_epch:.3f} | \\tValid Loss: {loss_epch_valid:.3f}')\n",
    "  i += 1\n",
    " \n",
    "\n",
    "t1 = time.time()\n",
    "\n",
    "total = t1-t0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(best_model_afterFRCNN, \"RPN_after_FRCNN.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_RPN_after_FRCNN.load_state_dict(torch.load(\"RPN_after_FRCNN.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "fast_rcnn_after_alter = Fast_RCNN(12544, 4096, 1000, copy.deepcopy(model_RPN_after_FRCNN.pre_trained_model),16,12000,1000,0.7,\n",
    "                                  copy.deepcopy(model_RPN_after_FRCNN),128,0.25,0.5,7,1/4).to(device)\n",
    "best_model_frcnn_after_alter = Fast_RCNN(12544, 4096, 1000, copy.deepcopy(model_RPN_after_FRCNN.pre_trained_model),16,12000,1000,0.7,\n",
    "                                  copy.deepcopy(model_RPN_after_FRCNN),128,0.25,0.5,7,1/4).to(device)\n",
    "optimizer = optim.SGD(fast_rcnn_after_alter.parameters(), lr=0.0001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in fast_rcnn_after_alter.pre_trained_model.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "#TODO : save better model ! on valid ?\n",
    "t0 = time.time()\n",
    "loss_list_train = [float('inf')]\n",
    "loss_list_valid = [float('inf')]\n",
    "nb_epoch = N_EPOCH\n",
    "i = 0\n",
    "\n",
    "for epoch in range(nb_epoch):\n",
    "  #training\n",
    "  fast_rcnn_after_alter.train()\n",
    "  losses = 0\n",
    "  for element in instances_train_dataloader:\n",
    "    exemple =  batch_training_proposal_multi_RPN(batch_exemple, feature_shape, ratio, anchor_scales, device)    \n",
    "    output = fast_rcnn_after_alter(exemple)\n",
    "    loss = FRCNN_loss(output, output[2])\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "    losses =+ loss.item()\n",
    "  loss_epch = losses/len(instances_train_dataloader)\n",
    "  loss_list_train.append(loss_epch)\n",
    "  #valid\n",
    "  with torch.no_grad():\n",
    "    losses = 0\n",
    "    for element in instances_valid_dataloader:\n",
    "        if(len(element[1][0][\"labels\"]) != 0):\n",
    "          input = batch_training_proposal_multi_RPN(element, feature_shape, ratio, anchor_scales, device)\n",
    "          output = fast_rcnn_after_alter(exemple)\n",
    "          fast_rcnn_after_alter.eval()\n",
    "          loss = FRCNN_loss(output, output[2])\n",
    "          losses =+ loss.item()\n",
    "    loss_epch_valid = losses/len(instances_valid_dataloader)\n",
    "    if loss_epch_valid < min(loss_list_valid):\n",
    "        best_model_frcnn_after_alter = copy.deepcopy(fast_rcnn_after_alter.state_dict())\n",
    "        #Ajoutez des mÃ©triques\n",
    "    loss_list_valid.append(loss_epch_valid)\n",
    "    print(f'\\tEpoch {i} : Train Loss: {loss_epch:.3f} | \\tValid Loss: {loss_epch_valid:.3f}')\n",
    "  i += 1\n",
    "\n",
    "t1 = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(best_model_frcnn_after_alter, \"final_model.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "after_rpn = RPN_to_FRCNN(exemple, 224,16,12000,1000,0.7,model_RPN, pre_trained_model_after_RPN(exemple[\"images\"]))\n",
    "\n",
    "res = batch_training_proposal_FastRCNN(after_rpn[\"feature_map\"],after_rpn[\"list_roi\"],after_rpn[\"list_gt_box\"],after_rpn[\"list_gt_labels\"],device,\n",
    "                                       nsize = 128, pos_ratio = 0.25, pos_iou_threshold = 0.5,\n",
    "                                adaptative_max_pool = torch.nn.AdaptiveMaxPool2d((7,7),return_indices=False), scale = 1/4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction_to_pict_and_boxes(exemple, model, iou_min = 0.2,\n",
    "                                invTrans = transforms.Normalize(\n",
    "    mean=[-0.485/0.229, -0.456/0.224, -0.406/0.255],\n",
    "    std=[1/0.229, 1/0.224, 1/0.255]\n",
    ")):\n",
    "    pred = model(exemple)\n",
    "\n",
    "    tables = [table.squeeze().detach().cpu().numpy() for table in pred[0].view(128,-1,4).split(1,1)]\n",
    "\n",
    "    res_deparam = deloc_list(np.repeat(pred[2][\"non_reparam_box\"].detach().cpu().numpy(),1001,axis = 1),\n",
    "                            np.hstack(tables))\n",
    "\n",
    "    res_deparam = res_deparam.clip(0, 255)\n",
    "    \n",
    "    sm1 = nn.Softmax(dim = 1)\n",
    "    proba = sm1(model(exemple)[1])\n",
    "\n",
    "    score = proba[:,411]\n",
    "    ind = torchvision.ops.batched_nms(torch.from_numpy(res_deparam).to(device), score, torch.zeros(res_deparam.shape[0]).to(device),\n",
    "                                      iou_min)\n",
    "\n",
    "    box_to_keep = res_deparam[ind.cpu().numpy(),:]\n",
    "\n",
    "    box_to_keep = box_to_keep.clip(0,255)\n",
    "    box = box_to_keep[:, (1,0,3,2)]\n",
    "    \n",
    "    return {\"images\" : invTrans(exemple[\"images\"]), \"boxes\" : box}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = prediction_to_pict_and_boxes(exemple, fast_rcnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'images': tensor([[[[0.0980, 0.0824, 0.0824,  ..., 0.0314, 0.0314, 0.0314],\n",
       "           [0.2235, 0.2000, 0.1922,  ..., 0.0784, 0.0784, 0.0784],\n",
       "           [0.2000, 0.1922, 0.1882,  ..., 0.0784, 0.0784, 0.0784],\n",
       "           ...,\n",
       "           [0.3020, 0.3098, 0.2902,  ..., 0.6353, 0.2941, 0.2000],\n",
       "           [0.3020, 0.3059, 0.2941,  ..., 0.5569, 0.2392, 0.2118],\n",
       "           [0.3098, 0.2941, 0.2784,  ..., 0.1529, 0.1529, 0.1882]],\n",
       " \n",
       "          [[0.1216, 0.1020, 0.0941,  ..., 0.0549, 0.0549, 0.0549],\n",
       "           [0.2353, 0.2078, 0.2000,  ..., 0.1137, 0.1137, 0.1137],\n",
       "           [0.2118, 0.2039, 0.2000,  ..., 0.1137, 0.1137, 0.1137],\n",
       "           ...,\n",
       "           [0.2824, 0.2902, 0.2667,  ..., 0.6784, 0.3412, 0.2471],\n",
       "           [0.2784, 0.2824, 0.2667,  ..., 0.5804, 0.2627, 0.2314],\n",
       "           [0.2824, 0.2667, 0.2431,  ..., 0.1490, 0.1490, 0.1843]],\n",
       " \n",
       "          [[0.0970, 0.0748, 0.0659,  ..., 0.0392, 0.0392, 0.0392],\n",
       "           [0.3014, 0.2748, 0.2659,  ..., 0.1059, 0.1059, 0.1059],\n",
       "           [0.2748, 0.2659, 0.2614,  ..., 0.1103, 0.1103, 0.1103],\n",
       "           ...,\n",
       "           [0.3636, 0.3814, 0.3681,  ..., 0.7103, 0.3236, 0.2170],\n",
       "           [0.3236, 0.3414, 0.3370,  ..., 0.6036, 0.2436, 0.2081],\n",
       "           [0.3059, 0.2925, 0.2748,  ..., 0.1236, 0.1281, 0.1681]]]],\n",
       "        device='cuda:0'),\n",
       " 'boxes': array([[0.        , 0.        , 0.78483731, 0.60554735]])}"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pictures_and_boxes(image_tensor, boxes):\n",
    "    img = transforms.functional.to_pil_image(image_tensor.squeeze())\n",
    "    draw = ImageDraw.Draw(img)\n",
    "    for box in boxes.tolist():\n",
    "        draw.rectangle(xy = box)\n",
    "    #Add grown trunth :\n",
    "    gt_box = [224*box[[1,0,3,2]] for box in exemple[\"gt_box\"][0]]\n",
    "    gt_box = [yxhw_to_vertice(box)[[1,0,3,2]] for box in gt_box]    \n",
    "    for gtb in gt_box:\n",
    "        draw.rectangle(xy = gtb.tolist(), outline = \"red\")\n",
    "        print(gtb)\n",
    "    return(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_exemple = next(iter(instances_train_dataloader))\n",
    "exemple =  batch_training_proposal_multi_RPN(batch_exemple, feature_shape, ratio, anchor_scales, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.22s)\n",
      "creating index...\n",
      "index created!\n",
      "0.4666666666666667\n",
      "0.4666666666666667\n"
     ]
    }
   ],
   "source": [
    "instances_train2 = CocoDetection_diy_bis(root = data_path, annFile = labels_path, size=(224,224), elements_index = ind_n_train)\n",
    "instances_train_dataloader2 = DataLoader(instances_train2, batch_size=1, shuffle=False, collate_fn = collate_fn_diy)\n",
    "\n",
    "batch_exemple = next(iter(instances_train2))\n",
    "batch_exemple_dl = next(iter(instances_train_dataloader2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[[-0.0287, -0.0458, -0.0629,  ..., -1.4500, -1.4329, -1.5014],\n",
       "           [-0.1143, -0.1314, -0.1657,  ..., -1.4500, -1.4329, -1.5014],\n",
       "           [-0.2171, -0.2342, -0.2513,  ..., -1.4672, -1.4500, -1.5014],\n",
       "           ...,\n",
       "           [ 0.1254,  0.5022,  0.9132,  ...,  0.6392, -0.8507, -1.2617],\n",
       "           [ 0.9646,  0.9646,  1.3070,  ...,  0.3309, -1.0562, -1.1760],\n",
       "           [ 1.7352,  1.6324,  1.8722,  ..., -0.8164, -1.0390, -1.1075]],\n",
       " \n",
       "          [[ 0.2402,  0.2227,  0.2402,  ..., -1.0728, -1.0553, -1.1253],\n",
       "           [ 0.2927,  0.2752,  0.2927,  ..., -1.0728, -1.0553, -1.1253],\n",
       "           [ 0.3803,  0.3627,  0.3627,  ..., -1.0553, -1.0378, -1.1078],\n",
       "           ...,\n",
       "           [ 0.9580,  1.3256,  1.7108,  ...,  1.0980, -0.4251, -0.8452],\n",
       "           [ 1.5882,  1.5707,  1.8859,  ...,  0.5903, -0.8277, -0.9678],\n",
       "           [ 2.0959,  1.9734,  2.2185,  ..., -0.7927, -1.0203, -1.0903]],\n",
       " \n",
       "          [[ 0.6356,  0.6182,  0.6182,  ..., -0.8633, -0.8458, -0.9156],\n",
       "           [ 0.6356,  0.6182,  0.6182,  ..., -0.8633, -0.8458, -0.9156],\n",
       "           [ 0.6531,  0.6356,  0.6356,  ..., -0.8633, -0.8284, -0.8981],\n",
       "           ...,\n",
       "           [ 1.3851,  1.7511,  2.1171,  ...,  1.2457, -0.2707, -0.6890],\n",
       "           [ 1.9951,  1.9603,  2.2566,  ...,  0.7925, -0.5844, -0.7413],\n",
       "           [ 2.4657,  2.3786,  2.5354,  ..., -0.4973, -0.7238, -0.7936]]]]),\n",
       " [{'labels': tensor([ 1, 14]),\n",
       "   'boxes': tensor([[4.2702e-04, 7.8981e+01, 6.6421e+01, 6.0783e+01],\n",
       "           [7.9054e+01, 7.7433e+01, 6.0969e+00, 5.8059e+00]], dtype=torch.float64)}])"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_exemple_dl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[-0.0287, -0.0458, -0.0629,  ..., -1.4500, -1.4329, -1.5014],\n",
       "          [-0.1143, -0.1314, -0.1657,  ..., -1.4500, -1.4329, -1.5014],\n",
       "          [-0.2171, -0.2342, -0.2513,  ..., -1.4672, -1.4500, -1.5014],\n",
       "          ...,\n",
       "          [ 0.1254,  0.5022,  0.9132,  ...,  0.6392, -0.8507, -1.2617],\n",
       "          [ 0.9646,  0.9646,  1.3070,  ...,  0.3309, -1.0562, -1.1760],\n",
       "          [ 1.7352,  1.6324,  1.8722,  ..., -0.8164, -1.0390, -1.1075]],\n",
       " \n",
       "         [[ 0.2402,  0.2227,  0.2402,  ..., -1.0728, -1.0553, -1.1253],\n",
       "          [ 0.2927,  0.2752,  0.2927,  ..., -1.0728, -1.0553, -1.1253],\n",
       "          [ 0.3803,  0.3627,  0.3627,  ..., -1.0553, -1.0378, -1.1078],\n",
       "          ...,\n",
       "          [ 0.9580,  1.3256,  1.7108,  ...,  1.0980, -0.4251, -0.8452],\n",
       "          [ 1.5882,  1.5707,  1.8859,  ...,  0.5903, -0.8277, -0.9678],\n",
       "          [ 2.0959,  1.9734,  2.2185,  ..., -0.7927, -1.0203, -1.0903]],\n",
       " \n",
       "         [[ 0.6356,  0.6182,  0.6182,  ..., -0.8633, -0.8458, -0.9156],\n",
       "          [ 0.6356,  0.6182,  0.6182,  ..., -0.8633, -0.8458, -0.9156],\n",
       "          [ 0.6531,  0.6356,  0.6356,  ..., -0.8633, -0.8284, -0.8981],\n",
       "          ...,\n",
       "          [ 1.3851,  1.7511,  2.1171,  ...,  1.2457, -0.2707, -0.6890],\n",
       "          [ 1.9951,  1.9603,  2.2566,  ...,  0.7925, -0.5844, -0.7413],\n",
       "          [ 2.4657,  2.3786,  2.5354,  ..., -0.4973, -0.7238, -0.7936]]]),\n",
       " {'labels': [1, 14],\n",
       "  'boxes': [[0.0004270152505446623,\n",
       "    78.98149987896393,\n",
       "    66.42136819172113,\n",
       "    60.78267925441782],\n",
       "   [79.05376034858388,\n",
       "    77.43285790365529,\n",
       "    6.09692374727669,\n",
       "    5.805889130961028]]})"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_exemple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([7.89814999e+01, 4.27015251e-04, 1.39764179e+02, 6.64217952e+01])"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xywh_to_vertice(batch_exemple[1][\"boxes\"][0])"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "anchor_boxes.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "toc-autonumbering": true,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false,
  "toc-showtags": true,
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "04e8eac584c947f8a4dd2e7ae9721dd7": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "079761f87ba8483e9ccf4b97da879ef1": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5d702123da9e48129ff7557a02f72b99": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_079761f87ba8483e9ccf4b97da879ef1",
      "placeholder": "â",
      "style": "IPY_MODEL_6eefd4e548024038b8e50141bdeef898",
      "value": "100%"
     }
    },
    "5dfa0e0da40b4ddda4e7dd26a4243622": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_04e8eac584c947f8a4dd2e7ae9721dd7",
      "max": 102530333,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_f0bcc1811e5b4fb49be83fd1bf52c557",
      "value": 102530333
     }
    },
    "6eefd4e548024038b8e50141bdeef898": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "7ff93690eff6496ca0eaff04af2d197e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9ac6009437a84a5d8868dbfa7a209924": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "dd231e88790d42bea6867aa556651c8e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_5d702123da9e48129ff7557a02f72b99",
       "IPY_MODEL_5dfa0e0da40b4ddda4e7dd26a4243622",
       "IPY_MODEL_f77ce4b2ae214c80984ba0b4fa2d2c14"
      ],
      "layout": "IPY_MODEL_7ff93690eff6496ca0eaff04af2d197e"
     }
    },
    "e9e980dc151b4fc3a242255c7f92dd34": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "f0bcc1811e5b4fb49be83fd1bf52c557": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "f77ce4b2ae214c80984ba0b4fa2d2c14": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_9ac6009437a84a5d8868dbfa7a209924",
      "placeholder": "â",
      "style": "IPY_MODEL_e9e980dc151b4fc3a242255c7f92dd34",
      "value": " 97.8M/97.8M [00:01&lt;00:00, 97.8MB/s]"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
